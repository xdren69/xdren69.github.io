{"pages":[{"title":"about","text":"WelcomeGlad to see you here. I will write down my insights about DL,NLP,Rsys in this place. Education 2015.09-2019.06 Software Engineering, Northwestern Polytechnical University 2020.09 Going to study at the National Cyber Security College of Wuhan University Research InterestDL，NLP，Recommend System Contect Email:","link":"/about/index.html"}],"posts":[{"title":"datawhale-零基础入门NLP-Task2","text":"学习目标对需要训练的数据进行分析，同时通过可视化了解待训练数据的特点 前半部分的代码是官方文档提供的；作业部分属于自己完成的 数据分析读入并观察数据的格式1234import pandas as pdtrain_df = pd.read_csv('../data/train_set.csv', sep='\\t')train_df.head() 分析每段文本的长度12345%pylab inlinetrain_df['text_len'] = train_df['text'].apply(lambda x: len(x.split(' ')))print(train_df.head())print(train_df['text_len'].describe()) 由此我们可以看到，对于所有文本，平均长度为907，最短为2，最长为57921 文本长度可视化分析123_ = plt.hist(train_df['text_len'], bins=200)plt.xlabel('Text char count')plt.title(\"Histogram of char count\") 由此我们可以看出文本长度的分布是不均匀的 文本类别可视化分析123train_df['label'].value_counts().plot(kind='bar')plt.title('News class count')plt.xlabel(\"category\") 由此我们可以看出，不同类别的文本数也是不同的，可能对模型的训练精度产生影响 单词频次分析先展示错误代码 12345678from collections import Counterall_lines = ' '.join(list(train_df['text']))word_count = Counter(all_lines.split(\" \"))word_count = sorted(word_count.items(), key=lambda d:d[1], reverse = True)print(len(word_count))print(word_count[0])print(word_count[-1]) 错误原因：在执行到第三行时会因内存占用太多而导致程序奔溃； 解决办法：将训练数据分批读入，而不是一次性全部读入 再展示正确代码 123456789101112from collections import Counterword_count = Counter()chunk_iterator = pd.read_csv('../data/train_set.csv',sep='\\t', chunksize=10000) for chunk in chunk_iterator: all_lines = ' '.join(list(chunk['text'])) word_count.update(all_lines.split(\" \"))word_count = sorted(word_count.items(), key=lambda d:d[1], reverse = True)print(len(word_count))print(word_count[0])print(word_count[-1]) 表明一共出现了6869个汉字，其中3750号汉字出现的次数最多，为7482224次；3133号汉字出现的次数最少，为1次 统计覆盖率最高的前三个单词 1234567891011121314from collections import Counter# 统计每个句子中出现的不同的单词，为每个句子构造一个单词集train_df['text_unique'] = train_df['text'].apply(lambda x: ' '.join(list(set(x.split(' ')))))# 将所有句子的单词集连接成文本all_lines = ' '.join(list(train_df['text_unique']))# 统计每个单词在单词集中出现的次数word_count = Counter(all_lines.split(\" \"))# 对统计结果进行排序word_count = sorted(word_count.items(), key=lambda d:int(d[1]), reverse = True)# 展示统计结果前三名，含义是每个单词在多少个句子中出现过print(word_count[0])print(word_count[1])print(word_count[2]) 表明：编号’3750’的单词出现在了197997个句子中；编号’900’的单词出现在了197653个句子中；编号’648’的单词出现在了191975个句子中 本章作业 假设字符3750，字符900和字符648是句子的标点符号，请分析赛题每篇新闻平均由多少个句子构成？ 12345678910111213# 可以直接套用前一段代码，统计这三个标点在所有文本中出现的总次数即可from collections import Counterword_count = Counter()chunk_iterator = pd.read_csv('../data/train_set.csv',sep='\\t', chunksize=10000) for chunk in chunk_iterator: all_lines = ' '.join(list(chunk['text'])) word_count.update(all_lines.split(\" \"))# 计算出总次数sum = word_count['3750']+word_count['900']+word_count['648']print(sum)# 计算出平均次数print(sum/200000.0) 统计每类新闻中出现次数对多的字符 1234567891011chunk_iterator = pd.read_csv('../data/train_set.csv',sep='\\t', chunksize=10000) for label in range(14): word_count = Counter() all_lines = '' for chunk in chunk_iterator: for index, l in enumerate(list(train_df['label'])): if l==label: all_lines = all_lines+' '+ train_df['text'][index] word_count.update(all_lines.split(\" \")) word_count = sorted(word_count.items(), key=lambda d:int(d[1]), reverse = True) print(label,word_count[0]) 引用[1] 提高python处理数据的效率方法 [2] Datawhale零基础入门NLP赛事 - Task2 数据读取与数据分析","link":"/2020/07/22/datawhale-NLP-t2/"},{"title":"Hexo+Icarus博客搭建记录","text":"个人感受我是一名前端小白，完全是跟着网上的教程走的，首先记录一下个人在看教程时的感受： 对整体架构不是很清楚，只能跟着教程一步一步摸索着走，遇到问题再google 找教程时要先看发布的时间，像Hexo和Icarus更新的挺快的，较早的教程是不适合现在的 不用跟着一个教程走到黑，可以看适合自己的部分，多找几个教程对比验证，就可以找到比较合适的实现方式了 网上关于这部分的教程说的已经很好很详细了，我就没必要造重复造轮子了，针对我自己（大神请忽略。。）一开始摸不着头脑的情况，我想先列出一个整体框架，让大家清楚自己每走一步都在那个阶段，并清楚下一阶段该做什么，我觉得这是比较有帮助的 整体架构这个blog搭建在github上的，使用了适用于个人blog的框架Hexo，Hexo的主题(theme)可由用户自定义(可以类比android的手机主题)，很多大神开源了很多主题，其中我选择的是Icarus，戳这里看demo 以上便是整体框架，接下来按照此可以划分出三步流程： 整体流程 在github上申请Repositories，并设置其为浏览器可访问的网页形式，具体教程可参照godweiyang在知乎的回答，PS：godweiyang自己改版了matery主题也很不错，大家也可以去看看他的主页，如果喜欢就可以一路跟着他在知乎的教程啦~ 安装并配置Hexo，具体教程同样可参照godweiyang在知乎的回答 安装好后，默认使用landscape主题，不过你现在已经可以尝试在此主题下写博客啦~PS：Hexo本身提供了标签、文章、归档等所有博客该有的功能，下一步的主题选择不影响当前编辑的内容的 此时需要先停下来，不着急进行下一步，先阅读Hexo的官方文档，PS：很短，官方文档是最直接了解这个框架功能的方式~，先了解一下怎么发文章，图片放在哪个目录下 在Hexo的基础上配置Icarus主题，完整过一遍Icarus的文档，建议阅读顺序如下： Icarus用户指南 - 主题配置对整体网站的配置 Icarus用户指南 - 挂件对于侧边栏的配置 随后运行整个网站，你会发现还有一些地方在报错，接下来就配置这些插件部分 做一些个性化改进： 图床配置，我是使用的腾讯云的COS服务，一开始会赠送6个月的服务，教程见这里 配置live2d看板娘（就是右手边的那只黑猫~），教程可以看这里 由于时间有限，不能改完所有bug使网站达到perfect，先记录下来，后期逐渐修改 bug(待修正)记录 个别渲染数学公式的格式不能使用的问题： 这是一个行内公式：\\(ax^2+bx+c=0\\)。 这是另一个行内公式：$ax^2+bx+c&gt;0$。 这是一个块状公式： $$\\displaystyle \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5} }-\\phi\\Bigr) e^{\\frac25 \\pi} } = 1+\\frac{e^{-2\\pi} } {1+\\frac{e^{-4\\pi} } {1+\\frac{e^{-6\\pi} } {1+\\frac{e^{-8\\pi} } {1+\\cdots} } } }$$ 这是另一个块状公式： \\\\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)e^{2 \\pi i \\xi x}d\\xi\\\\] source12345678910111213 这是一个行内公式：\\\\(ax^2+bx+c=0\\\\)。 这是另一个行内公式：$ax^2+bx+c&gt;0$。 这是一个块状公式（注意一定用&lt;div&gt;包裹）： &lt;div&gt; $$\\displaystyle \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5} }-\\phi\\Bigr) e^{\\frac25 \\pi} } = 1+\\frac{e^{-2\\pi} } {1+\\frac{e^{-4\\pi} } {1+\\frac{e^{-6\\pi} } {1+\\frac{e^{-8\\pi} } {1+\\cdots} } } }$$ &lt;/div&gt; 这是另一个块状公式： \\\\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)e^{2 \\pi i \\xi x}d\\xi\\\\] 以下为目前不能使用的 或者使用\\\\(\\LaTeX\\\\)环境： &lt;div&gt; \\\\begin{equation} A = \\\\begin{bmatrix} a &amp; b \\\\\\\\ c &amp; c \\\\end{bmatrix} \\\\end{equation} &lt;/div&gt;source1234567891011或者使用\\\\(\\LaTeX\\\\)环境：&lt;div&gt;\\\\begin{equation}A =\\\\begin{bmatrix}a &amp; b \\\\\\\\c &amp; c\\\\end{bmatrix}\\\\end{equation}&lt;/div&gt; busuanzi网站分析与live2d的看板娘冲突的问题 有人已提过issue，相关blog；由于加入live2d有两种实现方式，还有一种需要修改源码（官网推荐），教程yingchi’s blog navBar中的svg格式的logo不显示的问题（和footer用的是同一个logo，现在只有footer显示了） 有人已提过issue 给每一个页面加图片(optional) 最后欢迎大家评论与交流~","link":"/2020/06/25/how-to-build-a-blog/"},{"title":"整体认识——pytorch-learning（一）","text":"重要提醒：本文不求原创和创新，是基于哈工大SCIR实验室的大佬们写的这篇文章的补充和拓展，因此会合原文有许多重复之处，请大家注意！ 常用术语 iteration：表示1次迭代（也叫training step），每次迭代更新1次网络结构的参数，所需的数据量为1个batch的大小 batch-size：1次迭代所使用的样本量 epoch：1个epoch表示过了1遍训练集中的所有样本（包含多个batch的数据） 基本步骤我们首先将模型的训练过程分为四个步骤，分别是： 输入处理模块 (X 输入数据，变成网络能够处理的Tensor类型) 模型构建模块 (主要负责从输入的数据，得到预测的y^, 这就是我们经常说的前向过程) 定义代价函数和优化器模块 (注意，前向过程只会得到模型预测的结果，只有当前模块负责自动求导和参数更新) 构建训练过程（迭代训练过程，就是下图表情包的训练迭代过程） 详细解释一、数据处理对于数据处理，最为简单的⽅式就是将数据组织成为⼀个 。但许多训练需要⽤到mini-batch，直接组织成Tensor不便于我们操作。pytorch为我们提供了Dataset和Dataloader两个类来方便的构建。 torch.utils.data.Dataset 继承Dataset 类需要override 以下⽅法： 12345678910from torch utils. data import Dataset class trainDataset（Dataset）： def init （self）： # constructor def getitem （self, index）： # 获得第⊥ndex号的数据和标签 def len（self）： # 获得数据量 torch.utils.data.DataLoader 1torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False) DataLoader Batch。如果选择shuffle = True，每⼀个epoch 后，都会对所有数据打乱重新排序 mini-Batch batch_size 常⻅的使⽤⽅法如下： 123dataLoader Dataloader(dataset, shuffle=True, batch size=32)for i, data in enumerate (dataloader, 0) x, y # data同时获得数据和标签 shuffle=true表示每次取一个batch的数据前，先进行排序。官方注解如下： shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False). 二、模型构建所有的模型都需要继承torch.nn.Module ， 需要实现以下⽅法： 12345678class MyModel(torch, nn.Module)： def init(self)： super(MyModel, self).__init__() def forward(self, x)： return model=MyModel() 其中forward() ⽅法是前向传播的过程。在实现模型时，我们不需要考虑反向传播。 三、 定义代价函数和优化器主要负责自动求导和参数更新，也就是反向传播部分 12criterion = torch.nn.BCELOSS(reduction='sum') # 代价函数optimizer = torch.optim.Adam(model, parameters(), lr=0.0001, betas=(0.9，0.999), eps=1e-08, weight_decay=0, amsgrad=False) # 优化器 其中反向传播的基础：计算图，如下所示： 由图可以看出，全局偏导是由局部偏导根据链式法则，连续相乘而得出的 四、构建训练过程12345678def train(epoch)：#一个epoc的训练 for i, data in enumerate(dataLoader, 0): x,y = data #取出 minibatch数据和标签y pred = mode1(x) #前向传播 loss = criterion(y_pred,y) #计算代价函数 optimizer.zero_grad（）#清零梯度准备计算 1oss.backward（）#反向传播 optimizer.step（）#更新训练参数 引用[1] 推荐给大家！Pytorch编写代码基本步骤思想_哈工大SCIR实验室的大佬们写的 [2] 河北工业大学刘洪普老师的视频教程","link":"/2020/08/01/pytorch-learning-1/"},{"title":"datawhale-零基础入门NLP-Task1","text":"题目理解目标是用多种思路完成天池的NLP新手级题目零基础入门NLP - 新闻文本分类，题目的大意是训练一个模型，对不同的文本段进行分类，分成财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐这十四个类（记作0-13），其中会对输入的文本段做匿名化处理，如下所示： 流程划分由于题目给出的训练用的文本段是匿名化的，无法进行分词操作，将整个比赛流程划分成特征提取和分类模型两个方面，因此在后续任务中会通过四种思路来完成这一题目，分别是： TF-IDF + 机器学习分类器 TF-IDF用于对文本提取特征 FastText FastText是入门款的词向量，由facebook提供 WordVec + 深度学习分类器 WordVec是进阶款的词向量 Bert词向量 Bert是高配款的词向量 测评指标评价标准为类别f1_score的均值，结果越大越好。 对于评测标准的理解评测标准的本质是用来筛选模型的好坏的，而模型的好坏在不同的条件下有不同的标准，因此评测标准也应该是多样的。 在本题中模型的作用是预测一段文本的类别，细化而言，即对于一段文本，判断其是否属于14类中的某一类（例如第3类），如果是模型预测是第3类就是positive，模型预测不是第3类就是negative，再结合实际情况，一共得到四类结果，如下图所示： 在实际中有很多种计算方式： Precision： 公式：$$Precision =\\frac{ True Positive }{ True Positive + False Positive }$$ 理解：这一标准关注的是对于某一类，在所有预测为positive的文本中，在实际中也是positive（即属于true positive）的概率 用途：Precision is a good measure to determine, when the costs of False Positive is high.（当分类器将一个实际negative的样本识别为positive会产生巨大损失时，即对false positive敏感，例如非垃圾邮件被识别成垃圾邮件） Recall： 公式：$$ Recall =\\frac{True Positive}{True Positive+False Negative}$$ 理解：这一标准关注的是对于某一类，在所有实际中为positive的文本中，在预测时也是positive（即属于true positive）的概率，也可以理解成模型对正样例的捕获能力 用途：Recall shall be the model metric we use to select our best model when there is a high cost associated with False Negative.（用于当一个正样例被分类错误会带来巨大损失的模型，即对false negative敏感，例如对强传染疾病的分类，将患病病人识别成不患病的） Accuracy： 公式：$$\\mathrm{Accuracy}=\\frac{\\text { True Positive }+\\text { True Negative }}{ \\text { (True Positive }+\\text { False Positive }+\\text { True Negative }+\\text { False Negative })}$$ 理解：计算的是所有样本中分类正确（不论正样例还是负样例）占算有样本的比例 用途：It is most used when all the classes are equally important.（用于所有分类结果都一样重要时） F1_score： 公式：$$\\mathrm{F} 1=\\left(\\frac{\\text { Recall }^{-1}+\\text {Precision }^{-1}}{2}\\right)^{-1} =2 \\times \\frac{ Precision*Recall} {Precision+Recall}$$ 理解：This is the harmonic mean of Precision and Recall and gives a better measure of the incorrectly classified cases than the Accuracy Metric（属于Precision和Recall的平衡点，即对Precision和Recall计算调和平均数） 对于F1_score和Accuracy的比较： Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes as in the above case. In most real-life classification problems, imbalanced class distribution exists and thus F1-score is a better metric to evaluate our model on. Purva HuilgolAccuracy vs. F1-Score 参考[1] Datawhale零基础入门NLP赛事 - Task1官方文档 [2] Accuracy, Precision, Recall or F1? [3] Accuracy vs. F1-Score [3] 图片引用自这里","link":"/2020/07/21/datawhale-NLP-t1/"},{"title":"datawhale-零基础入门NLP-Task4","text":"学习目标不同于之前使用传统的机器学习方法，此次任务使用深度学习方法fastText来解决文本分类问题 文本表示方法（part 2）之前的博客讨论了文本的表示方法： TF-IDF Bag of Words (AKA Count Vectors) 通过观察，我们可以发现它们存在一定的缺陷：每一段文本被表示所需的向量维度很大，两种方法均是vocab-size大小的；导致训练模型所需要的时间很长 于是我们思考用一种更加简短的方式来表达一段文本，于是想到了fastText模型，与之前的文本表示方法不同，其基本思想是将一个单词通过深度学习模型表示在一个更小维度的向量空间，然后将整个句子的词向量求和——由于使用了更小维度的向量空间，训练和预测都会变得更快！ fastText模型简介fastText与word2vec、Glove均是对单词进行embedding的一种方法，本此任务我们先介绍fastText，其他两种之后会进行介绍 首先我们介绍embedding相比较与传统的文本表示方法（TF-IDF、Bag of Words）的优点：从task3我们可以知道，传统的文本表示方法形成的向量大小是vocab-size的，比较看重的是这段文本在每一个单词（或字）上的权重大小；而embedding会将一段文本中的每个单词映射到一个更小的空间中，使得每一个维度都有其意义（比如一个维度表示颜色，另一个维度表示大小之类的），因此展示在向量空间中，文本向量的距离可以用来衡量文本间的语义相似程度 接下来，我们介绍fastText这一模型 fastText的核心思想是：使用character级别的N-gram（之前我们使用过单词级别的n-gram，即将一段文本中连续的n个单词组成一个新的词语），而这里使用的是字符级的N-gram，即将一个单词中连续的n个字符拆出来，形成一个行的单词，具体的例子为：对于单词“where”，在n=3时，形成的单词为：&lt;wh, whe, her, ere, re&gt;以及where，因此从某种角度而言，字符级别的N-gram也是扩充词库的一种重要方法 fastText与word2vec（下一篇博客中会使用）相比，有如下优点： 在word2vec中，我们并没有直接利用构词学中的信息。无论是在Skip-Gram模型还是Continuous Bag of Words (CBOW)模型中，我们都将形态不同的单词用不同的向量来表示。例如，“dog”和“dogs”分别用两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。鉴于此，fastText提出了子词嵌入（subword embedding）的方法，从而试图将构词信息引入word2vec中的跳字模型 ….与Skip-Gram模型相比，fastText中词典规模更大，造成模型参数更多，同时一个词的向量需要对所有子词向量求和，继而导致计算复杂度更高。但与此同时，较生僻的复杂单词，甚至是词典中没有的单词，可能会从同它结构类似的其他词那里获取更好的词向量表示 李沐动手学深度学习 用fastText方法进行文本分类的模型架构如下所示： 代码实现 环境配置： 为了完成本次实验，需要首先安装安装包：我用的是conda下配置的python环境，直接使用会报错，说需要安装Microsoft Visual C++ 14.0的依赖；因此推荐离线安装，具体推荐参考这里 具体代码实现如下： 1234567891011121314import pandas as pdfrom sklearn.metrics import f1_score# 转换为FastText需要的格式train_df = pd.read_csv('../data/train_set.csv', sep='\\t', nrows=15000)train_df['label_ft'] = '__label__' + train_df['label'].astype(str)train_df[['text','label_ft']].iloc[:-5000].to_csv('train.csv', index=None, header=None, sep='\\t')import fasttextmodel = fasttext.train_supervised('train.csv', lr=1.0, wordNgrams=2, verbose=2, minCount=1, epoch=25, loss=\"hs\")val_pred = [model.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[-5000:]['text']]print(f1_score(train_df['label'].values[-5000:].astype(str), val_pred, average='macro')) 引用[1]","link":"/2020/07/27/datawhale-NLP-t4/"},{"title":"datawhale-零基础入门NLP-Task3","text":"学习目标此次任务的目标是：在不同的文档表示方法下，使用传统机器学习算法来完成文本分类任务 文本表示方法(part1) 由于机器学习模型的输入只能是数字，而不是文本；所以在实现文本分类任务时，我们首先应该考虑的是如何将一段文本向量化 Bag of Words (AKA Count Vectors)用双城记来举例子展示我们是如何计算Bag of Words的 It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, 假设这四句话每一句都是一段文本，这四个文本共同组成了一个语料库 设计词典： 通过对以上的文本进行分析，我们可以得出语料库中一共出现了如下10个词语 1it、was、the、best、of、times、worst、age、wisdom、foolishness 计算出这段文本的向量表示，即将向量的长度设计为10，每一维度对应某个文本中一个单词出现的次数，即可以得到 1234\"It was the best of times\" = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\"it was the worst of times\" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\"it was the age of wisdom\" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\"it was the age of foolishness\" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1] 对这一算法的直观理解是：词汇分布越相似的文本，其含义越相近 TF-IDF根据这一算法的名字，我们将分成TF、DF、IDF三个小部分来介绍 我们的基本思想是：寻找一些词语，用这些词语来代表一个文本；这一算法多用在搜索引擎等技术中，用于搜索和待检查词语匹配的文章 根据日常经验，我们的基本思路是：寻找在一个文本中出现频率最高的几个词来代表这段文本，但是要排除停顿词，如英语中的“a, the, is, was”等 TF全名是Term Frequency，即词频。因为词频最能直观反映一篇文章的keywords，比如一篇写猫的文章，其中“猫”这个字的出现的次数一定很多。 由于有些文本是长文本，有些是短文本；就算同是写猫的文本，长文本中“猫”出现的次数一定多于短文本中“猫”出现的次数，但是却不能表明长文本中与“猫”的相关度比短文本强。因此我们在这里需要进行正则化，即： 1tf(t,d) = count of t in d / number of words in d DF全名是Document Frequency，即文档频率。这个词可不是它的字面意思哈！当我们计算一个单词A的DF时，实际上我们是在计算语料库中包含单词A的文档的数量。其公式如下： 1df(t) = occurrence of t in documents 我们之所以要计算DF，是为了用到它的倒数IDF，DF表明了一个单词的普遍程度，DF的值越大，表明当前单词在大部分的文章都出现过，十分普遍，不具备代表意义 IDF全名是Inverse Document Frequency，即逆文档频率，它就是DF的倒数。其含义已在DF中说过了，这里就说一下计算公式吧： $$\\operatorname{idf}(\\mathrm{t})=\\log (\\mathrm{N} /(\\mathrm{df}+1))$$ 之所以+1，是因为防止除数为0（在搜索系统中，待查询的词可能没在文本中出现过）；之所以取log，是因为防止数据爆炸（value explode） 综上， 一个词语的TF和IDF与这个词语在一段文本中是否具有代表性是正相关的，因此TF*IDF与这个词语也是正相关的；在实际文本中一般选择TF*IDF中最大的头几个词来代表文本 在本题中会将文本转化成一个长度固定为vocab_size的向量，每一维度对应着相应下标的单词的TF*IDF值 Trick: N-gram通过对以上两种算法的学习，我们发现这两种算法的需要构建词典来进行词频统计，在构建词典时，我们可以使用N-gram算法来扩充我们的词典，让词典中的一些词更有意义。N-gram是将连续的N个词组成一个新的词语来看待，基本思想将文本内容按照字节顺序进行大小为N的滑动窗口操作，最终形成长度为N的字节片段序列 举例而言，对于“It was the best of times”这句话使用2-gram（bigram）来分析，会在词典中增加这些单词： 1“it was”、“was the”、“the best”、“best of”、“of times” 以后在统计词频时，会将以上算作是一个单词来看待。 机器学习模型本次调用的是Ridge Classifier分类模型，是由岭回归（Ridge Regression）变化而来的，我们接下来先介绍岭回归。 线性回归用如下所示的线性模型，去拟合数据 $$\\hat{y}=w[0] \\times x[0]+w[1] \\times x[1]+\\ldots+w[n] \\times x[n]+b$$ 为了检测拟合结果，引入了代价函数（cost function），训练时代价越高，模型效果越差 $$\\sum_{i=1}^{M}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}=\\sum_{i=1}^{M}\\left(y_{i}-\\sum_{j=0}^{p} w_{j} \\times x_{i j}\\right)^{2}$$ 可能出现的结果： 训练集cost 测试集cost 模型效果 大 大 欠拟合（under fitting） 小 小 效果好 小 大 过拟合（over fitting） 岭回归（Ridge Regression） 本质是在线性回归的基础上加入了L2正则化 进行拟合的函数是不变的，变化的是代价函数（cost function）的计算方法 $$\\sum_{i=1}^{M}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}=\\sum_{i=1}^{M}\\left(y_{i}-\\sum_{j=0}^{p} w_{j} \\times x_{i j}\\right)^{2}+\\lambda \\sum_{j=0}^{p} w_{j}^{2}$$ 相当于在减小cost function的同时需要满足如下条件： $$\\text { For some } c&gt;0, \\sum_{j=0}^{p} w_{j}^{2}&lt;c$$ 即对模型中的参数的大小都进行了一定程度的抑制，通过$\\lambda$的值来控制抑制程度；$\\lambda$越接近0，就越接近普通的线性回归 注意：只能用来解决过拟合问题，不能用来筛选特征 Lasso回归 本质是在线性回归的基础上加入了L1正则化 $$\\sum_{i=1}^{M}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}=\\sum_{i=1}^{M}\\left(y_{i}-\\sum_{j=0}^{p} w_{j} \\times x_{i j}\\right)^{2}+\\lambda \\sum_{j=0}^{p}\\left|w_{j}\\right|$$ 相当于在减小cost function的同时需要满足如下条件： $$\\text { For some } t&gt;0, \\sum_{j=0}^{p}\\left|w_{j}\\right|&lt;t$$ 可以解决的问题： 筛选特征，会将无用的特征前的权重赋值为0 防止过拟合 ==对于欠拟合模型，适当减少$\\lambda$ ，并增加迭代次数，有助于减小cost的值== Ridge Classifier 本质是将Ridge Regression转化为一个多输出的回归，预测类对应回归的最高值输出 代码实现具体代码如下所示： Count Vectors + RidgeClassifier 12345678910111213141516import pandas as pdfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.linear_model import RidgeClassifierfrom sklearn.metrics import f1_scoretrain_df = pd.read_csv('../data/train_set.csv', sep='\\t', nrows=15000)vectorizer = CountVectorizer(max_features=3000)train_test = vectorizer.fit_transform(train_df['text'])clf = RidgeClassifier()clf.fit(train_test[:10000], train_df['label'].values[:10000])val_pred = clf.predict(train_test[10000:])print(f1_score(train_df['label'].values[10000:], val_pred, average='macro')) TF-IDF + RidgeClassifier 12345678910111213141516import pandas as pdfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.linear_model import RidgeClassifierfrom sklearn.metrics import f1_scoretrain_df = pd.read_csv('../data/train_set.csv', sep='\\t', nrows=15000)tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=3000)train_test = tfidf.fit_transform(train_df['text'])clf = RidgeClassifier()clf.fit(train_test[:10000], train_df['label'].values[:10000])val_pred = clf.predict(train_test[10000:])print(f1_score(train_df['label'].values[10000:], val_pred, average='macro')) 引用[1] Datawhale零基础入门NLP赛事 - Task3官方文档 [2] A Gentle Introduction to the Bag-of-Words Model [3] TF-IDF from scratch in python on real world dataset. [4] Ridge and Lasso Regression: L1 and L2 Regularization","link":"/2020/07/25/datawhale-NLP-t3/"},{"title":"线性回归——pytorch-learning（二）","text":"线性模型假设我们的线性模型的格式为： $$\\hat{y}=x * \\omega$$ 先确定优化目标，也就是cost function，我们算则MSE（Mean Square Error） $$\\cos t=\\frac{1}{N} \\sum_{n=1}^{N}\\left(\\hat{y}_{n}-y_{n}\\right)^{2}$$ 12345678910111213141516171819202122232425262728import numpy as npimport matplotlib.pyplot as pltx_data = [1.0, 2.0, 3.0]y_data = [2.0, 4.0, 6.0]# 前向传播def forward(x):return x * w# 计算loss，评价模型def loss(x, y):y_pred = forward(x)return (y_pred - y) * (y_pred - y)w_list = []mse_list = []for w in np.arange(0.0, 4.1, 0.1): #对参数进行穷举 print('w=', w) l_sum = 0 for x_val, y_val in zip(x_data, y_data): y_pred_val = forward(x_val) loss_val = loss(x_val, y_val) l_sum += loss_val print('\\t', x_val, y_val, y_pred_val, loss_val) print('MSE=', l_sum / 3) w_list.append(w) mse_list.append(l_sum / 3) 注：可以用visdom进行实时绘图，对数据进行可视化 梯度下降算法我们将寻找使目标函数值最小的参数的问题称为优化问题，为了代替对于参数的遍历方法，引入梯度下降算法 $$\\omega=\\omega-\\alpha \\frac{\\partial \\cos t}{\\partial \\omega}$$ 更新公式的方法 123456789101112131415161718192021222324252627x_data = [1.0, 2.0, 3.0]y_data = [2.0, 4.0, 6.0]w = 1.0def forward(x): return x * wdef cost(xs, ys): cost = 0 for x, y in zip(xs, ys): y_pred = forward(x) cost += (y_pred - y) ** 2 return cost / len(xs)def gradient(xs, ys): grad = 0 for x, y in zip(xs, ys): grad += 2 * x * (x * w - y) return grad / len(xs)print('Predict (before training)', 4, forward(4))for epoch in range(100): cost_val = cost(x_data, y_data) grad_val = gradient(x_data, y_data) w -= 0.01 * grad_val print('Epoch:', epoch, 'w=', w, 'loss=', cost_val)print('Predict (after training)', 4, forward(4)) 关于梯度下降，以上使用的是普通的梯度下降：即用所有数据的平均损失值来进行梯度更新；我们也可以使用SGD（Stochastic Gradient Descent）：即随机使用一个数据来更新梯度。SGD的好处是：单个样本包含了随机噪声，这样就避免停留在鞍点，缺点是：时间效率太低；因此现在主流是每次将训练数据分为过个batch，没经过一个batch进行一次数据更新 反向传播本质是构建计算图，以两层神经网络为例： $$\\hat{y}=W_{2}\\left(W_{1} \\cdot X+b_{1}\\right)+b_{2}$$ 其计算图为： pytorch基础 Tensor：一般针对每一个权重或中间变量使用一个tensor，其内部组成如下：包含权重本身和梯度（也是一个Tensor）两个部分 注意： 每进行一次反向传播，计算图就会被释放！ Tensor在做加法运算时会构建计算图；如过不进行backward()，就一直不会释放； Tensor中的梯度在.backward()方法执行后会被累加，所以每次在更新完梯度后都需要，将梯度清零 在进行计算时应使用w.grad.data，而不是w.grad（Tensor），否则会产生新的计算图 12345678910print(\"predict (before training)\", 4, forward(4).item())for epoch in range(100): for x, y in zip(x_data, y_data): l = loss(x, y) l.backward() #释放计算图，累加梯度 print('\\tgrad:', x, y, w.grad.item()) w.data = w.data - 0.01 * w.grad.data #更新梯度 w.grad.data.zero_() #梯度清零 print(\"progress:\", epoch, l.item())print(\"predict (after training)\", 4, forward(4).item()) 用pytorch实现线性回归123456789101112131415161718import torch# 准备数据x_data = torch.Tensor([[1.0], [2.0], [3.0]])y_data = torch.Tensor([[2.0], [4.0], [6.0]])# 设计计算图（根据上一篇）class LinearModel(torch.nn.Module): # 通过继承moudle，自动计算backword def __init__(self): super(LinearModel, self).__init__() self.linear = torch.nn.Linear(1, 1) # Class nn.Linear contain two member Tensors: weight and bias def forward(self, x): y_pred = self.linear(x) return y_predmodel = LinearModel() 关于torch.nn.Linear，官方文档如下： 优化目标函数MSE如下所示： 1criterion = torch.nn.MSELoss(size_average=False) 其中官方文档为： 迭代优化器如下所示： 12# 不继承于moudle，不会构建计算图optimizer = torch.optim.SGD(model.parameters(), lr=0.01) 通过model.parameters()，提取模型中所有需要更新的参数 接下来写训练过程 12345678for epoch in range(100): y_pred = model(x_data) loss = criterion(y_pred, y_data) print(epoch, loss) optimizer.zero_grad() loss.backward() optimizer.step() #更新 输出与测试 12345678# Output weight and biasprint('w = ', model.linear.weight.item())print('b = ', model.linear.bias.item())# Test Modelx_test = torch.Tensor([[4.0]])y_test = model(x_test)print('y_pred = ', y_test.data) 引用[1] 河北工业大学刘洪普老师的视频教程","link":"/2020/08/01/pytorch-learning-2/"},{"title":"卷积神经网络——pytorch-learning（四）","text":"图像形成简述一般图像分为两类： 栅格图像：图像由像素点组成，一般可以由人工捕获，在相机摄像时，每个像素点都通过一个光学元件来采集——放大后变成马赛克 矢量图像：一般无法人工捕获，由程序生成，相当于一段描述：圆心在哪，半径多少….，一般由程序画出来，而不是现成的——放大不变形 卷积神经网络简述使用DNN（全连接神经网络）来进行图像分类时，是将二维图像完全展开成一维图像，只保留了像素间横向的联系，却忽略了纵向的联系。 因此我们引入了卷积神经网络，提取其二维特征，其工作过程是通过黄色的卷积核来提取绿色的图像中的二维特征，随后将特征输出到粉色矩阵中，以便进行下一步的处理。单通道卷积的运算过程如下所示： 对于卷积核作用的直观理解：检测图像的局部是否满足某一特征，如果满足，则返回一个较大的值；否则返回一个较小的值 注意：对于多通道的图像，每次卷积是针对所有的通道来进行的，卷积核应该是三维的，第三个维度的大小应该和通道数相同，因此在讨论卷积核的大小时，只考虑二维的大小。如下图所示： 展开后具体如下： 压缩后如下所示（卷积时不加padding）： 泛化图像的尺寸，以及投影到多个卷积核之后可以得到： 注意：卷积核的数目与输出结果的通道数相同 卷积核的相关参数 其中： m是卷积核的个数 n是输入图像的通道数 padding在图像外层填充0，借此来保证经过卷积计算后的图形与原图形大小保持一致。一般而言，外围填充的0的圈数为 $$\\frac{kernel_size}{2}$$ 如下图所示： stride在进行卷积运算时，每一次移动卷积核的距离 123456789101112131415161718192021import torchin_channels, out_channels= 5, 10width, height = 100, 100kernel_size = 3batch_size = 1input = torch.randn(batch_size, in_channels, width, height)conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size)# padding参数# conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1, bias=False)# stride参数# conv_layer = torch.nn.Conv2d(1, 1, kernel_size=3, stride=2, bias=False)# 可以选择自己初始化卷积的权重# kernel = torch.Tensor([1,2,3,4,5,6,7,8,9]).view(1, 1, 3, 3)# conv_layer.weight.data = kernel.dataoutput = conv_layer(input)print(input.shape)print(output.shape)print(conv_layer.weight.shape) Pooling可以理解成下采样，用来筛选特征，压缩图像大小。但是特征筛选只在同一个通道内进行；经过pooling之后图像的通道数不改变；在pooling层中： 1234567891011121314- Max Pooling：每次取局部内的最大值```pythonimport torchinput = [3,4,6,5,2,4,6,8,1,6,7,8,9,7,4,6,]input = torch.Tensor(input).view(1, 1, 4, 4)maxpooling_layer = torch.nn.MaxPool2d(kernel_size=2)output = maxpooling_layer(input)print(output) 代码示例在此处我们使用MNIST数据集作为演示，具体网络架构如下： 注意：卷积层和池化层不在意图像的大小，但是线性层在意，需要提前计算 123456789101112131415161718class Net(torch.nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5) self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5) self.pooling = torch.nn.MaxPool2d(2) self.fc = torch.nn.Linear(320, 10) def forward(self, x): # Flatten data from (n, 1, 28, 28) to (n, 784) batch_size = x.size(0) x = F.relu(self.pooling(self.conv1(x))) x = F.relu(self.pooling(self.conv2(x))) x = x.view(batch_size, -1) # flatten x = self.fc(x) return xmodel = Net() 对于GPU的使用： 123456789101112131415161718192021# 选择设备device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")# 迁移模型model.to(device)# 迁移数据def train(epoch): running_loss = 0.0 for batch_idx, data in enumerate(train_loader, 0): inputs, target = data # 这里 inputs, target = inputs.to(device), target.to(device) optimizer.zero_grad() # forward + backward + update outputs = model(inputs) loss = criterion(outputs, target) loss.backward() optimizer.step() running_loss += loss.item() if batch_idx % 300 == 299: print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 2000)) running_loss = 0.0 卷积神经网络进阶 以复杂网络GoogLeNet为例子 对Inception模块进行解释 1*1的卷积核 作用： 将所有输入通道同一像素点的数值进行加权 改变通道的数目，相比于其他大小的卷积，1*1的卷积在改变通道数目时，计算量较小。对比如下： 代码实现 注意：张量的维度表示方式为（B,C,W,H），因此torch.cat操作时dim=1 卷积的深度及其效果 结论：并不是卷积越深，效果越好 解决方法：使用残差网络（Residual net） 残差网络（Residual net） 搭建网络的建议 逐层搭建，逐层测试，主要测试网络的输入与输出的矩阵的大小 引用[1] 河北工业大学刘洪普老师的视频教程 [2] He K, Zhang X, Ren S, et al. Identity Mappings in Deep Residual Networks [3] Huang G, Liu Z, Laurens V D M, et al. Densely Connected Convolutional Networks","link":"/2020/08/01/pytorch-learning-4/"},{"title":"chap 1-Boolean retrieval","text":"布尔检索模型 目标：在目标语料库中查询出现某些词和不出现某些词的文章，即通过具有精确语义的逻辑表达式来构建查询 词项-文档关联矩阵 倒排序索引 收集需要建立索引的文档 将文章词条化 词条归一化 建立倒排序索引 存储方式：词典在内存，倒排记录表在磁盘 查询优化 改变处理交集的次序来提升处理速度，一般根据每个词语的文档频率来作为改变合并次序的原则：两个两个合并，从小到大逐渐合并 对基本布尔操作的扩展 临近操作符：限制待查询的两个词之间的距离；即“/s”表示位于同一个句子中，”/k”表示距离K个词之内，”/p”表示位于同一个段落中 短语查询 布尔检索模型的改进方向 对返回结果进行排序 对短语进行搜索，而不是单个词 对于词典内的单词，能够容忍不同的形式和错拼 引入词频统计来验证搜索的可信度","link":"/2020/09/07/information-retrieval-ch1/"},{"title":"chap 3-Dictionaries and tolerant retrieval","text":"本章主要讲解了如何对倒排索引进行优化，分为三个部分：对于词典部分的查询进行优化、对通配符查询的处理、拼写校正的问题以及基于发音的校正技术 词典搜索的数据结构 哈希表 容易产生冲突问题，尤其是词汇表不断变大的情况下 搜索树——二叉树、B树 二叉树：注意二叉树的平衡问题 B树：适用于部分词典常驻磁盘的情况 通配符查询使用B树和反向B树相结合单一通配符：对于单词lemon，其中反向B树中的路径就是 root-n-o-m-e-l；对于查询se*mon，可以通过B-树来返回所有前缀为se且后缀非空的词项子集W，再通过反向B-树来返 回所有后缀为mon且前缀非空的词项子集R，然后，对W和R求交集W ∩ R 多通配符：通过穷举法检查返回的集合中的每个元素 轮排索引 此方法是对于正反向B树的改进，即只使用一棵B树 单一通配符：我们在字符集中引入一个新的符号$，用于标识词项结束；对于单词hello的轮排索引的结果如下所示： 处理通配符时的应用：考虑通配符查询 m*n，这里的关键是将查询进行旋转让*号出现 在字符串末尾，即得到 n$m*，此时便可以用正向B树来解决 多通配符：通过穷举法检查返回的集合中的每个元素 K-gram索引 k-gram表示的是将每个单词按照k个字母来划分，分成字母片段对于 castle 来说，所有的 3-gram包括$ca、cas、ast、stl、tle 及 le$ 本方法直接适用于多通配符，此时词汇表变为k-gram形式，而每个倒排记录表则由包含该k-gram的词项组成，而每个单词还会对应一个文章ID的倒排记录表 拼写校正 用于解决在用户输入拼写错误的查询词时，能检测到并还原为正确的查询词，最终返回正确查询词的查询结果。 拼写校正的实现对于一个拼写错误的查询，在其可能的正确拼写中，选择“距离”最近或者邻近度最小的那个。在距离和邻居度相同的情况下，选择其他用户查询最频繁的 校正方法 词项独立的校正方法 在对查询进行分词之后，只对单个查询词进行校正，此时很难检测到flew form Heathrow中的错误 编辑距离法 编辑距离的定义：将字符串s1转换为字符串s2所需要的最小编辑操作数（删除一个字符、替换一个字符、增加一个字符） 一般通过动态规划来计算，其算法描述为： K-gram法 原理：正确的目标查询词为待匹配词项包含查询 q（拼写错误） 中某个固定数目的 k-gram 即可 举例：假定我们想返回 bord 的 3 个 2-gram 中的至少 2 个词项，对倒排记录表的单遍扫描会返回满足该条件的所有词项，本例当中，这些词项包括 aboard、boardroom 及 border 混合法 首先使用 k-gram 索引返回可能是 q 的潜在正确拼写形式的词项集合，然后计算该集合中的每个元素和 q 之间的编辑距离并选择 具有较小距离的那些词项。 上下文敏感的拼写校正尝试对短语中的每个词进行替换。比如对于上面 flew form Heathrow 的例子，我们 可能会返回如下短语 fled from Heathrow 和 flew fore Heathrow。对每个替换后的短语，搜索引擎 进行查找并确定最后的返回数目。 基于发音的校正技术原理：：（1）在名称转录时，元音是可以互换的；（2）发音相似的辅音字母归为一类。这就会导致相关的名称通常有相同的 soundex 编码结果 方法：使用soundex编码方法","link":"/2020/09/08/information-retrieval-ch3/"},{"title":"Logistic回归与多分类问题——pytorch-learning（三）","text":"Logistic回归基本概念Logistic回归是将实数范围映射的值到[0,1]的范围内，虽然是回归模型，但是常被用作分类任务，理解成是否属于某一类别的概率，其映射又公式称为sigmoid函数，如下： $$\\sigma(x)=\\frac{1}{1+e^{-x}}$$ 优化的目标函数也应该相应做出改变，改为使用交叉熵函数cross-entropy，其含义是计算两个概率分布之间的差异性，越小越好，具体公式展示如下： $$H(p, q)=\\sum_{i} p\\left(x_{i}\\right) \\log \\frac{1}{\\log q\\left(x_{i}\\right)}=-\\sum_{i} p\\left(x_{i}\\right) \\log q\\left(x_{i}\\right)$$ 在二分类问题中，具体应用如下： $$\\operatorname{los} s=-(y \\log \\hat{y}+(1-y) \\log (1-\\hat{y}))$$ 代码展示123456789101112131415161718192021222324252627import torch.nn.functional as Fx_data = torch.Tensor([[1.0], [2.0], [3.0]])y_data = torch.Tensor([[0], [0], [1]])#-------------------------------------------------------#class LogisticRegressionModel(torch.nn.Module): def __init__(self): super(LogisticRegressionModel, self).__init__() self.linear = torch.nn.Linear(1, 1) def forward(self, x): y_pred = F.sigmoid(self.linear(x)) return y_pred model = LogisticRegressionModel()#-------------------------------------------------------#criterion = torch.nn.BCELoss(size_average=False)optimizer = torch.optim.SGD(model.parameters(), lr=0.01)#-------------------------------------------------------#for epoch in range(1000): y_pred = model(x_data) loss = criterion(y_pred, y_data) print(epoch, loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() 多维特征值问题假设输入8维，输出1维 12345678910111213141516171819class Model(torch.nn.Module): def __init__(self): super(Model, self).__init__() ## 输入特征4维，输出特征1维 self.linear = torch.nn.Linear(4,2) ## 此时对同一组2维数据中的每一个取sigmoid self.sigmoid = torch.nn.Sigmoid() def forward(self, x): x1 = self.linear(x) print(x1) x2 = self.sigmoid(x1) print(x2) return x2model = Model()x_data = torch.Tensor([[1.0,2.0,3.0,4.0],[1.5,2.5,3.5,4.5]])y_predic = model(x_data) 结果如下： 多层神经网络如上所示，在多层神经网络中，一般利用torch.nn.Linear进行升维或降维，即： 将上图的神经网络实现成代码，如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243## 数据读取import numpy as npimport torch# 读取成numpy格式，注意一般gpu只接受32位，分隔符按照数据的内容自己选定xy = np.loadtxt('diabetes.csv.gz', delimiter=',', dtype=np.float32)# from_numpy可以自动转化为Tensor格式x_data = torch.from_numpy(xy[:,:-1])y_data = torch.from_numpy(xy[:, [-1]])## 设计网络class Model(torch.nn.Module): def __init__(self): super(Model, self).__init__() self.linear1 = torch.nn.Linear(8, 6) self.linear2 = torch.nn.Linear(6, 4) self.linear3 = torch.nn.Linear(4, 1) self.sigmoid = torch.nn.Sigmoid() def forward(self, x): x = self.sigmoid(self.linear1(x)) x = self.sigmoid(self.linear2(x)) x = self.sigmoid(self.linear3(x)) return x model = Model()## 构造损失和优化函数## 进行迭代for epoch in range(100): # Forward y_pred = model(x_data) loss = criterion(y_pred, y_data) print(epoch, loss.item()) # Backward optimizer.zero_grad() loss.backward() # Update optimizer.step() 以上代码有两个问题： 每个epoch计算的是全部的数据，而不是mini-batch的 目前的代码均只使用了sigmoid作为激活函数 接下来我们尝试解决这两个问题： 首先，用ReLu替换sigmoid作为激活函数： 123456789101112131415161718import torchclass Model(torch.nn.Module): def __init__(self): super(Model, self).__init__() self.linear1 = torch.nn.Linear(8, 6) self.linear2 = torch.nn.Linear(6, 4) self.linear3 = torch.nn.Linear(4, 1) self.relu = torch.nn.ReLU() self.sigmoid = torch.nn.Sigmoid() def forward(self, x): x = self.relu(self.linear1(x)) x = self.relu(self.linear2(x)) x = self.sigmoid(self.linear3(x)) return xmodel = Model() 注意：使用ReLu时，为防止最后的输出值被用来计算ln，所以最后一层用sigmoid作为激活函数 接下来，我们研究如何利用mini-batch来加载数据集 数据加载问题主要工具理解主要使用Dataset和DataLoader这两个工具，其中： Dataset：负责根据下标取出数据 DataLoader：对Dataset进行包裹，负责取出mini-batch大小的数据 涉及的概念： iteration：表示1次迭代（也叫training step），每次迭代更新1次网络结构的参数，所需的数据量为1个batch的大小 batch-size：1次迭代所使用的样本量 epoch：1个epoch表示过了1遍训练集中的所有样本（包含多个batch的数据） 若总的数据量为1000，batch的大小为100，则每一次经过一次epoch，需要通过10次iteration 代码展示： 123456789101112131415161718192021import torch## 注意Dataset为抽象类，而DataLoader不是from torch.utils.data import Datasetfrom torch.utils.data import DataLoaderclass DiabetesDataset(Dataset): def __init__(self): ## 1.一次加载所有数据，按下标存取 ## 2.只加载文件名 pass def __getitem__(self, index): ## 如果初始化时读取的是文件名，则这里开始根据文件名读入文件 pass def __len__(self): passdataset = DiabetesDataset()train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2) num_workers的含义：读取数据的并行进程的数目 对于参数shuffle的理解： 注意：windows和linux系统上训练数据的差别，因为两个系统实现多线程的方式不同，windows使用spawn来代替fork windows： 1234567train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2)……# 不能让for循环直接打头if __name__ == '__main__': for epoch in range(100): for i, data in enumerate(train_loader, 0): …… linux： 12345train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2)……for epoch in range(100): for i, data in enumerate(train_loader, 0): …… Dataset的实现（以加载全部数据为例）12345678910111213141516171819202122232425262728293031323334class DiabetesDataset(Dataset): def __init__(self, filepath): xy = np.loadtxt(filepath, delimiter=',', dtype=np.float32) self.len = xy.shape[0] self.x_data = torch.from_numpy(xy[:, :-1]) self.y_data = torch.from_numpy(xy[:, [-1]]) def __getitem__(self, index): ## 返回下标元组 return self.x_data[index], self.y_data[index] def __len__(self): return self.lendataset = DiabetesDataset('diabetes.csv.gz')train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2)......## 使用数据加载器后的训练流程的变化for epoch in range(100): ## 此处使用mini-batch，batch-size为32，每次迭代只使用一个数据 for i, data in enumerate(train_loader, 0): # 1. Prepare data（dataLoader会自动将数据转化为Tensor） inputs, labels = data # 2. Forward y_pred = model(inputs) loss = criterion(y_pred, labels) print(epoch, i, loss.item()) # 3. Backward optimizer.zero_grad() loss.backward() # 4. Update optimizer.step() 多分类问题 softmax分类器在多分类问题中，我们期望的输出时当前数据属于各个类别的概率分布，因此我们期望它满足如下的条件： $$P(y=i) \\geq 0$$ $$\\sum_{i=0}^{n} P(y=i)=1$$ 因此，人们提出了如下的模型： $$P(y=i)=\\frac{e^{z_{i}}}{\\sum_{j=0}^{K-1} e^{z_{j}}}, i \\in\\{0, \\ldots, K-1\\}$$ 注意：在pytorch中，Cross-Entropy Loss与negative Log Likelihood Loss之间的区别 引用在pytorch论坛中看到的一句话： If you apply Pytorch’s CrossEntropyLoss to your output layer, you get the same result as applying Pytorch’s NLLLoss to a LogSoftmax layer added after your original output layer. 对比示意图如下所示（均会自动将类别标签转化成one-hot向量）： 代码验证： 1234567891011121314151617181920import torch# Y是长整型的张量Y = torch.LongTensor([2, 0, 1])Y_pred1 = torch.Tensor([[0.1, 0.2, 0.9],[1.1, 0.1, 0.2],[0.2, 2.1, 0.1]])Y_pred2 = torch.Tensor([[0.8, 0.2, 0.3],[0.2, 0.3, 0.5],[0.2, 0.2, 0.5]])# criterion1 = torch.nn.CrossEntropyLoss()l1 = criterion1(Y_pred1, Y)l2 = criterion1(Y_pred2, Y)print(\"Batch Loss1 = \", l1.data, \"\\nBatch Loss2=\", l2.data)#actFunction = torch.nn.Softmax()r1 = torch.log(actFunction(Y_pred1))r2 = torch.log(actFunction(Y_pred2))criterion2 = torch.nn.NLLLoss()r1 = criterion2(r1, Y)r2 = criterion2(r2, Y)print(\"Batch Loss1 = \", r1.data, \"\\nBatch Loss2=\", r2.data) 结果如下： 引用[1] 河北工业大学刘洪普老师的视频教程","link":"/2020/08/01/pytorch-learning-3/"},{"title":"chap 5-Index compression","text":"压缩索引的目的：1.增加cache的利用率；2.加速数据从磁盘到内存的速度 本章介绍的压缩技术均为无损压缩，即压缩之后所有的原始信息都被保留；大小写转换、词干还原、和停用词剔除均是有损压缩技术 信息检索中词项的统计特性 Heaps定律：对于词汇表大小的估计 Zipf定律：对词项分布的建模 词典压缩 使用变长字符串 查询表中只存指针，每个词项均对应一个指针 按块存储 块间二分查找，块内遍历（块内K个词项），减少指针数目 k越大，压缩率越高；但是查找效率越低 倒排记录表的压缩 倒排记录表中的内容为文档的序号 主要采用：1.可变字节码；2.Y编码这两种压缩方式 可变字节编码利用整数个字节对同一单词的倒排记录表中的相邻文档间的间距进行编码，字节的后七位是间距的有效编码区，第一位是延续位（如果该位为1，表示结尾；否则不是） 举例如下： Y编码Y编码的组成：将间距G表示成长度和偏移两个部分进行变长编码，G的偏移实际上是G的二进制编码，但是前端的1被去掉；eg：对 13（二进制为 1101）进行编码，其偏移为 101。偏移的长度为3位，长度部分采用一元编码（一开始有连续个1，最后以0结尾，1的个数表示长度）。即为1110. 举例如下：","link":"/2020/09/11/information-retrieval-ch5/"},{"title":"chap 4-Index construction","text":"基于块的排序索引方法（BSBI） 此处主要使用的算法是外部排序算法和归并算法 主要步骤为： 将文档集分割成几个大小相等的部分（所谓的块） 将每个部分的词项 ID—文档 ID 对排序（将每个词项映射为ID） 将中间产生的临时排序结果存放到磁盘中 将所有的中间文件合并成最终的索引 特点： 需要构建词项-词项ID的映射表 所有的索引结果都需要经过排序 single-pass in-memory indexing（SPIMI）相比于BSBI，其优点为： 不需要构建词项-词项ID的映射表，节约内存 索引结果不需要排序，节约时间 算法描述： 最后的内存块的合并过程此处省略，与BSBI相同。 特点：将词典直接存入每一个内存块中 分布式索引构建方法采用MapReduce框架，其图示如下所示： Map 阶段：将输入的数据片映射成键—值对。这个映射过程对应于 BSBI 和 SPIMI 算法中的分析任务，因此也将执行 Map 过程的机器称为分析器（parser） Reduce阶段：将同一键（词项ID）的所有值（文档ID）集中存储，以便快速读取和处理。实现时，将所有的键按照词项区间划分成j个段，并将属于每个段的键—值对写入各自分区文件即可 动态索引构建方法 周期性重建索引：周期性地对文档集从头开始进行索引重构。如果随时间的推移 文档更新的次数不是很多，并且能够接受对新文档检索的一定延迟，再加上如果有足够的资源 能够支持在建立新索引的同时让旧索引继续工作，那么周期性索引重构不失为一种较好的选择 添加辅助索引：要求能够及时检索到新文档，一个是大的主 索引，另一个是小的用于存储新文档信息的辅助索引，后者保存在内存中。检 索时可以同时遍历两个索引并将结果合并。文档的删除记录在一个无效位向量中，在返回结果之前可以利用它过滤掉已删除文档。某篇文档的更新通过先删除后重新 插入来实现。 其他检索类型 排序式检索系统（ranked retrieval）：相比于布尔检索系统，不按照文档ID排序，仅仅按照权重或者影响程度排序 安全性检索系统：添加用户访问权限，构建ACL（access control list，访问控制表）","link":"/2020/09/09/information-retrieval-ch4/"},{"title":"chap 2-The term vocabulary and postings lists","text":"Document delineation and character sequence decoding根据不同的编码方式，将字节序列转化为字符序列 文本的序列化处理 将文档和查询使用相同的方法转化为词条 关键术语的区分 token： tokenization is the task of chopping it up into pieces, called tokens； type： A type is the class of all tokens term containing the same character sequence term: A term is a (perhaps normalized) type that is included in the IR system’s dictionary. example: For example, if the document to be indexed is to sleep perchance to dream, then there are five tokens, but only four types (because there are two instances of to). However, if to is omitted from the index (as a stop word) , then there are only three terms: sleep, perchance, and dream. 需要解决的问题 分词问题： 英语：对于空格和连字符的处理 中文：无法通过空格来分词，词边界不明显 德语：对复合词进行拆分 去除停用词：通过文档集频率来选择停用词，频率越大说明越不具有特殊性（现代搜索系统的影响并不大）——一般先分词，后去除停用词 term normalization：将看起来不完全一致的多个词条归纳成一个等价类，如：anti-discriminatory和antidiscriminatory均映射成antidiscriminatory，一般的方法是构建同义词表，归一化的方法如下： 合并同义词表中多个词的查询结果 构建索引时，便对词进行扩展 词干还原（stemming）与词形归并（lemmatization） 词干还原：利用启发式规则去除两端前缀 词形归并：利用词汇表和词形分析 基于skip pointers的倒排记录表快速合并算法 只适用于AND操作，不适用于OR操作 算法图示： 算法描述： 跳表指针的设置的位置： 每隔$$\\sqrt{P}$$均设置一个跳表指针，指向下一个跳表指针的位置，P是跳表的长度 短语查询的解决方法 目前一般采用如下两种方式的混合 二元词索引 将文档中的每个连接词对看成一个短语 “stanford university palo alto”会被转化成“stanford university” and “university palo” and “palo alto” 扩展的二元词对：先进行词性标注，再将一个多词序列看成一个扩展的二元词 位置信息索引 在每个倒排记录中，存储该词在文本中出现的位置 举例如下：","link":"/2020/09/07/information-retrieval-ch2/"},{"title":"datawhale-零基础入门NLP-Task5","text":"Introduction使用fastText之外的深度学习模型，例如word2vec、TextCNN、TextRNN，来完成文本分类任务 模型学习首先我们先学习一下我们即将使用的三种模型 word2vecTextCNN TextCNN利用CNN（卷积神经网络）进行文本特征抽取，不同大小的卷积核分别抽取n-gram特征，卷积计算出的特征图经过MaxPooling保留最大的特征值，然后将拼接成一个向量作为文本的表示。 TextRNN TextRNN利用RNN（循环神经网络）进行文本特征抽取，由于文本本身是一种序列，而LSTM天然适合建模序列数据。TextRNN将句子中每个词的词向量依次输入到双向双层LSTM，分别将两个方向最后一个有效位置的隐藏层拼接成一个向量作为文本的表示。 Reference[1] Datawhale零基础入门NLP赛事 - Task5-word2vec官方文档 [2] Datawhale零基础入门NLP赛事 - Task5-TextCNN官方文档 [3] Datawhale零基础入门NLP赛事 - Task5-TextRNN官方文档","link":"/2020/07/31/datawhale-NLP-t5/"},{"title":"chap 6-Scoring, term weighting, and the vector space model","text":"参数化索引及域索引 元数据：指的是和文档有关的一些具有特定形式的数据，通常包含字段和数值两部分 域数据：同字段的意义相似 图示如下： 另一种实现方式： 在域索引的基础上添加权重假定每篇文档有 l 个域，其对应的权重分别是$$g_{1}, \\ldots, g_{l} \\in [0,1]$$，且满足$$\\sum_{i=1}^{l} g_{i}=1$$，其中$$s_{i}$$是查询文档和某个域的匹配得分（若匹配，值为1；否则，值为0），则一篇文档的查询得分为$$\\sum_{i=1}^{l} g_{i} s_{i}$$ 对于权重的学习考虑一个简单的域加权评分的例子，其中每篇文档只包含 title 和 body 两个域。给定查询 q和文档d，根据 title 及 body 域是否和 q 匹配，利用布尔匹配函数分别计算出布尔变量$$s_{T}(d, q)$$和$$s_{B}(d, q)$$，主要接下来要确定g的值： $$\\operatorname{socre}(d, q)=g \\cdot s_{T}(d, q)+(1-g) s_{B}(d, q)$$ 词项频率及权重计算 目前只考虑了词项在文档中出现与否的情况，未考虑词项出现的频率。不同于之前的权重计算方法， 我们认为：如果一个查询词在文档中出现的频率越高，所应该赋予的权重就越大 此处，我们引入词项频率（TF），我们使用词袋模型（直接将出现的次数作为权重），即不在乎词的位置，只在乎出现的次数 我们认为：并不是所有词的重要性都是一样的，应该赋予不同的词以不同的权重（根据它在所有文档中出现的频繁程度，越频繁价值越低） 此处，我们引入逆文档频率（IDF），其中对于文档频率（DF）的定义为：词项在文档集中出现的次数（同一文档出现多次算做一次）；而IDF与DF的关系如下： $$i d f_{t}=\\log \\frac{N}{d f_{t}}$$ 我们通常将TF和IDF结合起来计算一个文档与查询之间的相关度，即：$$\\operatorname{tf-idf}{t, d}=\\operatorname{tf}{t, d} \\times \\mathrm{idf}_{t}$$ 当查询由多个词组成时，tf和idf用向量来表示，结果用内积的形式来表示；具体可见之前的一篇博客 空间向量模型一组文档的集合可以看成向量空间中的多个向量，每个词项 对应一个坐标轴。这种表示忽略了词项在文档中的相对次序。即在这种模型表示下，文档 Mary is quicker than John 和 John is quicker than Mary 是等价的 利用向量空间进行文档相似度的计算此处使用余弦相似度这一计算模型： $$\\operatorname{sim}\\left(d_{1}, d_{2}\\right)=\\frac{\\vec{V}\\left(d_{1}\\right) \\cdot \\vec{V}\\left(d_{2}\\right)}{\\left|\\vec{V}\\left(d_{1}\\right) | \\vec{V}\\left(d_{2}\\right)\\right|}$$ 于是，将查找与 d 最相似的文档这个问题可以归结成寻找和d有最大内积结果的过程。在查询时，一般需要构建查询向量，即将查询文本视为一个短文档，并为它构建向量。 通过向量进行查询结果排序 用于给定查询，从文档集中返回得分最高的k篇文档 计算向量相似度的基本算法如下： 原理：对于每一个查询词项，对于其倒排序表中的每一个文档累加该词项的权重。最后比较所有参与过计算的文档的权重和，选出前K个文档。","link":"/2020/09/16/information-retrieval-ch6/"},{"title":"chap 10-XML retrieval","text":"XML文本的基本概念 XML文本的主要特点为：具有复杂的树形结构，属性之间还存在嵌套关系 XML文本举例： 转化为树形结构： XML DOM：将元素、属性以及元素内部的文本表示成树的节点 XPath: 是XML文档集中的路径表达式描述标准，也称为XML上下文。路径上前后元素间使用”/“来分割；”//“表示中间可以插入多个元素。eg：act/scene 表示选择所有父节点为 act 元素的scene元素；plan//scene表示选择出现在play元素下的所有scene 元素 XML检索中的挑战性问题 结构化检索中的挑战是用户希望返回文档的一部分（即 XML 元素），而不像非结构 化检索那样往往返回整个文档 选择最合适的文档部分的一个准则是：系统应该总是检索出回答查询的最明确最具体的文档部分，即返回信息需求的最小单位 该问题相对应的问题是：“对文档的哪些部分建立索引”，具体方法如下： 将节点分组 使用最大的元素作为索引单位，然后在最大的元素中寻找相关的子元素——自顶向下 先搜索最相关的子节点，然后扩展成更大的单位（父节点）——自底向上 对所有元素建立索引 由此产生的问题，即冗余性增大，同时元素间存在嵌套关系 解决方法：构造元素选择时的限制策略： 忽略所有的小元素 忽略用户不会浏览的所有元素类型（这需要记录当前 XML 检索系统的运行日志信息） 忽略通常被评估者判定为不相关性的元素类型（如果有相关性判定的话） 只保留系统设计人员或图书馆员认定为有用的检索结果所对应的的元素类型 对于剩余的冗余元素，将嵌套元素组合起来，并将查询词高亮显示来吸引用户关注相关段落 对于嵌套问题，还会引起词项统计信息的不准确，解决方法如下： 为XML的每个上下文-词项对计算idf，只考虑目标节点的父节点：比如 author#”Gates” 和 section#”Gates”","link":"/2020/09/19/information-retrieval-ch10/"},{"title":"chap 8-Evaluation in information retrieval","text":"信息检索的评价信息系统测试集的组成： 一个文档集 一组用于测试的信息需求集合，信息需求可以表示为查询（信息系统!=查询词） 一组相关性测试结果，对于每个查询-文档而言，赋予一个二值判断结果（相关、不相关） 对无序检索结果集合的评价正确率： $$\\text { Precision }=\\frac{\\text { 返回结果中相关文档的数目 }}{\\text { 返回结果的数目 }}=P(\\text { relevant } \\mid \\text { retrieved })$$ 召回率： $$\\text { Recall }=\\frac{\\text { 返回结果中相关文档的数目 }}{\\text { 所有相关文档的数目 }}=P(\\text { retrieved } \\mid \\text { relevant })$$ 精确率： $$\\text { Recall }=\\frac{\\text { 返回结果中真正例+正反例 }}{\\text { 所有被判断的文档的数目 }}$$ 精确率往往导致不准确的结果：绝大多数情况下，信息检索中的数据存在着极度的不均衡性，比如通常情况下，超过 99.9%的文档 都是不相关文档。这样的话，一个简单地将所有的文档都判成不相关文档的系统就会获得非常 高的精确率值，从而使得该系统的效果看上去似乎很好。而即使系统实际上非常好 正确率+召回率（F值）： $$F=\\frac{1}{\\alpha \\frac{1}{P}+(1-\\alpha) \\frac{1}{R}}$$ 可以通过调整$$\\alpha$$来控制正确率和召回率的权重 对有序检索结果的评价相比于无序检索结果，有序检索结果只对top-K个返回的结果进行处理 正确率-召回率曲线：随着K的增加，出现锯齿形图案 插值正确率：起到平滑的作用，具体做法为：对每一个Precision值，使用其右边最大的Precision值替代 11点插值平均正确率：对平滑后的Precision曲线进行均匀采样出11个点（每个点间隔0.1），然后计算这11个点的平均Precision $$A P=\\frac{1}{11} \\times\\left(A P_{r}(0)+A P_{r}(0.1)+\\ldots+A P_{r}(1.0)\\right)$$ 平均正确率均值MAP（Mean Average Precision）： 目前普遍使用，具有较好的稳定性和代表性 平均正确率AP：在每个相关文档位置上正确率的平均值 某个查询Q共有6个相关结果，某系统排序 返回了5篇相关文档，其位置分别是第1，第2，第5，第 10，第20位，则AP=(1/1+2/2+3/5+4/10+5/20+0)/6；其中1/1，2/2，3/5等就是平均正确率 平均正确率均值MAP：对一组查询的top-K个返回结果求平均正确率 $$\\operatorname{MAP}(Q)=\\frac{1}{|Q|} \\sum_{j=1}^{|Q|} \\frac{1}{m_{j}} \\sum_{k=1}^{m_{j}} \\operatorname{Precision}\\left(R_{j k}\\right)$$ 相关性判定在构建测试集时，需要： 设计用于测试的查询 需要判定文档的相关性 此处主要讨论判定文档的相关性，即考虑雇佣多个人来进行相关性判定，所需要做的是判定多个人之间的判定是否一致，采用kappa统计量，即： $$\\text {kappa}=\\frac{P(A)-P(E)}{1-P(E)}$$ 其中P(A)是观察到的一致性判断比率，p(E)是比较对象间的随机一致性比率，距离如下： reference[1] 白话mAP [2] 中科大课件","link":"/2020/09/16/information-retrieval-ch8/"},{"title":"chap 9-Relevance feedback and query expansion","text":"相关反馈及伪相关反馈 属于查询优化的局部方法，在一个查询的初始返回结果的基础上进行完善，使得再次返回的结果得到优化 相关反馈的主要思想：在信息检索的过程中通过用户交互来提高最终的检索效果。让用户来判断相关性 具体过程如下： 用户提交一个简短的查询 系统返回初次检索结果 用户对部分结果进行标注，将它们标注为相关或不相关 系统基于用户的反馈计算出一个更好的查询来表示信息需求 利用新查询系统返回新的检索结果 Rocchio 相关反馈算法基本原理：假定我们要找一个最优查询向量$\\vec{q}$ ，它与相关文档之间的 相似度最大且同时又和不相关文档之间的相似度最小 若$C_{r}$表示相关文档集，$C_{n r}$表示不相关文档集，那么我们希望找到的最优的$\\vec{q}$ 是： $$\\vec{q}_{o p t}=\\underset{\\vec{q}}{\\arg \\max }\\left[\\operatorname{sim}\\left(\\vec{q}, C_{r}\\right)-\\operatorname{sim}\\left(\\vec{q}, C_{n r}\\right)\\right]$$ 另一种定义为： $$\\vec{q}_{o p t}=\\frac{1}{\\left|C_{r}\\right|} \\sum_{\\bar{d}_{j} \\in C_{r}} \\vec{d}_{j}-\\frac{1}{\\left|C_{n r}\\right|} \\sum_{\\bar{d}_{j} \\in C_{m r}} \\vec{d}_{j}$$ 增加权重后为： $$\\vec{q}_{m}=\\alpha \\vec{q}_{0}+\\beta \\frac{1}{\\left|D_{r}\\right|} \\sum_{\\vec{d}_{j} \\in D_{r}} \\vec{d}_{j}-\\gamma \\frac{1}{\\left|D_{n r}\\right|} \\sum_{\\vec{d}_{j} \\in D_{n r}} \\vec{d}_{j}$$ 其中定义$C_{r}$为已知的相关文档集，$C_{n r}$为已知的不相关文档集 影响相关反馈的因素 初始查询时出问题： 拼写错误，可通过拼写校正技术来解决 跨语言IR 用户的词汇表和文档集的词汇表不同，比如laptop和notebook computer 相关反馈方法的使用条件： 理想条件下，所有相关文档中的词项分布应该与用户标出的相关文档中的词项分布相似，而同时所 有不相关文档中的词项分布与相关文档中的词项分布差别很大。如果相关文档包括多个不同子类，即它们在向量空间中可 以聚成多个簇，那么 Rocchio 方法效果会不太好 对于相关反馈策略的评价 首先计算出原始查询 $q_{0}$ 的正确率—召回率曲线，一轮相关反馈之后，我们计算出修改后的 查询 $q_{m}$ 并再次计算出新的正确率—召回率曲线。这样，反馈前与反馈后我们都可以在所有文档 上对结果进行评价，然后直接进行比较 利用剩余文档集（residual collection，所有文档集中除去用户判定的相关文档 后的文档集）对反馈后的结果进行评价。这种思路看上去更具现实性。不过，性能的度量结果 往往低于原始查询的结果。 伪相关反馈 伪相关反馈（pseudo relevance），也称为盲相关反馈（blind relevance feedback），提供了一 种自动局部分析的方法。它将相关反馈的人工操作部分自动化，因此用户不需要进行额外的交 互就可以获得检索性能的提升。 该方法首先进行正常的检索过程，返回最相关的文档构成初始集，然后假设排名靠前的 k 篇文档是相关的，最后在此假设上像以往一样进行相关反馈 间接相关反馈 在反馈过程中，我们也可以利用间接的资源而不是显式的反馈结果作为反馈的基础。这种 方法也常常称为隐式相关反馈（implicit relevance feedback）。 隐式反馈不如显式反馈可靠，但是 会比没有任何用户判定信息的伪相关反馈更有用 查询扩展 属于查询优化的全局方法，在不考虑查询及其返回文档情况下对初始查询进行扩展和重构的方法 主要思想是使用同义词词典对于查询词t进行自动扩展，其中同义词词典的构建方法共有3种，即： 简单辅助用户进行查询扩展 采用人工词典的方法 自动构建词典的方法","link":"/2020/09/19/information-retrieval-ch9/"},{"title":"ubuntu16.04安装cuda和cudnn","text":"cuda，cudnn之间的关系cuda安装安装之前的准备开始安装一般安装分为装驱动和装cuda两个部分，但是现在在安装cuda的时候会可以由我们选择为我们安装适配的驱动，所以这里选择在安装cuda时安装驱动，那么先去官网下载合适版本的cuda，这里我们安装cuda10.1，根据系统选择如下： 注意：安装前要看清自己的ubuntu版本号 卸载安装","link":"/2020/10/03/cuda-ubuntu-install/"},{"title":"无题","text":"我愿 升入青空 坠入深海 只愿 成长的忧伤 追不上我轻灵的魂魄","link":"/2020/10/25/poem/"},{"title":"chap 7-Computing scores in a complete search system","text":"以文档为单位的评分方法：每次计算一篇文档的得分 以词项为单位的评分方法：遇到每个词项时，得分能够逐渐累加 快速评分及排序 第六章介绍的是精确返回前K篇得分最高的文档的办法；此处我们开始关注非精确返回前K篇文档的方法，即前K篇返回的文档与最相关的K篇近似，但又不完全相同。同时用户感受不到返回的前K篇文档间的相关度有所降低，这样做的好处是可以降低运算的复杂度 此处介绍的算法主要包含如下的两个步骤： 找到一个文档集合 A，它包含了参与最后竞争的候选文档，其中$$K&lt;|A|&lt;&lt;N$$。A 不必包 含前 K 篇得分最高的文档，但是它应该包含很多和前 K 篇文档得分相近的文档 返回 A 中得分最高的 K 篇文档 索引去除技术 仅考虑查询中词项idf值超过一定阈值的单词所对应的倒排记录表 仅考虑包含多个（K个）查询词项的文档 胜者表胜者表（champion list），有时也称为优胜表（fancy list）或高分文档（top doc），它的基本 思路是，对于词典中的每个词项 t，预先计算出 r 个最高权重的文档，其中 r 的值需要事先给定。 对于 tf-idf 权重计算机制而言，词项 t 所对应的 tf 值最高的 r 篇文档构成 t 的胜者表 静态得分和排序很多搜索引擎中，每篇文档 d 往往都有 一个与查询无关的静态得分 g（d）。该得分函数的取值往往在 0 到 1 之间。比如，对于 Web 上 的新闻报道，g（d）可以基于用户的正面评价次数来定义。 可将静态得分和相似度组合得出每个文档的得分，即： $$\\text { net-score }(q, d)=g(d)+\\frac{\\vec{V}(q) \\cdot \\vec{V}(d)}{|\\vec{V}(q) | \\vec{V}(d)|}$$ 簇剪枝方法主要原理是对所有的文本使用聚类操作，聚类操作如下： 从 N 篇文档组成的文档集中随机选出$$\\sqrt{N}$$篇文档，它们称为先导者（leader）集合 对于剩余的$$N - \\sqrt{N}$$篇（称为追随者，follower）每篇不属于先导者集合的文档，计算离之最近的先导者 查询操作如下： 给定查询 q，通过与$$\\sqrt{N}$$个先导者计算余弦相似度，找出和它最近的先导者 L 候选集合 A 包括 L 及其追随者，然后对 A 中的所有的文档计算余弦相似度 信息检索系统的组成此处介绍信息检索系统中常用的一些概念，即： 层次型索引是对索引去除方法的一般化技术：例如，第 1 层索引中的 tf 阈值是 20， 第 2 层阈值是 10。这意味着第 1 层索引只保留 tf 值超过 20 的倒排记录，而第 2 层的记录只保 留 tf 值超过10的倒排记录。 查询词项的邻近性 对于检索中的查询，特别是Web上的自由文本查询来说，用户往往希望返回的文档中大部分或者全部查询词项之间的距离比较近，因为这表明返回文档中具有聚焦用户 查询意图的文本 考虑一个由两个或者多个查询词项构成的查询$$t_{1}, t_{2}, \\ldots, t_{k}$$，令文档中包含所有查询词项得最小窗口大小为$$\\omega$$，其取值为窗口内词的个数。如果文档中不包含所有的查询词项， 那么此时可以将ω设成一个非常大的数字。直观上讲，ω的值越小，文档d和查询匹配程度更高，即可以根据ω的大小来设计权重 查询分析及文档评分函数的设计 查询分析：将自由文本查询通过查询分析器，转化成带操作符的查询，比如对于rising interest rates的查询的分析如下： 查询rising interest rates这一短语 如果太少，转而分别查询 rising interest 和 interest rates 两个查询短语 如果太少，转而分别查询 rising ，interest 和 rates 这三个查询短语 评分函数的设计：必须融入向量空间计算、静态得分、邻近度加权或其他因素 搜索系统的组成","link":"/2020/09/16/information-retrieval-ch7/"},{"title":"对于Bert模型的学习","text":"[toc] BERT之前ELMo的提出ELMo属于一种word embeding，由于同一个单词可能有不同的含义，因此对于同一个单词应该有多重向量来表示。具体的应用在于，先观察整个句子，然后再为每个单词生成相应的embeding。 具体而言是将整个句子的初始embeding过一遍预训练的Bi-LSTM（language modeling）——本质上而言，也是在句子单侧的编码，为每个单词生成相应的embeding。如下图所示： 随后将这些隐藏层（包括初始的embeding）通过每种方式结合在一起 最后得到相应的embeding，如上图湖蓝色所示 ULM-FiT：明确的将迁移学习引入NLPULM-FiT介绍了一个语言模型和一种将语言模型进行微调以适应各种其他任务的方法 Transformer：超过LSTMs在处理长序列方面的能力超过了LSTM OpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling将Transformer的Decoder改造为可以与训练的语言模型，The OpenAI Transformer通过7000本书来实现预训练 迁移至下流任务 BERTBERT：From Decoders to Encoders 目标：改变Transformer模型，使其能够学习句子双侧的序列信息 使用Transformer的多层Encoder部分（而不是Decoder部分）来学习句子中词向量的表示，提出BERT的论文的全名是BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding，从其名字可以看出模型的特点：Pre-training、Deep、Bidirectional、Transformer、Language Understanding 由于语言模型的特点是：已知前N个词来预测第N+1个词，因此以往的语言模型均是单向学习的语言模型。而BERT提出遮蔽语言模型（masked language model，MLM），即在训练时随机MASK掉15%的单词，对于一个被MASK的单词： 有80%的概率用“[mask]”标记来替换 有10%的概率用随机采样的一个单词来替换 有10%的概率不做替换（虽然不做替换，但是还是要预测哈） 具体模型如下： 即进行多层Transformer中的Encoder的堆叠，论文中的模型参数为： BERTBASE (L=12, H=768, A=12, Total Parameters=110M) BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M). BERT模型的训练特点共分为两步进行训练： 对于被MASK的词汇进行预测：进行语言模型的学习 进行下一句预测：学习句子间的关系 其中[seq]为句子分隔符，每次输入两个句子，训练模型用来判断第二个句子是否是第一个句子的下一个 此后得到的模型为预训练的模型，可以作为后期词向量embeding的生成器 BERT模型的应用Sentence Classification 在句子前面加入[CLS]标致，通过最后一层输出的[CLS]的向量表示来进行句子分类。因为该向量已经学得了整个句子的信息。因此像RNN一样不用考虑其他位置单词的信息。直接判断即可 Reference[1] The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) [2] NLP的游戏规则从此改写？从word2vec, ELMo到BERT [3] NLP必读：十分钟读懂谷歌BERT模型 [4] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","link":"/2020/10/18/learning-Bert/"},{"title":"对于transformer的学习与认识","text":"[toc] 整体架构 encoders中多个堆叠的encoder之间不共享参数 encoder的个数与decoder的个数相同 其中每个encoder的内部构造如下所示： self-encoder的作用，帮助编码器在编码特单词时留意其他的单词 其中decoder和encoder之间的连接如下： transformer的主要优点在于其并行计算的能力，主要在于feed forward层面 通过self-attention之后的词向量，分别经过相同的feed forward network（参数相同）因此可以并行 Encoder解读self-attention的作用机制对于句子“The animal didn’t cross the street because it was too tired.”，利用self-attention可以将it与the animal产生关联，即通过句子中的其他词更好的解释当前词 Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing. self-attention详解 此处介绍的是Encoder中的self-attention，不同于Decoder 对于其中一层的self-attention而言： 从向量X到向量Z的计算步骤为： 初始化三个权重矩阵$$ W^Q $$，$$ W^K $$，$$ W^V $$（这三个权重矩阵在训练过程中参数会得到训练，在代码中表示为经过了三个线性层），分别将单词的embedding与这三个矩阵相乘，得到每个embedding对应的Query，key，value向量 这一步的主要内容是来计算得分，即当前词和所输入句子中其他词相关性的得分。计算方法为：假设当前单词所处的位置为#1，计算其与#2位置的单词的相关性的方法为：用$$q_1$$和$$k_2$$进行点乘。举个栗子而言：假设输入的句子为“thinking machines”，其计算如下： 将得分除以8（key vector维度的平方根，paper中定义为64维），目的是为了维持更稳定的梯度。 将当前词与每个单词的得分通过softmax函数处理 根据#1，#2位置的softmax的值，计算加权的#1，#2位置单词对应的结果向量。self-attention执行完毕，接下来将向量送入前馈网络即可 multi-headed attention机制详解 本质是每次计算self-attention模块时，使用多组Query/Key/Value权重矩阵（Transformer使用的是8个），每一组矩阵代表一个头 执行结果如下： 由于feedforword模块只需要输入一个矩阵，因此执行如下操作： 多头注意力机制的可视化：我们可以看出每个attention关注的侧重点不同 对于单词的embedding加入位置和序列信息 之前值注重于考虑词与词之间的关联性信息，对于输入序列的单词在编码时没有考虑其位置信息，在此处加以处理 通过向量，表示出位置和序列信息，与原有信息进行加和。具体如下图所示： 残差连接其可视化结果如下，在进行残差连接后，经过一个LayerNorm层 关于LayerNorm和BatchNorm之间的区别 Decoders解读 Decoders中含有和Encoders中数量相同的encoder，每一个Decoder的结构包含三层，分别是：self-attention，Encoder-Decoder attention，Feed forward层 其图示如下： Decoders中每一个Decoder的输出会传入上一层作为输入；第一层decoder之前也是加入位置信息的embeding层，如下图所示： Decoders是自回归的，意味着每次Decoders均只预测一个单词，每一次预测的输出都会累加起来重新作为对于下一次预测的输入（此处可以将Decoders看做一个RNN，具体如下图所示）——在输出停止符号&lt;end of sentence&gt;后停止迭代 比如：输入&lt;start&gt;得到“I” Decodrs的输入 Decoders的输出 &lt;start&gt; I &lt;start&gt; I am &lt;start&gt; I am fine &lt;start&gt; I am fine &lt;end&gt; Decoder中的self-attention不同于之前介绍的encoder中的self-attention，只对序列中的前面的向量计算softmax，而对后面的向量进行mask操作 Decoder和Encoder之间的合作 对于Encoder-Decoder attention的解读Encoder-Decoder attention的本质还是self-attention，分别将encoder输出的序列经过线性变换得到k和v序列，将第一层经过self-attention层输出的向量作为q向量序列 对于最后的Linear and Softmax Layer的解读 主要作用在于将输出的向量转化为对应的单词 Linear Layer是一个全连接网络，用于将Decoders输出的向量映射到词汇表相应的维度，得到logits向量（即进行归一化前的对应到各个词的可能得分） Softmax layer则将各个维度对应的得分映射到 (0,1] 之间的概率，对应概率最高的维度对应的单词即为目标单词 参考及图片来源[1] Illustrated Guide to Transformers- Step by Step Explanation [2] The Annotated Transformer harvard的源码解读 [3] The Illustrated Transformer","link":"/2020/10/13/learning-transformer/"},{"title":"新闻推荐现有工作的调研(持续更新)","text":"新闻推荐的特点 具有较强的时效性，相比于其它推荐任务，不能使用MF方法，可用的用户交互数据较少 新闻用于往往简洁、准确。便于使用NLP模型进行处理 新闻中包含很多实体，往往成为文章的keyword 本文所包含的论文汇总： 题目 时间 会议 是否包含时间序列 是否包含知识图谱 KRED 2020 RecSys 是 KNI 2020 DLP-KDD 是 DKN 2018 WWW 是 KRED: Knowledge-Aware Document Representation for NewsIntro正如文章的名字Knowledge-Aware Document Representation for News Recommendations，这篇文章更注重的是创建一个位于任务上游的news representation，为位于下游的新闻推荐子任务提供类似BERT在NLU领域的工具。 创新点认为现存的文档理解模型要么是没有考虑知识图谱信息，比如BERT；要么是依赖于特殊的文章编码模型，以至于缺乏泛化能力和效率，比如DKN。本篇文章将知识图谱与BERT相结合 模型整体模型分为三层，分别是： Entity Representation Layer：使用Knowledge Graph Attention (KGAT) Network，将知识图谱中当前实体周围实体的信息汇聚与当前实体，用于增强对于当前实体的表示。其具体公式如下： $$\\mathbf{e}_{\\mathcal{N}_{h}}=\\operatorname{ReLU}\\left(\\mathbf{W}_{0}\\left(\\mathbf{e}_{h} \\oplus \\sum_{(h, r, t) \\in \\mathcal{N}_{h}} \\pi(h, r, t) \\mathbf{e}_{t}\\right)\\right)$$ 其中Nh表示以h为头部实体的三元组，$\\pi(h, r, t)$表示注意力权重，用来控制周围节点传播多少信息量到中心节点，其计算方式如下： $$\\pi_{0}(h, r, t)=\\mathbf{w}_{2} \\operatorname{Re} L U\\left(\\mathbf{W}_{1}\\left(\\mathbf{e}_{h} \\oplus \\mathbf{e}_{r} \\oplus \\mathbf{e}_{t}\\right)+\\mathbf{b}_{1}\\right)+b_{2}$$ $$\\pi(h, r, t)=\\frac{\\exp \\left(\\pi_{0}(h, r, t)\\right)}{\\sum_{\\left(h, r^{\\prime}, t^{\\prime}\\right) \\in \\mathcal{N}_{h}} \\exp \\left(\\pi_{0}\\left(h, r^{\\prime}, t^{\\prime}\\right)\\right)}$$ Context Embedding Layer：为每一个实体向量加入位置、频率、类别这三种信息，通过wise-to-wise的直接相加 Information Distillation Layer：使用类似self-attention的机制来得到news representation 随后，为了提升这个上游模型的性能，对该模型进行Multi-Task的学习（包含Category Classification、Popularity Prediction、Local News Detection、Item Recommendation、Item-to-item Recommendation），一共分为两个阶段： stage1：we alternately train different tasks every few mini-batches. stage2：we only include the target task’s data to finalize a task-specific model. 对于知识图谱的使用其中对于知识图谱的使用也是在news representation的阶段，处于整体模型的第一层 KNI: An End-to-End Neighborhood-based Interaction Model for Knowledge-enhanced RecommendationDKN: Deep Knowledge-Aware Network for News Recommendation","link":"/2020/10/29/news-recommend-papers/"},{"title":"实体消歧(Entity Disambiguation)","text":"为了将新闻标题中的实体与知识图谱中的实体相对应，我们需要先进行实体识别（NER），再进行实体与知识图谱的映射。在这个过程中受导师指点，需要考虑实体消歧（Entity Disambiguation）的问题。因此进行调研。 对于实体消歧问题的描述在一段文本中，由于文本和语言的多态性，一个语言字段在不考虑上下文的情况下可能对应多个实体，为了将文中的实体与知识图谱中相应的实体正确对应，提出了实体消歧问题。 注：在文本中需要与知识图谱中的entity相对应的文字段称为mention 实体链接的步骤一共分为两个步骤： 对于每一个mention产生一个候选实体集（candidate），实体集中的实体全部来自于知识图谱 对于候选实体集中的实体进行实体消歧，通过排序选出相关度最高的那个实体作为mention的对应实体 相关论文 题目 时间 会议 Collective Entity Linking in Web Text: A Graph-Based Method 2011 SIGIR NeuPL 2017 CIKM Pair-Linking for Collective Entity Disambiguation: Two Could Be Better Than All 2018 TKDE Collective Entity Linking in Web Text: A Graph-Based Method创新点设计了全局协同推理方法Collective Entity Linking，如下图例子所示： 即认为单独只依赖上下文standout career at Bulls, [] also acts in the movie无法直接推断出Jordan就是Michael Jordan，需要借助其他的mention对应的entity，即Chicago Bulls and Space Jam来进行推断，此种方法称为协同实体链接。 方法对于文本During his standout career at Bull, Jordan also acts in the movie Space Jam. 构建Referent Graph，如下所示： 图中包含两种关系，即 Local Mention-to-Entity Compatibility: mention与entity之间的连接 Semantic Relation between Entities: entity之间的连接 NeuPL: Attention-based Semantic Matching and Pair-Linking for Entity DisambiguationIntro本篇论文所需要解决的两个问题是： 充分利用上下文信息来消除歧义 增强linked entity之间的一致性 创新点目标函数： $$\\Gamma^{*}=\\underset{\\Gamma}{\\arg \\max }\\left[\\sum_{i=1}^{N} \\phi\\left(m_{i}, t_{i}\\right)+\\psi(\\Gamma)\\right]$$ 本文将目标函数拆成两个部分（模型）进行优化，即： local model 作用及含义：根据mention的context来推断mention与哪一个entity相对应，计算得到Local confidence or local score $\\phi\\left(m_{i}, t_{i}\\right)$反映了$m_{i} \\longmapsto t_{i}$的概率 现存的方法：用DNN对mentions的context进行建模，此种方法存在一些问题，没有充分利用local context的信息，即： 上下文中可能包含多个mention，DNN无法确定应该关注于哪一个mention 忽视了上下文中单词间的顺序 本文的改进： 通过两个LSTM来学习目标mention两侧的上下文信息 同时为了消除上下文的噪音以及学习对于不同mention的权重，引入了attention机制来提取信息 具体模型如下所示： global model 作用及含义：在一篇文章中通过多个实体间的关系来确定待确定的实体的对应 传统的方法： 构建整篇文章的指代实体图（Referent Graph），传统的collective entity linking方法在图中实体个数过多时会消耗过多的计算时间。 同时，也不是一篇文章中的所有实体都可以进行相互推断，比如The Sun and The Times reported that Greece will have to leave the Euro soon. 这句话中的关系如下： 观察发现，并不是所有实体间都有联系 本文的改进：论文中提出了Pair-Linking (PL)算法，特点是，只使用一次迭代便可以完成一篇文章中所有的mentions之间的collective linking，具体的做法见下面这篇文章中介绍 Pair-Linking for Collective Entity Disambiguation: Two Could Be Better Than All 2-4行：计算任意两个mention对应的的candidate entity set之间的最小语义相似距离（构建图中的边） 7-9行：类似Kruskal算法，每次选取confident最高（语义相似距离最小）的边 10-13行：一旦一个candidate entity set中的entity被选中，则此set中的其他entity被从图中抹去，更新剩余的集合间语义距离 可以图示如下： 算法停止时间：所有的实体集中都有一个实体被选中，不需要形成最小生成树","link":"/2020/11/01/Entity-Disambiguation/"},{"title":"推荐系统的评测标准","text":"hit@k定义如下： In a nutshell, it is the count of how many positive triples are ranked in the top-n positions against a bunch of synthetic negatives. 具体计算方法为： In the following example, pretend the test set includes two ground truth positive only: 12Jack born_in ItalyJack friend_with Thomas Let’s assume such positive triples (identified by * below) are ranked against four synthetic negatives each. Now, assign a score to each of the positives and its synthetic negatives using your pre-trained embedding model. Then, sort the triples in descending order. In the example below, the first triple ranks 2nd, and the other triple ranks first (against their respective synthetic negatives): 12345678910111213s p o score rankJack born_in Ireland 0.789 1Jack born_in Italy 0.753 2 *Jack born_in Germany 0.695 3Jack born_in China 0.456 4Jack born_in Thomas 0.234 5s p o score rankJack friend_with Thomas 0.901 1 *Jack friend_with China 0.345 2Jack friend_with Italy 0.293 3Jack friend_with Ireland 0.201 4Jack friend_with Germany 0.156 5 Then, count how many positives occur in the top-1 or top-3 positions, and divide by the number of triples in the test set (which in this example includes 2 triples): 12Hits@3= 2/2 = 1.0Hits@1= 1/2 = 0.5 ndcg@k全名Normalized Discounted Cumulative Gain，是一个测量排序质量的指标。我们将会分成三个步骤来介绍，即： Cumulative Gain(CG) Discounted Cumulative Gain(DCG) Normalized Discounted Cumulative Gain(NDCG) CG每一个待推荐的item都有一个相关性的得分，所有相关性得分总和即为CG，对于推荐系统给出的已排序（按照推荐算法）序列A，其中相关性得分（与推荐系统的推荐顺序无关）和CG分别如下所示： DCG对于推荐系统给出的两个有序推荐序列A和B，虽然B的结果比A的要好（按相关性从大到小进行排序），但是按照CG进行评测两者表现效果相同，因此单纯依靠CG存在很大的缺陷 因此我们提出DCG，希望能够测量推荐系统能否将待推荐的item按照相关性的降序进行排列，其公式如下：$$D C G=\\sum_{i=1}^{n} \\frac{2^{r e l e v a n c e_{i}}-1}{\\log {2}(i+1)}$$或者$$D C G=\\sum{i=1}^{n} \\frac{\\text {relevance}_{i}}{\\log _{2}(i+1)}$$其中第一个公式对于具有较高相关性但在推荐序列中排名较后的item具有很大的惩罚，可以作为对于推荐系统排序能力进行测量的工具。对于前面举例子的集合A，B，其DCG的得分如下： 如果相关性用0/1来表示，则两个公式的效果相同 NDCG由于不同推荐系统对于不同用户给出的待推荐序列的长度不同，相关度的范围不同，因此DCG无法作为一个通用的衡量标准，为了使其通用化，我们引入了NDCG，对于每一个推荐系统生成的序列，其计算方法如下： 按照推荐系统给出的顺序计算DCG 按照相关性排序的顺序计算DCG，带到iDCG 求出两者的比率DCG/iDCG，取值范围应该在0,1之间 举例如下，对于推荐系统给出的序列： 按照相关性排序可以得到： 两个序列DCG分别计算如下： 因此该序列NDCG为： referrence[1] How is hits@k calculated and what does it mean in the context of link prediction in knowledge bases [2] Evaluate your Recommendation Engine using NDCG","link":"/2020/11/06/performance-measure-of-resys/"}],"tags":[{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"datawhale","slug":"datawhale","link":"/tags/datawhale/"},{"name":"Hexo,Icarus","slug":"Hexo-Icarus","link":"/tags/Hexo-Icarus/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"cuda","slug":"cuda","link":"/tags/cuda/"},{"name":"deep learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"news recommendation","slug":"news-recommendation","link":"/tags/news-recommendation/"},{"name":"entity disambiguation","slug":"entity-disambiguation","link":"/tags/entity-disambiguation/"}],"categories":[{"name":"Datawhale学习手记","slug":"Datawhale学习手记","link":"/categories/Datawhale%E5%AD%A6%E4%B9%A0%E6%89%8B%E8%AE%B0/"},{"name":"博客搭建","slug":"博客搭建","link":"/categories/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"},{"name":"pytorch-learning","slug":"pytorch-learning","link":"/categories/pytorch-learning/"},{"name":"information retrieval","slug":"information-retrieval","link":"/categories/information-retrieval/"},{"name":"环境配置","slug":"环境配置","link":"/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"},{"name":"随想","slug":"随想","link":"/categories/%E9%9A%8F%E6%83%B3/"},{"name":"recsys","slug":"recsys","link":"/categories/recsys/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"}]}