{"pages":[{"title":"about","text":"WelcomeGlad to see you here. I will write down my insights about DL,NLP,Rsys in this place. Education 2015.09-2019.06 Software Engineering, Northwestern Polytechnical University 2020.09 Going to study at the National Cyber Security College of Wuhan University Research InterestDL，NLP，Recommend System Contect Email:","link":"/about/index.html"}],"posts":[{"title":"Hexo+Icarus博客搭建记录","text":"个人感受我是一名前端小白，完全是跟着网上的教程走的，首先记录一下个人在看教程时的感受： 对整体架构不是很清楚，只能跟着教程一步一步摸索着走，遇到问题再google 找教程时要先看发布的时间，像Hexo和Icarus更新的挺快的，较早的教程是不适合现在的 不用跟着一个教程走到黑，可以看适合自己的部分，多找几个教程对比验证，就可以找到比较合适的实现方式了 网上关于这部分的教程说的已经很好很详细了，我就没必要造重复造轮子了，针对我自己（大神请忽略。。）一开始摸不着头脑的情况，我想先列出一个整体框架，让大家清楚自己每走一步都在那个阶段，并清楚下一阶段该做什么，我觉得这是比较有帮助的 整体架构这个blog搭建在github上的，使用了适用于个人blog的框架Hexo，Hexo的主题(theme)可由用户自定义(可以类比android的手机主题)，很多大神开源了很多主题，其中我选择的是Icarus，戳这里看demo 以上便是整体框架，接下来按照此可以划分出三步流程： 整体流程 在github上申请Repositories，并设置其为浏览器可访问的网页形式，具体教程可参照godweiyang在知乎的回答，PS：godweiyang自己改版了matery主题也很不错，大家也可以去看看他的主页，如果喜欢就可以一路跟着他在知乎的教程啦~ 安装并配置Hexo，具体教程同样可参照godweiyang在知乎的回答 安装好后，默认使用landscape主题，不过你现在已经可以尝试在此主题下写博客啦~PS：Hexo本身提供了标签、文章、归档等所有博客该有的功能，下一步的主题选择不影响当前编辑的内容的 此时需要先停下来，不着急进行下一步，先阅读Hexo的官方文档，PS：很短，官方文档是最直接了解这个框架功能的方式~，先了解一下怎么发文章，图片放在哪个目录下 在Hexo的基础上配置Icarus主题，完整过一遍Icarus的文档，建议阅读顺序如下： Icarus用户指南 - 主题配置对整体网站的配置 Icarus用户指南 - 挂件对于侧边栏的配置 随后运行整个网站，你会发现还有一些地方在报错，接下来就配置这些插件部分 做一些个性化改进： 图床配置，我是使用的腾讯云的COS服务，一开始会赠送6个月的服务，教程见这里 配置live2d看板娘（就是右手边的那只黑猫~），教程可以看这里 由于时间有限，不能改完所有bug使网站达到perfect，先记录下来，后期逐渐修改 bug(待修正)记录 个别渲染数学公式的格式不能使用的问题： 这是一个行内公式：\\(ax^2+bx+c=0\\)。 这是另一个行内公式：$ax^2+bx+c&gt;0$。 这是一个块状公式： $$\\displaystyle \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5} }-\\phi\\Bigr) e^{\\frac25 \\pi} } = 1+\\frac{e^{-2\\pi} } {1+\\frac{e^{-4\\pi} } {1+\\frac{e^{-6\\pi} } {1+\\frac{e^{-8\\pi} } {1+\\cdots} } } }$$ 这是另一个块状公式： \\\\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)e^{2 \\pi i \\xi x}d\\xi\\\\] source12345678910111213 这是一个行内公式：\\\\(ax^2+bx+c=0\\\\)。 这是另一个行内公式：$ax^2+bx+c&gt;0$。 这是一个块状公式（注意一定用&lt;div&gt;包裹）： &lt;div&gt; $$\\displaystyle \\frac{1}{\\Bigl(\\sqrt{\\phi \\sqrt{5} }-\\phi\\Bigr) e^{\\frac25 \\pi} } = 1+\\frac{e^{-2\\pi} } {1+\\frac{e^{-4\\pi} } {1+\\frac{e^{-6\\pi} } {1+\\frac{e^{-8\\pi} } {1+\\cdots} } } }$$ &lt;/div&gt; 这是另一个块状公式： \\\\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)e^{2 \\pi i \\xi x}d\\xi\\\\] 以下为目前不能使用的 或者使用\\\\(\\LaTeX\\\\)环境： &lt;div&gt; \\\\begin{equation} A = \\\\begin{bmatrix} a &amp; b \\\\\\\\ c &amp; c \\\\end{bmatrix} \\\\end{equation} &lt;/div&gt;source1234567891011或者使用\\\\(\\LaTeX\\\\)环境：&lt;div&gt;\\\\begin{equation}A =\\\\begin{bmatrix}a &amp; b \\\\\\\\c &amp; c\\\\end{bmatrix}\\\\end{equation}&lt;/div&gt; busuanzi网站分析与live2d的看板娘冲突的问题 有人已提过issue，相关blog；由于加入live2d有两种实现方式，还有一种需要修改源码（官网推荐），教程yingchi’s blog navBar中的svg格式的logo不显示的问题（和footer用的是同一个logo，现在只有footer显示了） 有人已提过issue 给每一个页面加图片(optional) 最后欢迎大家评论与交流~","link":"/2020/06/25/how-to-build-a-blog/"},{"title":"datawhale-零基础入门NLP-Task1","text":"题目理解目标是用多种思路完成天池的NLP新手级题目零基础入门NLP - 新闻文本分类，题目的大意是训练一个模型，对不同的文本段进行分类，分成财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐这十四个类（记作0-13），其中会对输入的文本段做匿名化处理，如下所示： 流程划分由于题目给出的训练用的文本段是匿名化的，无法进行分词操作，将整个比赛流程划分成特征提取和分类模型两个方面，因此在后续任务中会通过四种思路来完成这一题目，分别是： TF-IDF + 机器学习分类器 TF-IDF用于对文本提取特征 FastText FastText是入门款的词向量，由facebook提供 WordVec + 深度学习分类器 WordVec是进阶款的词向量 Bert词向量 Bert是高配款的词向量 测评指标评价标准为类别f1_score的均值，结果越大越好。 对于评测标准的理解评测标准的本质是用来筛选模型的好坏的，而模型的好坏在不同的条件下有不同的标准，因此评测标准也应该是多样的。 在本题中模型的作用是预测一段文本的类别，细化而言，即对于一段文本，判断其是否属于14类中的某一类（例如第3类），如果是模型预测是第3类就是positive，模型预测不是第3类就是negative，再结合实际情况，一共得到四类结果，如下图所示： 在实际中有很多种计算方式： Precision： 公式：$$Precision =\\frac{ True Positive }{ True Positive + False Positive }$$ 理解：这一标准关注的是对于某一类，在所有预测为positive的文本中，在实际中也是positive（即属于true positive）的概率 用途：Precision is a good measure to determine, when the costs of False Positive is high.（当分类器将一个实际negative的样本识别为positive会产生巨大损失时，即对false positive敏感，例如非垃圾邮件被识别成垃圾邮件） Recall： 公式：$$ Recall =\\frac{True Positive}{True Positive+False Negative}$$ 理解：这一标准关注的是对于某一类，在所有实际中为positive的文本中，在预测时也是positive（即属于true positive）的概率，也可以理解成模型对正样例的捕获能力 用途：Recall shall be the model metric we use to select our best model when there is a high cost associated with False Negative.（用于当一个正样例被分类错误会带来巨大损失的模型，即对false negative敏感，例如对强传染疾病的分类，将患病病人识别成不患病的） Accuracy： 公式：$$\\mathrm{Accuracy}=\\frac{\\text { True Positive }+\\text { True Negative }}{ \\text { (True Positive }+\\text { False Positive }+\\text { True Negative }+\\text { False Negative })}$$ 理解：计算的是所有样本中分类正确（不论正样例还是负样例）占算有样本的比例 用途：It is most used when all the classes are equally important.（用于所有分类结果都一样重要时） F1_score： 公式：$$\\mathrm{F} 1=\\left(\\frac{\\text { Recall }^{-1}+\\text {Precision }^{-1}}{2}\\right)^{-1} =2 \\times \\frac{ Precision*Recall} {Precision+Recall}$$ 理解：This is the harmonic mean of Precision and Recall and gives a better measure of the incorrectly classified cases than the Accuracy Metric（属于Precision和Recall的平衡点，即对Precision和Recall计算调和平均数） 对于F1_score和Accuracy的比较： Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes as in the above case. In most real-life classification problems, imbalanced class distribution exists and thus F1-score is a better metric to evaluate our model on. Purva HuilgolAccuracy vs. F1-Score 参考[1] Datawhale零基础入门NLP赛事 - Task1官方文档 [2] Accuracy, Precision, Recall or F1? [3] Accuracy vs. F1-Score [3] 图片引用自这里","link":"/2020/07/21/datawhale-NLP-t1/"},{"title":"datawhale-零基础入门NLP-Task2","text":"学习目标对需要训练的数据进行分析，同时通过可视化了解待训练数据的特点 前半部分的代码是官方文档提供的；作业部分属于自己完成的 数据分析读入并观察数据的格式1234import pandas as pdtrain_df = pd.read_csv('../data/train_set.csv', sep='\\t')train_df.head() 分析每段文本的长度12345%pylab inlinetrain_df['text_len'] = train_df['text'].apply(lambda x: len(x.split(' ')))print(train_df.head())print(train_df['text_len'].describe()) 由此我们可以看到，对于所有文本，平均长度为907，最短为2，最长为57921 文本长度可视化分析123_ = plt.hist(train_df['text_len'], bins=200)plt.xlabel('Text char count')plt.title(\"Histogram of char count\") 由此我们可以看出文本长度的分布是不均匀的 文本类别可视化分析123train_df['label'].value_counts().plot(kind='bar')plt.title('News class count')plt.xlabel(\"category\") 由此我们可以看出，不同类别的文本数也是不同的，可能对模型的训练精度产生影响 单词频次分析先展示错误代码 12345678from collections import Counterall_lines = ' '.join(list(train_df['text']))word_count = Counter(all_lines.split(\" \"))word_count = sorted(word_count.items(), key=lambda d:d[1], reverse = True)print(len(word_count))print(word_count[0])print(word_count[-1]) 错误原因：在执行到第三行时会因内存占用太多而导致程序奔溃； 解决办法：将训练数据分批读入，而不是一次性全部读入 再展示正确代码 123456789101112from collections import Counterword_count = Counter()chunk_iterator = pd.read_csv('../data/train_set.csv',sep='\\t', chunksize=10000) for chunk in chunk_iterator: all_lines = ' '.join(list(chunk['text'])) word_count.update(all_lines.split(\" \"))word_count = sorted(word_count.items(), key=lambda d:d[1], reverse = True)print(len(word_count))print(word_count[0])print(word_count[-1]) 表明一共出现了6869个汉字，其中3750号汉字出现的次数最多，为7482224次；3133号汉字出现的次数最少，为1次 统计覆盖率最高的前三个单词 1234567891011121314from collections import Counter# 统计每个句子中出现的不同的单词，为每个句子构造一个单词集train_df['text_unique'] = train_df['text'].apply(lambda x: ' '.join(list(set(x.split(' ')))))# 将所有句子的单词集连接成文本all_lines = ' '.join(list(train_df['text_unique']))# 统计每个单词在单词集中出现的次数word_count = Counter(all_lines.split(\" \"))# 对统计结果进行排序word_count = sorted(word_count.items(), key=lambda d:int(d[1]), reverse = True)# 展示统计结果前三名，含义是每个单词在多少个句子中出现过print(word_count[0])print(word_count[1])print(word_count[2]) 表明：编号’3750’的单词出现在了197997个句子中；编号’900’的单词出现在了197653个句子中；编号’648’的单词出现在了191975个句子中 本章作业 假设字符3750，字符900和字符648是句子的标点符号，请分析赛题每篇新闻平均由多少个句子构成？ 12345678910111213# 可以直接套用前一段代码，统计这三个标点在所有文本中出现的总次数即可from collections import Counterword_count = Counter()chunk_iterator = pd.read_csv('../data/train_set.csv',sep='\\t', chunksize=10000) for chunk in chunk_iterator: all_lines = ' '.join(list(chunk['text'])) word_count.update(all_lines.split(\" \"))# 计算出总次数sum = word_count['3750']+word_count['900']+word_count['648']print(sum)# 计算出平均次数print(sum/200000.0) 统计每类新闻中出现次数对多的字符 1234567891011chunk_iterator = pd.read_csv('../data/train_set.csv',sep='\\t', chunksize=10000) for label in range(14): word_count = Counter() all_lines = '' for chunk in chunk_iterator: for index, l in enumerate(list(train_df['label'])): if l==label: all_lines = all_lines+' '+ train_df['text'][index] word_count.update(all_lines.split(\" \")) word_count = sorted(word_count.items(), key=lambda d:int(d[1]), reverse = True) print(label,word_count[0]) 引用[1] 提高python处理数据的效率方法 [2] Datawhale零基础入门NLP赛事 - Task2 数据读取与数据分析","link":"/2020/07/22/datawhale-NLP-t2/"},{"title":"datawhale-零基础入门NLP-Task4","text":"学习目标不同于之前使用传统的机器学习方法，此次任务使用深度学习方法fastText来解决文本分类问题 文本表示方法（part 2）之前的博客讨论了文本的表示方法： TF-IDF Bag of Words (AKA Count Vectors) 通过观察，我们可以发现它们存在一定的缺陷：每一段文本被表示所需的向量维度很大，两种方法均是vocab-size大小的；导致训练模型所需要的时间很长 于是我们思考用一种更加简短的方式来表达一段文本，于是想到了fastText模型，与之前的文本表示方法不同，其基本思想是将一个单词通过深度学习模型表示在一个更小维度的向量空间，然后将整个句子的词向量求和——由于使用了更小维度的向量空间，训练和预测都会变得更快！ fastText模型简介​ fastText与word2vec、Glove均是对单词进行embedding的一种方法，本此任务我们先介绍fastText，其他两种之后会进行介绍 ​ 首先我们介绍embedding相比较与传统的文本表示方法（TF-IDF、Bag of Words）的优点：从前面我们可以知道，传统的文本表示方法形成的向量大小是vocab-size的，比较看重的是这段文本在每一个单词（或字）上的大小；而embedding会将一段文本中的每个单词映射到一个更小的空间中，使得每一个维度都有其意义（比如一个维度表示颜色，另一个维度表示大小之类的），因此展示在向量空间中，文本向量的距离可以用来衡量文本间的语义相似程度 ​ 接下来，我们介绍fastText这一模型 ​ fastText的核心思想是：使用character级别的N-gram（之前我们使用过单词级别的n-gram，即将一段文本中连续的n个单词组成一个新的词语），而这里使用的是字符级的N-gram，即将一个单词中连续的n个字符拆出来，形成一个行的单词，具体的例子为：对于单词“where”，在n=3时，形成的单词为：&lt;wh, whe, her, ere, re&gt;以及where，因此从某种角度而言，字符级别的N-gram也是扩充词库的一种重要方法 ​ fastText与word2vec（下一篇博客中会使用）相比，有如下优点： 在word2vec中，我们并没有直接利用构词学中的信息。无论是在Skip-Gram模型还是Continuous Bag of Words (CBOW)模型中，我们都将形态不同的单词用不同的向量来表示。例如，“dog”和“dogs”分别用两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。鉴于此，fastText提出了子词嵌入（subword embedding）的方法，从而试图将构词信息引入word2vec中的跳字模型 ….与Skip-Gram模型相比，fastText中词典规模更大，造成模型参数更多，同时一个词的向量需要对所有子词向量求和，继而导致计算复杂度更高。但与此同时，较生僻的复杂单词，甚至是词典中没有的单词，可能会从同它结构类似的其他词那里获取更好的词向量表示 李沐动手学深度学习 ​ 用fastText方法进行文本分类的模型架构如下所示： 代码实现为了完成本次实验，需要首先安装fastText安装包 我用的是conda下配置的python环境，直接使用pip install fasttext会报错，说需要安装Microsoft Visual C++ 14.0的依赖；因此推荐离线安装，具体推荐参考这里 1234567891011121314import pandas as pdfrom sklearn.metrics import f1_score# 转换为FastText需要的格式train_df = pd.read_csv('../data/train_set.csv', sep='\\t', nrows=15000)train_df['label_ft'] = '__label__' + train_df['label'].astype(str)train_df[['text','label_ft']].iloc[:-5000].to_csv('train.csv', index=None, header=None, sep='\\t')import fasttextmodel = fasttext.train_supervised('train.csv', lr=1.0, wordNgrams=2, verbose=2, minCount=1, epoch=25, loss=\"hs\")val_pred = [model.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[-5000:]['text']]print(f1_score(train_df['label'].values[-5000:].astype(str), val_pred, average='macro'))","link":"/2020/07/27/datawhale-NLP-t4/"},{"title":"datawhale-零基础入门NLP-Task3","text":"学习目标此次任务的目标是：在不同的文档表示方法下，使用传统机器学习算法来完成文本分类任务 文本表示方法(part1) 由于机器学习模型的输入只能是数字，而不是文本；所以在实现文本分类任务时，我们首先应该考虑的是如何将一段文本向量化 Bag of Words (AKA Count Vectors)用双城记来举例子展示我们是如何计算Bag of Words的 It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, 假设这四句话每一句都是一段文本，这四个文本共同组成了一个语料库 设计词典： 通过对以上的文本进行分析，我们可以得出语料库中一共出现了如下10个词语 1it、was、the、best、of、times、worst、age、wisdom、foolishness 计算出这段文本的向量表示，即将向量的长度设计为10，每一维度对应某个文本中一个单词出现的次数，即可以得到 1234\"It was the best of times\" = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\"it was the worst of times\" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\"it was the age of wisdom\" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\"it was the age of foolishness\" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1] 对这一算法的直观理解是：词汇分布越相似的文本，其含义越相近 TF-IDF根据这一算法的名字，我们将分成TF、DF、IDF三个小部分来介绍 我们的基本思想是：寻找一些词语，用这些词语来代表一个文本；这一算法多用在搜索引擎等技术中，用于搜索和待检查词语匹配的文章 根据日常经验，我们的基本思路是：寻找在一个文本中出现频率最高的几个词来代表这段文本，但是要排除停顿词，如英语中的“a, the, is, was”等 TF全名是Term Frequency，即词频。因为词频最能直观反映一篇文章的keywords，比如一篇写猫的文章，其中“猫”这个字的出现的次数一定很多。 由于有些文本是长文本，有些是短文本；就算同是写猫的文本，长文本中“猫”出现的次数一定多于短文本中“猫”出现的次数，但是却不能表明长文本中与“猫”的相关度比短文本强。因此我们在这里需要进行正则化，即： 1tf(t,d) = count of t in d / number of words in d DF全名是Document Frequency，即文档频率。这个词可不是它的字面意思哈！当我们计算一个单词A的DF时，实际上我们是在计算语料库中包含单词A的文档的数量。其公式如下： 1df(t) = occurrence of t in documents 我们之所以要计算DF，是为了用到它的倒数IDF，DF表明了一个单词的普遍程度，DF的值越大，表明当前单词在大部分的文章都出现过，十分普遍，不具备代表意义 IDF全名是Inverse Document Frequency，即逆文档频率，它就是DF的倒数。其含义已在DF中说过了，这里就说一下计算公式吧： $$\\operatorname{idf}(\\mathrm{t})=\\log (\\mathrm{N} /(\\mathrm{df}+1))$$ 之所以+1，是因为防止除数为0（在搜索系统中，待查询的词可能没在文本中出现过）；之所以取log，是因为防止数据爆炸（value explode） 综上， 一个词语的TF和IDF与这个词语在一段文本中是否具有代表性是正相关的，因此TF*IDF与这个词语也是正相关的；在实际文本中一般选择TF*IDF中最大的头几个词来代表文本 在本题中会将文本转化成一个长度固定为vocab_size的向量，每一维度对应着相应下标的单词的TF*IDF值 Trick: N-gram通过对以上两种算法的学习，我们发现这两种算法的需要构建词典来进行词频统计，在构建词典时，我们可以使用N-gram算法来扩充我们的词典，让词典中的一些词更有意义。N-gram是将连续的N个词组成一个新的词语来看待，基本思想将文本内容按照字节顺序进行大小为N的滑动窗口操作，最终形成长度为N的字节片段序列 举例而言，对于“It was the best of times”这句话使用2-gram（bigram）来分析，会在词典中增加这些单词： 1“it was”、“was the”、“the best”、“best of”、“of times” 以后在统计词频时，会将以上算作是一个单词来看待。 机器学习模型本次调用的是Ridge Classifier分类模型，是由岭回归（Ridge Regression）变化而来的，我们接下来先介绍岭回归。 线性回归用如下所示的线性模型，去拟合数据 $$\\hat{y}=w[0] \\times x[0]+w[1] \\times x[1]+\\ldots+w[n] \\times x[n]+b$$ 为了检测拟合结果，引入了代价函数（cost function），训练时代价越高，模型效果越差 $$\\sum_{i=1}^{M}\\left(y_{i}-\\hat{y}{i}\\right)^{2}=\\sum{i=1}^{M}\\left(y_{i}-\\sum_{j=0}^{p} w_{j} \\times x_{i j}\\right)^{2}$$ 可能出现的结果： 训练集cost 测试集cost 模型效果 大 大 欠拟合（under fitting） 小 小 效果好 小 大 过拟合（over fitting） 岭回归（Ridge Regression） 本质是在线性回归的基础上加入了L2正则化 进行拟合的函数是不变的，变化的是代价函数（cost function）的计算方法 $$\\sum_{i=1}^{M}\\left(y_{i}-\\hat{y}{i}\\right)^{2}=\\sum{i=1}^{M}\\left(y_{i}-\\sum_{j=0}^{p} w_{j} \\times x_{i j}\\right)^{2}+\\lambda \\sum_{j=0}^{p} w_{j}^{2}$$ 相当于在减小cost function的同时需要满足如下条件： $$\\text { For some } c&gt;0, \\sum_{j=0}^{p} w_{j}^{2}&lt;c$$ 即对模型中的参数的大小都进行了一定程度的抑制，通过$\\lambda$的值来控制抑制程度；$\\lambda$越接近0，就越接近普通的线性回归 注意：只能用来解决过拟合问题，不能用来筛选特征 Lasso回归 本质是在线性回归的基础上加入了L1正则化 $$\\sum_{i=1}^{M}\\left(y_{i}-\\hat{y}{i}\\right)^{2}=\\sum{i=1}^{M}\\left(y_{i}-\\sum_{j=0}^{p} w_{j} \\times x_{i j}\\right)^{2}+\\lambda \\sum_{j=0}^{p}\\left|w_{j}\\right|$$ 相当于在减小cost function的同时需要满足如下条件： $$\\text { For some } t&gt;0, \\sum_{j=0}^{p}\\left|w_{j}\\right|&lt;t$$ 可以解决的问题： 筛选特征，会将无用的特征前的权重赋值为0 防止过拟合 ==对于欠拟合模型，适当减少$\\lambda$ ，并增加迭代次数，有助于减小cost的值== Ridge Classifier 本质是将Ridge Regression转化为一个多输出的回归，预测类对应回归的最高值输出 代码实现具体代码如下所示： Count Vectors + RidgeClassifier 12345678910111213141516import pandas as pdfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.linear_model import RidgeClassifierfrom sklearn.metrics import f1_scoretrain_df = pd.read_csv('../data/train_set.csv', sep='\\t', nrows=15000)vectorizer = CountVectorizer(max_features=3000)train_test = vectorizer.fit_transform(train_df['text'])clf = RidgeClassifier()clf.fit(train_test[:10000], train_df['label'].values[:10000])val_pred = clf.predict(train_test[10000:])print(f1_score(train_df['label'].values[10000:], val_pred, average='macro')) TF-IDF + RidgeClassifier 12345678910111213141516import pandas as pdfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.linear_model import RidgeClassifierfrom sklearn.metrics import f1_scoretrain_df = pd.read_csv('../data/train_set.csv', sep='\\t', nrows=15000)tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=3000)train_test = tfidf.fit_transform(train_df['text'])clf = RidgeClassifier()clf.fit(train_test[:10000], train_df['label'].values[:10000])val_pred = clf.predict(train_test[10000:])print(f1_score(train_df['label'].values[10000:], val_pred, average='macro')) 引用[1] Datawhale零基础入门NLP赛事 - Task3官方文档 [2] A Gentle Introduction to the Bag-of-Words Model [3] TF-IDF from scratch in python on real world dataset. [4] Ridge and Lasso Regression: L1 and L2 Regularization","link":"/2020/07/25/datawhale-NLP-t3/"}],"tags":[{"name":"Hexo,Icarus","slug":"Hexo-Icarus","link":"/tags/Hexo-Icarus/"},{"name":"datawhale","slug":"datawhale","link":"/tags/datawhale/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"}],"categories":[{"name":"博客搭建","slug":"博客搭建","link":"/categories/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"},{"name":"Datawhale学习手记","slug":"Datawhale学习手记","link":"/categories/Datawhale%E5%AD%A6%E4%B9%A0%E6%89%8B%E8%AE%B0/"}]}