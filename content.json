{"pages":[{"title":"about","text":"Welcome Glad to see you here. I will write down my insights about DL,NLP,Rsys in this place. Education 2015.09-2019.06 Software Engineering, Northwestern Polytechnical University 2020.09 Going to study at the National Cyber Security College of Wuhan University Research Interest DL，NLP，Recommend System Contect Email:","link":"/about/index.html"}],"posts":[{"title":"Hexo+Icarus博客搭建记录","text":"个人感受 我是一名前端小白，完全是跟着网上的教程走的，首先记录一下个人在看教程时的感受： 对整体架构不是很清楚，只能跟着教程一步一步摸索着走，遇到问题再google 找教程时要先看发布的时间，像Hexo和Icarus更新的挺快的，较早的教程是不适合现在的 不用跟着一个教程走到黑，可以看适合自己的部分，多找几个教程对比验证，就可以找到比较合适的实现方式了 网上关于这部分的教程说的已经很好很详细了，我就没必要造重复造轮子了，针对我自己（大神请忽略。。）一开始摸不着头脑的情况，我想先列出一个整体框架，让大家清楚自己每走一步都在那个阶段，并清楚下一阶段该做什么，我觉得这是比较有帮助的 个人博客与博客网站的区别 这段话是我用了半年多再回过头来写的，个人还是很喜欢个人博客的，因为个人博客可以在本地浏览和编辑，相当于把本地的笔记本的功能与博客的功能结合起来。不满意的笔记可以直接放在Hexo的草稿部分，这一部分不会出现在博客中，只会出现在本地。十分方便 整体架构 这个blog搭建在github上的，使用了适用于个人blog的框架Hexo，Hexo的主题(theme)可由用户自定义(可以类比android的手机主题)，很多大神开源了很多主题，其中我选择的是Icarus，戳这里看demo 以上便是整体框架，接下来按照此可以划分出三步流程： 整体流程 在github上申请Repositories，并设置其为浏览器可访问的网页形式，具体教程可参照godweiyang在知乎的回答，PS：godweiyang自己改版了matery主题也很不错，大家也可以去看看他的主页，如果喜欢就可以一路跟着他在知乎的教程啦~ 安装并配置Hexo，具体教程同样可参照godweiyang在知乎的回答 安装好后，默认使用landscape主题，不过你现在已经可以尝试在此主题下写博客啦~PS：Hexo本身提供了标签、文章、归档等所有博客该有的功能，下一步的主题选择不影响当前编辑的内容的 此时需要先停下来，不着急进行下一步，先阅读Hexo的官方文档，PS：很短，官方文档是最直接了解这个框架功能的方式~，先了解一下怎么发文章，图片放在哪个目录下 在Hexo的基础上配置Icarus主题，完整过一遍Icarus的文档，建议阅读顺序如下： Icarus用户指南 - 主题配置对整体网站的配置 Icarus用户指南 - 挂件对于侧边栏的配置 随后运行整个网站，你会发现还有一些地方在报错，接下来就配置这些插件部分 做一些个性化改进： 图床配置，我是使用的腾讯云的COS服务，一开始会赠送6个月的服务，教程见这里 配置live2d看板娘（就是右手边的那只黑猫~），教程可以看这里 Latex支持：由于Hexo本身对于数学公式的支持不好，只能识别简单的latex公式，一旦公式复杂了，里面有一些和markdown冲突的符号，就会被Hexo错误解析，之前的做法是将包裹在$$间的latex公式嵌入在&lt;div&gt;&lt;&gt;之间，使得Hexo本身不去转译这段文本。这会造成：在本地Markdown编辑器无法看到公式的内容，如下图所示，十分不方便。具体修改方法是更换Markdown的引擎，可参照这里 由于时间有限，不能改完所有bug使网站达到perfect，先记录下来，后期逐渐修改 bug(待修正)记录 busuanzi网站分析与live2d的看板娘冲突的问题 有人已提过，相关blog；由于加入live2d有两种实现方式，还有一种需要修改源码（官网推荐），教程yingchi's blog navBar中的svg格式的logo不显示的问题（和footer用的是同一个logo，现在只有footer显示了）有人已提过issue 给每一个页面加图片(optional) 最后 欢迎大家评论与交流~","link":"/2020/06/25/how-to-build-a-blog/"},{"title":"datawhale-零基础入门NLP-Task1","text":"题目理解 目标是用多种思路完成天池的NLP新手级题目零基础入门NLP - 新闻文本分类，题目的大意是训练一个模型，对不同的文本段进行分类，分成财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐这十四个类（记作0-13），其中会对输入的文本段做匿名化处理，如下所示： 匿名化处理后的文本和标签 流程划分 由于题目给出的训练用的文本段是匿名化的，无法进行分词操作，将整个比赛流程划分成特征提取和分类模型两个方面，因此在后续任务中会通过四种思路来完成这一题目，分别是： TF-IDF + 机器学习分类器 TF-IDF用于对文本提取特征 FastText FastText是入门款的词向量，由facebook提供 WordVec + 深度学习分类器 WordVec是进阶款的词向量 Bert词向量 Bert是高配款的词向量 测评指标 评价标准为类别f1_score的均值，结果越大越好。 对于评测标准的理解 评测标准的本质是用来筛选模型的好坏的，而模型的好坏在不同的条件下有不同的标准，因此评测标准也应该是多样的。 在本题中模型的作用是预测一段文本的类别，细化而言，即对于一段文本，判断其是否属于14类中的某一类（例如第3类），如果是模型预测是第3类就是positive，模型预测不是第3类就是negative，再结合实际情况，一共得到四类结果，如下图所示： 在实际中有很多种计算方式： Precision： 公式：\\[Precision =\\frac{ True Positive }{ True Positive + False Positive }\\] 理解：这一标准关注的是对于某一类，在所有预测为positive的文本中，在实际中也是positive（即属于true positive）的概率 用途：Precision is a good measure to determine, when the costs of False Positive is high.（当分类器将一个实际negative的样本识别为positive会产生巨大损失时，即对false positive敏感，例如非垃圾邮件被识别成垃圾邮件） Recall： 公式：\\[ Recall =\\frac{True Positive}{True Positive+False Negative}\\] 理解：这一标准关注的是对于某一类，在所有实际中为positive的文本中，在预测时也是positive（即属于true positive）的概率，也可以理解成模型对正样例的捕获能力 用途：Recall shall be the model metric we use to select our best model when there is a high cost associated with False Negative.（用于当一个正样例被分类错误会带来巨大损失的模型，即对false negative敏感，例如对强传染疾病的分类，将患病病人识别成不患病的） Accuracy： 公式：\\[\\mathrm{Accuracy}=\\frac{\\text { True Positive }+\\text { True Negative }}{ \\text { (True Positive }+\\text { False Positive }+\\text { True Negative }+\\text { False Negative })}\\] 理解：计算的是所有样本中分类正确（不论正样例还是负样例）占算有样本的比例 用途：It is most used when all the classes are equally important.（用于所有分类结果都一样重要时） F1_score： 公式：\\[\\mathrm{F} 1=\\left(\\frac{\\text { Recall }^{-1}+\\text {Precision }^{-1}}{2}\\right)^{-1} =2 \\times \\frac{ Precision*Recall} {Precision+Recall}\\] 理解：This is the harmonic mean of Precision and Recall and gives a better measure of the incorrectly classified cases than the Accuracy Metric（属于Precision和Recall的平衡点，即对Precision和Recall计算调和平均数） 对于F1_score和Accuracy的比较： Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes as in the above case. In most real-life classification problems, imbalanced class distribution exists and thus F1-score is a better metric to evaluate our model on. Purva HuilgolAccuracy vs. F1-Score 参考 [1] Datawhale零基础入门NLP赛事 - Task1官方文档 [2] Accuracy, Precision, Recall or F1? [3] Accuracy vs. F1-Score [3] 图片引用自这里","link":"/2020/07/21/datawhale-NLP-t1/"},{"title":"datawhale-零基础入门NLP-Task4","text":"学习目标 不同于之前使用传统的机器学习方法，此次任务使用深度学习方法fastText来解决文本分类问题 文本表示方法（part 2） 之前的博客讨论了文本的表示方法： TF-IDF Bag of Words (AKA Count Vectors) 通过观察，我们可以发现它们存在一定的缺陷：每一段文本被表示所需的向量维度很大，两种方法均是vocab-size大小的；导致训练模型所需要的时间很长 于是我们思考用一种更加简短的方式来表达一段文本，于是想到了fastText模型，与之前的文本表示方法不同，其基本思想是将一个单词通过深度学习模型表示在一个更小维度的向量空间，然后将整个句子的词向量求和——由于使用了更小维度的向量空间，训练和预测都会变得更快！ fastText模型简介 fastText与word2vec、Glove均是对单词进行embedding的一种方法，本此任务我们先介绍fastText，其他两种之后会进行介绍 首先我们介绍embedding相比较与传统的文本表示方法（TF-IDF、Bag of Words）的优点：从task3我们可以知道，传统的文本表示方法形成的向量大小是vocab-size的，比较看重的是这段文本在每一个单词（或字）上的权重大小；而embedding会将一段文本中的每个单词映射到一个更小的空间中，使得每一个维度都有其意义（比如一个维度表示颜色，另一个维度表示大小之类的），因此展示在向量空间中，文本向量的距离可以用来衡量文本间的语义相似程度 接下来，我们介绍fastText这一模型 fastText的核心思想是：使用character级别的N-gram（之前我们使用过单词级别的n-gram，即将一段文本中连续的n个单词组成一个新的词语），而这里使用的是字符级的N-gram，即将一个单词中连续的n个字符拆出来，形成一个行的单词，具体的例子为：对于单词“where”，在n=3时，形成的单词为：&lt;wh, whe, her, ere, re&gt;以及where，因此从某种角度而言，字符级别的N-gram也是扩充词库的一种重要方法 fastText与word2vec（下一篇博客中会使用）相比，有如下优点： 在word2vec中，我们并没有直接利用构词学中的信息。无论是在Skip-Gram模型还是Continuous Bag of Words (CBOW)模型中，我们都将形态不同的单词用不同的向量来表示。例如，“dog”和“dogs”分别用两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。鉴于此，fastText提出了子词嵌入（subword embedding）的方法，从而试图将构词信息引入word2vec中的跳字模型 ....与Skip-Gram模型相比，fastText中词典规模更大，造成模型参数更多，同时一个词的向量需要对所有子词向量求和，继而导致计算复杂度更高。但与此同时，较生僻的复杂单词，甚至是词典中没有的单词，可能会从同它结构类似的其他词那里获取更好的词向量表示 李沐动手学深度学习 用fastText方法进行文本分类的模型架构如下所示： 代码实现 环境配置： 为了完成本次实验，需要首先安装安装包：我用的是conda下配置的python环境，直接使用会报错，说需要安装Microsoft Visual C++ 14.0的依赖；因此推荐离线安装，具体推荐参考这里 具体代码实现如下： 1234567891011121314import pandas as pdfrom sklearn.metrics import f1_score# 转换为FastText需要的格式train_df = pd.read_csv('../data/train_set.csv', sep='\\t', nrows=15000)train_df['label_ft'] = '__label__' + train_df['label'].astype(str)train_df[['text','label_ft']].iloc[:-5000].to_csv('train.csv', index=None, header=None, sep='\\t')import fasttextmodel = fasttext.train_supervised('train.csv', lr=1.0, wordNgrams=2, verbose=2, minCount=1, epoch=25, loss=&quot;hs&quot;)val_pred = [model.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[-5000:]['text']]print(f1_score(train_df['label'].values[-5000:].astype(str), val_pred, average='macro')) 引用 [1]","link":"/2020/07/27/datawhale-NLP-t4/"},{"title":"datawhale-零基础入门NLP-Task5(未完待续)","text":"Introduction 使用fastText之外的深度学习模型，例如word2vec、TextCNN、TextRNN，来完成文本分类任务 模型学习 首先我们先学习一下我们即将使用的三种模型 word2vec TextCNN TextCNN利用CNN（卷积神经网络）进行文本特征抽取，不同大小的卷积核分别抽取n-gram特征，卷积计算出的特征图经过MaxPooling保留最大的特征值，然后将拼接成一个向量作为文本的表示。 TextRNN TextRNN利用RNN（循环神经网络）进行文本特征抽取，由于文本本身是一种序列，而LSTM天然适合建模序列数据。TextRNN将句子中每个词的词向量依次输入到双向双层LSTM，分别将两个方向最后一个有效位置的隐藏层拼接成一个向量作为文本的表示。 Reference [1] Datawhale零基础入门NLP赛事 - Task5-word2vec官方文档 [2] Datawhale零基础入门NLP赛事 - Task5-TextCNN官方文档 [3] Datawhale零基础入门NLP赛事 - Task5-TextRNN官方文档","link":"/2020/07/31/datawhale-NLP-t5/"},{"title":"datawhale-零基础入门NLP-Task3","text":"学习目标 此次任务的目标是：在不同的文档表示方法下，使用传统机器学习算法来完成文本分类任务 文本表示方法(part1) 由于机器学习模型的输入只能是数字，而不是文本；所以在实现文本分类任务时，我们首先应该考虑的是如何将一段文本向量化 Bag of Words (AKA Count Vectors) 用双城记来举例子展示我们是如何计算Bag of Words的 It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, 假设这四句话每一句都是一段文本，这四个文本共同组成了一个语料库 设计词典： 通过对以上的文本进行分析，我们可以得出语料库中一共出现了如下10个词语 1it、was、the、best、of、times、worst、age、wisdom、foolishness 计算出这段文本的向量表示，即将向量的长度设计为10，每一维度对应某个文本中一个单词出现的次数，即可以得到 1234&quot;It was the best of times&quot; = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]&quot;it was the worst of times&quot; = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]&quot;it was the age of wisdom&quot; = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]&quot;it was the age of foolishness&quot; = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1] 对这一算法的直观理解是：词汇分布越相似的文本，其含义越相近 TF-IDF 根据这一算法的名字，我们将分成TF、DF、IDF三个小部分来介绍 我们的基本思想是：寻找一些词语，用这些词语来代表一个文本；这一算法多用在搜索引擎等技术中，用于搜索和待检查词语匹配的文章 根据日常经验，我们的基本思路是：寻找在一个文本中出现频率最高的几个词来代表这段文本，但是要排除停顿词，如英语中的“a, the, is, was”等 TF 全名是Term Frequency，即词频。因为词频最能直观反映一篇文章的keywords，比如一篇写猫的文章，其中“猫”这个字的出现的次数一定很多。 由于有些文本是长文本，有些是短文本；就算同是写猫的文本，长文本中“猫”出现的次数一定多于短文本中“猫”出现的次数，但是却不能表明长文本中与“猫”的相关度比短文本强。因此我们在这里需要进行正则化，即： 1tf(t,d) = count of t in d / number of words in d DF 全名是Document Frequency，即文档频率。这个词可不是它的字面意思哈！当我们计算一个单词A的DF时，实际上我们是在计算语料库中包含单词A的文档的数量。其公式如下： 1df(t) = occurrence of t in documents 我们之所以要计算DF，是为了用到它的倒数IDF，DF表明了一个单词的普遍程度，DF的值越大，表明当前单词在大部分的文章都出现过，十分普遍，不具备代表意义 IDF 全名是Inverse Document Frequency，即逆文档频率，它就是DF的倒数。其含义已在DF中说过了，这里就说一下计算公式吧： \\[\\operatorname{idf}(\\mathrm{t})=\\log (\\mathrm{N} /(\\mathrm{df}+1))\\] 之所以+1，是因为防止除数为0（在搜索系统中，待查询的词可能没在文本中出现过）；之所以取log，是因为防止数据爆炸（value explode） 综上， 一个词语的TF和IDF与这个词语在一段文本中是否具有代表性是正相关的，因此TFIDF与这个词语也是正相关的；在实际文本中一般选择TFIDF中最大的头几个词来代表文本 在本题中会将文本转化成一个长度固定为vocab_size的向量，每一维度对应着相应下标的单词的**TF*IDF**值 Trick: N-gram 通过对以上两种算法的学习，我们发现这两种算法的需要构建词典来进行词频统计，在构建词典时，我们可以使用N-gram算法来扩充我们的词典，让词典中的一些词更有意义。N-gram是将连续的N个词组成一个新的词语来看待，基本思想将文本内容按照字节顺序进行大小为N的滑动窗口操作，最终形成长度为N的字节片段序列 举例而言，对于“It was the best of times”这句话使用2-gram（bigram）来分析，会在词典中增加这些单词： 1“it was”、“was the”、“the best”、“best of”、“of times” 以后在统计词频时，会将以上算作是一个单词来看待。 机器学习模型 本次调用的是Ridge Classifier分类模型，是由岭回归（Ridge Regression）变化而来的，我们接下来先介绍岭回归。 线性回归 用如下所示的线性模型，去拟合数据 \\[\\hat{y}=w[0] \\times x[0]+w[1] \\times x[1]+\\ldots+w[n] \\times x[n]+b\\] 为了检测拟合结果，引入了代价函数（cost function），训练时代价越高，模型效果越差 \\[\\sum_{i=1}^{M}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}=\\sum_{i=1}^{M}\\left(y_{i}-\\sum_{j=0}^{p} w_{j} \\times x_{i j}\\right)^{2}\\] 可能出现的结果： 训练集cost 测试集cost 模型效果 大 大 欠拟合（under fitting） 小 小 效果好 小 大 过拟合（over fitting） 岭回归（Ridge Regression） 本质是在线性回归的基础上加入了L2正则化 进行拟合的函数是不变的，变化的是代价函数（cost function）的计算方法 \\[\\sum_{i=1}^{M}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}=\\sum_{i=1}^{M}\\left(y_{i}-\\sum_{j=0}^{p} w_{j} \\times x_{i j}\\right)^{2}+\\lambda \\sum_{j=0}^{p} w_{j}^{2}\\] 相当于在减小cost function的同时需要满足如下条件： \\[\\text { For some } c&gt;0, \\sum_{j=0}^{p} w_{j}^{2}&lt;c\\] 即对模型中的参数的大小都进行了一定程度的抑制，通过\\(\\lambda\\)的值来控制抑制程度；\\(\\lambda\\)越接近0，就越接近普通的线性回归 注意：只能用来解决过拟合问题，不能用来筛选特征 Lasso回归 本质是在线性回归的基础上加入了L1正则化 \\[\\sum_{i=1}^{M}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}=\\sum_{i=1}^{M}\\left(y_{i}-\\sum_{j=0}^{p} w_{j} \\times x_{i j}\\right)^{2}+\\lambda \\sum_{j=0}^{p}\\left|w_{j}\\right|\\] 相当于在减小cost function的同时需要满足如下条件： \\[\\text { For some } t&gt;0, \\sum_{j=0}^{p}\\left|w_{j}\\right|&lt;t\\] 可以解决的问题： 筛选特征，会将无用的特征前的权重赋值为0 防止过拟合 ==对于欠拟合模型，适当减少\\(\\lambda\\) ，并增加迭代次数，有助于减小cost的值== Ridge Classifier 本质是将Ridge Regression转化为一个多输出的回归，预测类对应回归的最高值输出 代码实现 具体代码如下所示： Count Vectors + RidgeClassifier 12345678910111213141516import pandas as pdfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.linear_model import RidgeClassifierfrom sklearn.metrics import f1_scoretrain_df = pd.read_csv('../data/train_set.csv', sep='\\t', nrows=15000)vectorizer = CountVectorizer(max_features=3000)train_test = vectorizer.fit_transform(train_df['text'])clf = RidgeClassifier()clf.fit(train_test[:10000], train_df['label'].values[:10000])val_pred = clf.predict(train_test[10000:])print(f1_score(train_df['label'].values[10000:], val_pred, average='macro')) TF-IDF + RidgeClassifier 12345678910111213141516import pandas as pdfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.linear_model import RidgeClassifierfrom sklearn.metrics import f1_scoretrain_df = pd.read_csv('../data/train_set.csv', sep='\\t', nrows=15000)tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=3000)train_test = tfidf.fit_transform(train_df['text'])clf = RidgeClassifier()clf.fit(train_test[:10000], train_df['label'].values[:10000])val_pred = clf.predict(train_test[10000:])print(f1_score(train_df['label'].values[10000:], val_pred, average='macro')) 引用 [1] Datawhale零基础入门NLP赛事 - Task3官方文档 [2] A Gentle Introduction to the Bag-of-Words Model [3] TF-IDF from scratch in python on real world dataset. [4] Ridge and Lasso Regression: L1 and L2 Regularization","link":"/2020/07/25/datawhale-NLP-t3/"},{"title":"整体认识——pytorch-learning（一）","text":"重要提醒：本文不求原创和创新，是基于哈工大SCIR实验室的大佬们写的这篇文章的补充和拓展，因此会合原文有许多重复之处，请大家注意！ 常用术语 iteration：表示1次迭代（也叫training step），每次迭代更新1次网络结构的参数，所需的数据量为1个batch的大小 batch-size：1次迭代所使用的样本量 epoch：1个epoch表示过了1遍训练集中的所有样本（包含多个batch的数据） 基本步骤 我们首先将模型的训练过程分为四个步骤，分别是： 输入处理模块 (X 输入数据，变成网络能够处理的Tensor类型) 模型构建模块 (主要负责从输入的数据，得到预测的y^, 这就是我们经常说的前向过程) 定义代价函数和优化器模块 (注意，前向过程只会得到模型预测的结果，只有当前模块负责自动求导和参数更新) 构建训练过程（迭代训练过程，就是下图表示的训练迭代过程） 详细解释 一、数据处理 对于数据处理，最为简单的⽅式就是将数据组织成为⼀个 。但许多训练需要⽤到mini-batch，直接组织成Tensor不便于我们操作。pytorch为我们提供了Dataset和Dataloader两个类来方便的构建。 torch.utils.data.Dataset 继承Dataset 类需要override 以下⽅法： 12345678910from torch utils. data import Dataset class trainDataset（Dataset）： def init （self）： # constructor def getitem （self, index）： # 获得第⊥ndex号的数据和标签 def len（self）： # 获得数据量 torch.utils.data.DataLoader 1torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False) DataLoader Batch。如果选择shuffle = True，每⼀个epoch 后，都会对所有数据打乱重新排序 mini-Batch batch_size 常⻅的使⽤⽅法如下： 123dataLoader = Dataloader(dataset, shuffle=True, batch size=32)for i, data in enumerate (dataloader, 0) x, y # data同时获得数据和标签 shuffle=true表示每次取一个batch的数据前，先进行排序。官方注解如下： shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False). 二、模型构建 所有的模型都需要继承torch.nn.Module ， 需要实现以下⽅法： 12345678class MyModel(torch, nn.Module)： def init(self)： super(MyModel, self).__init__() def forward(self, x)： return model=MyModel() 其中forward() ⽅法是前向传播的过程。在实现模型时，我们不需要考虑反向传播。 三、 定义代价函数和优化器 主要负责自动求导和参数更新，也就是反向传播部分 12criterion = torch.nn.BCELOSS(reduction='sum') # 代价函数optimizer = torch.optim.Adam(model, parameters(), lr=0.0001, betas=(0.9，0.999), eps=1e-08, weight_decay=0, amsgrad=False) # 优化器 其中反向传播的基础：计算图，如下所示： 由图可以看出，全局偏导是由局部偏导根据链式法则，连续相乘而得出的 四、构建训练过程 12345678def train(epoch)：#一个epoc的训练 for i, data in enumerate(dataLoader, 0): x,y = data #取出 minibatch数据和标签y pred = mode1(x) #前向传播 loss = criterion(y_pred,y) #计算代价函数 optimizer.zero_grad（）#清零梯度准备计算 1oss.backward（）#反向传播 optimizer.step（）#更新训练参数 引用 [1] 推荐给大家！Pytorch编写代码基本步骤思想_哈工大SCIR实验室的大佬们写的 [2] 河北工业大学刘洪普老师的视频教程","link":"/2020/08/01/pytorch-learning-1/"},{"title":"datawhale-零基础入门NLP-Task2","text":"学习目标 对需要训练的数据进行分析，同时通过可视化了解待训练数据的特点 前半部分的代码是官方文档提供的；作业部分属于自己完成的 数据分析 读入并观察数据的格式 1234import pandas as pdtrain_df = pd.read_csv('../data/train_set.csv', sep='\\t')train_df.head() 分析每段文本的长度 12345%pylab inlinetrain_df['text_len'] = train_df['text'].apply(lambda x: len(x.split(' ')))print(train_df.head())print(train_df['text_len'].describe()) 由此我们可以看到，对于所有文本，平均长度为907，最短为2，最长为57921 文本长度可视化分析 123_ = plt.hist(train_df['text_len'], bins=200)plt.xlabel('Text char count')plt.title(&quot;Histogram of char count&quot;) 由此我们可以看出文本长度的分布是不均匀的 文本类别可视化分析 123train_df['label'].value_counts().plot(kind='bar')plt.title('News class count')plt.xlabel(&quot;category&quot;) 由此我们可以看出，不同类别的文本数也是不同的，可能对模型的训练精度产生影响 单词频次分析 先展示错误代码 12345678from collections import Counterall_lines = ' '.join(list(train_df['text']))word_count = Counter(all_lines.split(&quot; &quot;))word_count = sorted(word_count.items(), key=lambda d:d[1], reverse = True)print(len(word_count))print(word_count[0])print(word_count[-1]) 错误原因：在执行到第三行时会因内存占用太多而导致程序奔溃； 解决办法：将训练数据分批读入，而不是一次性全部读入 再展示正确代码 123456789101112from collections import Counterword_count = Counter()chunk_iterator = pd.read_csv('../data/train_set.csv',sep='\\t', chunksize=10000) for chunk in chunk_iterator: all_lines = ' '.join(list(chunk['text'])) word_count.update(all_lines.split(&quot; &quot;))word_count = sorted(word_count.items(), key=lambda d:d[1], reverse = True)print(len(word_count))print(word_count[0])print(word_count[-1]) 表明一共出现了6869个汉字，其中3750号汉字出现的次数最多，为7482224次；3133号汉字出现的次数最少，为1次 统计覆盖率最高的前三个单词 1234567891011121314from collections import Counter# 统计每个句子中出现的不同的单词，为每个句子构造一个单词集train_df['text_unique'] = train_df['text'].apply(lambda x: ' '.join(list(set(x.split(' ')))))# 将所有句子的单词集连接成文本all_lines = ' '.join(list(train_df['text_unique']))# 统计每个单词在单词集中出现的次数word_count = Counter(all_lines.split(&quot; &quot;))# 对统计结果进行排序word_count = sorted(word_count.items(), key=lambda d:int(d[1]), reverse = True)# 展示统计结果前三名，含义是每个单词在多少个句子中出现过print(word_count[0])print(word_count[1])print(word_count[2]) 表明：编号'3750'的单词出现在了197997个句子中；编号'900'的单词出现在了197653个句子中；编号'648'的单词出现在了191975个句子中 本章作业 假设字符3750，字符900和字符648是句子的标点符号，请分析赛题每篇新闻平均由多少个句子构成？ 12345678910111213# 可以直接套用前一段代码，统计这三个标点在所有文本中出现的总次数即可from collections import Counterword_count = Counter()chunk_iterator = pd.read_csv('../data/train_set.csv',sep='\\t', chunksize=10000) for chunk in chunk_iterator: all_lines = ' '.join(list(chunk['text'])) word_count.update(all_lines.split(&quot; &quot;))# 计算出总次数sum = word_count['3750']+word_count['900']+word_count['648']print(sum)# 计算出平均次数print(sum/200000.0) 统计每类新闻中出现次数对多的字符 1234567891011chunk_iterator = pd.read_csv('../data/train_set.csv',sep='\\t', chunksize=10000) for label in range(14): word_count = Counter() all_lines = '' for chunk in chunk_iterator: for index, l in enumerate(list(train_df['label'])): if l==label: all_lines = all_lines+' '+ train_df['text'][index] word_count.update(all_lines.split(&quot; &quot;)) word_count = sorted(word_count.items(), key=lambda d:int(d[1]), reverse = True) print(label,word_count[0]) 引用 [1] 提高python处理数据的效率方法 [2] Datawhale零基础入门NLP赛事 - Task2 数据读取与数据分析","link":"/2020/07/22/datawhale-NLP-t2/"},{"title":"线性回归——pytorch-learning（二）","text":"线性模型 假设我们的线性模型的格式为： \\[\\hat{y}=x * \\omega\\] 先确定优化目标，也就是cost function，我们算则MSE（Mean Square Error） \\[\\cos t=\\frac{1}{N} \\sum_{n=1}^{N}\\left(\\hat{y}_{n}-y_{n}\\right)^{2}\\] 12345678910111213141516171819202122232425262728import numpy as npimport matplotlib.pyplot as pltx_data = [1.0, 2.0, 3.0]y_data = [2.0, 4.0, 6.0]# 前向传播def forward(x):return x * w# 计算loss，评价模型def loss(x, y):y_pred = forward(x)return (y_pred - y) * (y_pred - y)w_list = []mse_list = []for w in np.arange(0.0, 4.1, 0.1): #对参数进行穷举 print('w=', w) l_sum = 0 for x_val, y_val in zip(x_data, y_data): y_pred_val = forward(x_val) loss_val = loss(x_val, y_val) l_sum += loss_val print('\\t', x_val, y_val, y_pred_val, loss_val) print('MSE=', l_sum / 3) w_list.append(w) mse_list.append(l_sum / 3) 注：可以用visdom进行实时绘图，对数据进行可视化 梯度下降算法 我们将寻找使目标函数值最小的参数的问题称为优化问题，为了代替对于参数的遍历方法，引入梯度下降算法 \\[\\omega=\\omega-\\alpha \\frac{\\partial \\cos t}{\\partial \\omega}\\] 更新公式的方法 123456789101112131415161718192021222324252627x_data = [1.0, 2.0, 3.0]y_data = [2.0, 4.0, 6.0]w = 1.0def forward(x): return x * wdef cost(xs, ys): cost = 0 for x, y in zip(xs, ys): y_pred = forward(x) cost += (y_pred - y) ** 2 return cost / len(xs)def gradient(xs, ys): grad = 0 for x, y in zip(xs, ys): grad += 2 * x * (x * w - y) return grad / len(xs)print('Predict (before training)', 4, forward(4))for epoch in range(100): cost_val = cost(x_data, y_data) grad_val = gradient(x_data, y_data) w -= 0.01 * grad_val print('Epoch:', epoch, 'w=', w, 'loss=', cost_val)print('Predict (after training)', 4, forward(4)) 关于梯度下降，以上使用的是普通的梯度下降：即用所有数据的平均损失值来进行梯度更新；我们也可以使用SGD（Stochastic Gradient Descent）：即随机使用一个数据来更新梯度。SGD的好处是：单个样本包含了随机噪声，这样就避免停留在鞍点，缺点是：时间效率太低；因此现在主流是每次将训练数据分为过个batch，没经过一个batch进行一次数据更新 反向传播 本质是构建计算图，以两层神经网络为例： \\[\\hat{y}=W_{2}\\left(W_{1} \\cdot X+b_{1}\\right)+b_{2}\\] 其计算图为： pytorch基础 Tensor：一般针对每一个权重或中间变量使用一个tensor，其内部组成如下：包含权重本身和梯度（也是一个Tensor）两个部分 注意： 每进行一次反向传播，计算图就会被释放！ Tensor在做加法运算时会构建计算图；如过不进行backward()，就一直不会释放； Tensor中的梯度在.backward()方法执行后会被累加，所以每次在更新完梯度后都需要，将梯度清零 在进行计算时应使用w.grad.data，而不是w.grad（Tensor），否则会产生新的计算图 12345678910print(&quot;predict (before training)&quot;, 4, forward(4).item())for epoch in range(100): for x, y in zip(x_data, y_data): l = loss(x, y) l.backward() #释放计算图，累加梯度 print('\\tgrad:', x, y, w.grad.item()) w.data = w.data - 0.01 * w.grad.data #更新梯度 w.grad.data.zero_() #梯度清零 print(&quot;progress:&quot;, epoch, l.item())print(&quot;predict (after training)&quot;, 4, forward(4).item()) 用pytorch实现线性回归 123456789101112131415161718import torch# 准备数据x_data = torch.Tensor([[1.0], [2.0], [3.0]])y_data = torch.Tensor([[2.0], [4.0], [6.0]])# 设计计算图（根据上一篇）class LinearModel(torch.nn.Module): # 通过继承moudle，自动计算backword def __init__(self): super(LinearModel, self).__init__() self.linear = torch.nn.Linear(1, 1) # Class nn.Linear contain two member Tensors: weight and bias def forward(self, x): y_pred = self.linear(x) return y_predmodel = LinearModel() 关于torch.nn.Linear，官方文档如下： 优化目标函数MSE如下所示： 1criterion = torch.nn.MSELoss(size_average=False) 其中官方文档为： 迭代优化器如下所示： 12# 不继承于moudle，不会构建计算图optimizer = torch.optim.SGD(model.parameters(), lr=0.01) 通过model.parameters()，提取模型中所有需要更新的参数 接下来写训练过程 12345678for epoch in range(100): y_pred = model(x_data) loss = criterion(y_pred, y_data) print(epoch, loss) optimizer.zero_grad() loss.backward() optimizer.step() #更新 输出与测试 12345678# Output weight and biasprint('w = ', model.linear.weight.item())print('b = ', model.linear.bias.item())# Test Modelx_test = torch.Tensor([[4.0]])y_test = model(x_test)print('y_pred = ', y_test.data) 引用 [1] 河北工业大学刘洪普老师的视频教程","link":"/2020/08/01/pytorch-learning-2/"},{"title":"Logistic回归与多分类问题——pytorch-learning（三）","text":"Logistic回归 基本概念 Logistic回归是将实数范围映射的值到[0,1]的范围内，虽然是回归模型，但是常被用作分类任务，理解成是否属于某一类别的概率，其映射又公式称为sigmoid函数，如下： \\[\\sigma(x)=\\frac{1}{1+e^{-x}}\\] 优化的目标函数也应该相应做出改变，改为使用交叉熵函数cross-entropy，其含义是计算两个概率分布之间的差异性，越小越好，具体公式展示如下： \\[H(p, q)=\\sum_{i} p\\left(x_{i}\\right) \\log \\frac{1}{\\log q\\left(x_{i}\\right)}=-\\sum_{i} p\\left(x_{i}\\right) \\log q\\left(x_{i}\\right)\\] 在二分类问题中，具体应用如下： \\[\\operatorname{los} s=-(y \\log \\hat{y}+(1-y) \\log (1-\\hat{y}))\\] 代码展示 123456789101112131415161718192021222324252627import torch.nn.functional as Fx_data = torch.Tensor([[1.0], [2.0], [3.0]])y_data = torch.Tensor([[0], [0], [1]])#-------------------------------------------------------#class LogisticRegressionModel(torch.nn.Module): def __init__(self): super(LogisticRegressionModel, self).__init__() self.linear = torch.nn.Linear(1, 1) def forward(self, x): y_pred = F.sigmoid(self.linear(x)) return y_pred model = LogisticRegressionModel()#-------------------------------------------------------#criterion = torch.nn.BCELoss(size_average=False)optimizer = torch.optim.SGD(model.parameters(), lr=0.01)#-------------------------------------------------------#for epoch in range(1000): y_pred = model(x_data) loss = criterion(y_pred, y_data) print(epoch, loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() 多维特征值问题 假设输入8维，输出1维 12345678910111213141516171819class Model(torch.nn.Module): def __init__(self): super(Model, self).__init__() ## 输入特征4维，输出特征1维 self.linear = torch.nn.Linear(4,2) ## 此时对同一组2维数据中的每一个取sigmoid self.sigmoid = torch.nn.Sigmoid() def forward(self, x): x1 = self.linear(x) print(x1) x2 = self.sigmoid(x1) print(x2) return x2model = Model()x_data = torch.Tensor([[1.0,2.0,3.0,4.0],[1.5,2.5,3.5,4.5]])y_predic = model(x_data) 结果如下： 多层神经网络 如上所示，在多层神经网络中，一般利用torch.nn.Linear进行升维或降维，即： 将上图的神经网络实现成代码，如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344## 数据读取import numpy as npimport torch# 读取成numpy格式，注意一般gpu只接受32位，分隔符按照数据的内容自己选定xy = np.loadtxt('diabetes.csv.gz', delimiter=',', dtype=np.float32)# from_numpy可以自动转化为Tensor格式x_data = torch.from_numpy(xy[:,:-1])y_data = torch.from_numpy(xy[:, [-1]])## 设计网络class Model(torch.nn.Module): def __init__(self): super(Model, self).__init__() self.linear1 = torch.nn.Linear(8, 6) self.linear2 = torch.nn.Linear(6, 4) self.linear3 = torch.nn.Linear(4, 1) self.sigmoid = torch.nn.Sigmoid() def forward(self, x): x = self.sigmoid(self.linear1(x)) x = self.sigmoid(self.linear2(x)) x = self.sigmoid(self.linear3(x)) return x model = Model()## 构造损失和优化函数## 进行迭代for epoch in range(100): # Forward y_pred = model(x_data) loss = criterion(y_pred, y_data) print(epoch, loss.item()) # Backward optimizer.zero_grad() loss.backward() # Update optimizer.step() 以上代码有两个问题： 每个epoch计算的是全部的数据，而不是mini-batch的 目前的代码均只使用了sigmoid作为激活函数 接下来我们尝试解决这两个问题： 首先，用ReLu替换sigmoid作为激活函数： 123456789101112131415161718import torchclass Model(torch.nn.Module): def __init__(self): super(Model, self).__init__() self.linear1 = torch.nn.Linear(8, 6) self.linear2 = torch.nn.Linear(6, 4) self.linear3 = torch.nn.Linear(4, 1) self.relu = torch.nn.ReLU() self.sigmoid = torch.nn.Sigmoid() def forward(self, x): x = self.relu(self.linear1(x)) x = self.relu(self.linear2(x)) x = self.sigmoid(self.linear3(x)) return xmodel = Model() 注意：使用ReLu时，为防止最后的输出值被用来计算ln，所以最后一层用sigmoid作为激活函数 接下来，我们研究如何利用mini-batch来加载数据集 数据加载问题 主要工具理解 主要使用Dataset和DataLoader这两个工具，其中： Dataset：负责根据下标取出数据 DataLoader：对Dataset进行包裹，负责取出mini-batch大小的数据 涉及的概念： iteration：表示1次迭代（也叫training step），每次迭代更新1次网络结构的参数，所需的数据量为1个batch的大小 batch-size：1次迭代所使用的样本量 epoch：1个epoch表示过了1遍训练集中的所有样本（包含多个batch的数据） 若总的数据量为1000，batch的大小为100，则每一次经过一次epoch，需要通过10次iteration 代码展示： 123456789101112131415161718192021import torch## 注意Dataset为抽象类，而DataLoader不是from torch.utils.data import Datasetfrom torch.utils.data import DataLoaderclass DiabetesDataset(Dataset): def __init__(self): ## 1.一次加载所有数据，按下标存取 ## 2.只加载文件名 pass def __getitem__(self, index): ## 如果初始化时读取的是文件名，则这里开始根据文件名读入文件 pass def __len__(self): passdataset = DiabetesDataset()train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2) num_workers的含义：读取数据的并行进程的数目 对于参数shuffle的理解： 注意：windows和linux系统上训练数据的差别，因为两个系统实现多线程的方式不同，windows使用spawn来代替fork windows： 1234567train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2)……# 不能让for循环直接打头if __name__ == '__main__': for epoch in range(100): for i, data in enumerate(train_loader, 0): …… linux： 12345train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2)……for epoch in range(100): for i, data in enumerate(train_loader, 0): …… Dataset的实现（以加载全部数据为例） 12345678910111213141516171819202122232425262728293031323334class DiabetesDataset(Dataset): def __init__(self, filepath): xy = np.loadtxt(filepath, delimiter=',', dtype=np.float32) self.len = xy.shape[0] self.x_data = torch.from_numpy(xy[:, :-1]) self.y_data = torch.from_numpy(xy[:, [-1]]) def __getitem__(self, index): ## 返回下标元组 return self.x_data[index], self.y_data[index] def __len__(self): return self.lendataset = DiabetesDataset('diabetes.csv.gz')train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2)......## 使用数据加载器后的训练流程的变化for epoch in range(100): ## 此处使用mini-batch，batch-size为32，每次迭代只使用一个数据 for i, data in enumerate(train_loader, 0): # 1. Prepare data（dataLoader会自动将数据转化为Tensor） inputs, labels = data # 2. Forward y_pred = model(inputs) loss = criterion(y_pred, labels) print(epoch, i, loss.item()) # 3. Backward optimizer.zero_grad() loss.backward() # 4. Update optimizer.step() 多分类问题 softmax分类器 在多分类问题中，我们期望的输出时当前数据属于各个类别的概率分布，因此我们期望它满足如下的条件： \\[P(y=i) \\geq 0\\] \\[\\sum_{i=0}^{n} P(y=i)=1\\] 因此，人们提出了如下的模型： \\[P(y=i)=\\frac{e^{z_{i}}}{\\sum_{j=0}^{K-1} e^{z_{j}}}, i \\in\\{0, \\ldots, K-1\\}\\] 注意：在pytorch中，Cross-Entropy Loss与negative Log Likelihood Loss之间的区别 引用在pytorch论坛中看到的一句话： If you apply Pytorch’s CrossEntropyLoss to your output layer, you get the same result as applying Pytorch’s NLLLoss to a LogSoftmax layer added after your original output layer. 对比示意图如下所示（均会自动将类别标签转化成one-hot向量）： 代码验证： 1234567891011121314151617181920import torch# Y是长整型的张量Y = torch.LongTensor([2, 0, 1])Y_pred1 = torch.Tensor([[0.1, 0.2, 0.9],[1.1, 0.1, 0.2],[0.2, 2.1, 0.1]])Y_pred2 = torch.Tensor([[0.8, 0.2, 0.3],[0.2, 0.3, 0.5],[0.2, 0.2, 0.5]])# criterion1 = torch.nn.CrossEntropyLoss()l1 = criterion1(Y_pred1, Y)l2 = criterion1(Y_pred2, Y)print(&quot;Batch Loss1 = &quot;, l1.data, &quot;\\nBatch Loss2=&quot;, l2.data)#actFunction = torch.nn.Softmax()r1 = torch.log(actFunction(Y_pred1))r2 = torch.log(actFunction(Y_pred2))criterion2 = torch.nn.NLLLoss()r1 = criterion2(r1, Y)r2 = criterion2(r2, Y)print(&quot;Batch Loss1 = &quot;, r1.data, &quot;\\nBatch Loss2=&quot;, r2.data) 结果如下： 引用 [1] 河北工业大学刘洪普老师的视频教程","link":"/2020/08/01/pytorch-learning-3/"},{"title":"卷积神经网络——pytorch-learning（四）","text":"图像形成简述 一般图像分为两类： 栅格图像：图像由像素点组成，一般可以由人工捕获，在相机摄像时，每个像素点都通过一个光学元件来采集——放大后变成马赛克 矢量图像：一般无法人工捕获，由程序生成，相当于一段描述：圆心在哪，半径多少....，一般由程序画出来，而不是现成的——放大不变形 卷积神经网络简述 使用DNN（全连接神经网络）来进行图像分类时，是将二维图像完全展开成一维图像，只保留了像素间横向的联系，却忽略了纵向的联系。 因此我们引入了卷积神经网络，提取其二维特征，其工作过程是通过黄色的卷积核来提取绿色的图像中的二维特征，随后将特征输出到粉色矩阵中，以便进行下一步的处理。单通道卷积的运算过程如下所示： 对于卷积核作用的直观理解：检测图像的局部是否满足某一特征，如果满足，则返回一个较大的值；否则返回一个较小的值 注意：对于多通道的图像，每次卷积是针对所有的通道来进行的，卷积核应该是三维的，第三个维度的大小应该和通道数相同，因此在讨论卷积核的大小时，只考虑二维的大小。如下图所示： 展开后具体如下： 压缩后如下所示（卷积时不加padding）： 泛化图像的尺寸，以及投影到多个卷积核之后可以得到： 注意：卷积核的数目与输出结果的通道数相同 卷积核的相关参数 其中： m是卷积核的个数 n是输入图像的通道数 padding 在图像外层填充0，借此来保证经过卷积计算后的图形与原图形大小保持一致。一般而言，外围填充的0的圈数为 \\[\\frac{kernel_size}{2}\\] 如下图所示： stride 在进行卷积运算时，每一次移动卷积核的距离 123456789101112131415161718192021import torchin_channels, out_channels= 5, 10width, height = 100, 100kernel_size = 3batch_size = 1input = torch.randn(batch_size, in_channels, width, height)conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size)# padding参数# conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1, bias=False)# stride参数# conv_layer = torch.nn.Conv2d(1, 1, kernel_size=3, stride=2, bias=False)# 可以选择自己初始化卷积的权重# kernel = torch.Tensor([1,2,3,4,5,6,7,8,9]).view(1, 1, 3, 3)# conv_layer.weight.data = kernel.dataoutput = conv_layer(input)print(input.shape)print(output.shape)print(conv_layer.weight.shape) Pooling 可以理解成下采样，用来筛选特征，压缩图像大小。但是特征筛选只在同一个通道内进行；经过pooling之后图像的通道数不改变；在pooling层中： stride = kernal_size Max Pooling：每次取局部内的最大值 123456789import torchinput = [3,4,6,5,2,4,6,8,1,6,7,8,9,7,4,6,]input = torch.Tensor(input).view(1, 1, 4, 4)maxpooling_layer = torch.nn.MaxPool2d(kernel_size=2)output = maxpooling_layer(input)print(output) 代码示例 在此处我们使用MNIST数据集作为演示，具体网络架构如下： 注意：卷积层和池化层不在意图像的大小，但是线性层在意，需要提前计算 123456789101112131415161718class Net(torch.nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5) self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5) self.pooling = torch.nn.MaxPool2d(2) self.fc = torch.nn.Linear(320, 10) def forward(self, x): # Flatten data from (n, 1, 28, 28) to (n, 784) batch_size = x.size(0) x = F.relu(self.pooling(self.conv1(x))) x = F.relu(self.pooling(self.conv2(x))) x = x.view(batch_size, -1) # flatten x = self.fc(x) return xmodel = Net() 对于GPU的使用： 123456789101112131415161718192021# 选择设备device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)# 迁移模型model.to(device)# 迁移数据def train(epoch): running_loss = 0.0 for batch_idx, data in enumerate(train_loader, 0): inputs, target = data # 这里 inputs, target = inputs.to(device), target.to(device) optimizer.zero_grad() # forward + backward + update outputs = model(inputs) loss = criterion(outputs, target) loss.backward() optimizer.step() running_loss += loss.item() if batch_idx % 300 == 299: print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 2000)) running_loss = 0.0 卷积神经网络进阶 以复杂网络GoogLeNet为例子 对Inception模块进行解释 1*1的卷积核 作用： 将所有输入通道同一像素点的数值进行加权 改变通道的数目，相比于其他大小的卷积，1*1的卷积在改变通道数目时，计算量较小。对比如下： 代码实现 注意：张量的维度表示方式为（B,C,W,H），因此torch.cat操作时dim=1 卷积的深度及其效果 结论：并不是卷积越深，效果越好 解决方法：使用残差网络（Residual net） 残差网络（Residual net） ## 搭建网络的建议 逐层搭建，逐层测试，主要测试网络的输入与输出的矩阵的大小 引用 [1] 河北工业大学刘洪普老师的视频教程 [2] He K, Zhang X, Ren S, et al. Identity Mappings in Deep Residual Networks [3] Huang G, Liu Z, Laurens V D M, et al. Densely Connected Convolutional Networks","link":"/2020/08/01/pytorch-learning-4/"},{"title":"chap 1-Boolean retrieval","text":"布尔检索模型 目标：在目标语料库中查询出现某些词和不出现某些词的文章，即通过具有精确语义的逻辑表达式来构建查询 词项-文档关联矩阵 倒排序索引 收集需要建立索引的文档 将文章词条化 词条归一化 建立倒排序索引 存储方式：词典在内存，倒排记录表在磁盘 查询优化 改变处理交集的次序来提升处理速度，一般根据每个词语的文档频率来作为改变合并次序的原则：两个两个合并，从小到大逐渐合并 对基本布尔操作的扩展 临近操作符：限制待查询的两个词之间的距离；即“/s”表示位于同一个句子中，\"/k\"表示距离K个词之内，\"/p\"表示位于同一个段落中 短语查询 布尔检索模型的改进方向 对返回结果进行排序 对短语进行搜索，而不是单个词 对于词典内的单词，能够容忍不同的形式和错拼 引入词频统计来验证搜索的可信度","link":"/2020/09/07/information-retrieval-ch1/"},{"title":"chap 2-The term vocabulary and postings lists","text":"Document delineation and character sequence decoding 根据不同的编码方式，将字节序列转化为字符序列 文本的序列化处理 将文档和查询使用相同的方法转化为词条 关键术语的区分 token： tokenization is the task of chopping it up into pieces, called tokens； type： A type is the class of all tokens term containing the same character sequence term: A term is a (perhaps normalized) type that is included in the IR system’s dictionary. example: For example, if the document to be indexed is to sleep perchance to dream, then there are five tokens, but only four types (because there are two instances of to). However, if to is omitted from the index (as a stop word) , then there are only three terms: sleep, perchance, and dream. 需要解决的问题 分词问题： 英语：对于空格和连字符的处理 中文：无法通过空格来分词，词边界不明显 德语：对复合词进行拆分 去除停用词：通过文档集频率来选择停用词，频率越大说明越不具有特殊性（现代搜索系统的影响并不大）——一般先分词，后去除停用词 term normalization：将看起来不完全一致的多个词条归纳成一个等价类，如：anti-discriminatory和antidiscriminatory均映射成antidiscriminatory，一般的方法是构建同义词表，归一化的方法如下： 合并同义词表中多个词的查询结果 构建索引时，便对词进行扩展 词干还原（stemming）与词形归并（lemmatization） 词干还原：利用启发式规则去除两端前缀 词形归并：利用词汇表和词形分析 基于skip pointers的倒排记录表快速合并算法 只适用于AND操作，不适用于OR操作 算法图示： 算法描述： 跳表指针的设置的位置： 每隔\\[\\sqrt{P}\\]均设置一个跳表指针，指向下一个跳表指针的位置，P是跳表的长度 短语查询的解决方法 目前一般采用如下两种方式的混合 二元词索引 将文档中的每个连接词对看成一个短语 “stanford university palo alto”会被转化成“stanford university” and “university palo” and “palo alto” 扩展的二元词对：先进行词性标注，再将一个多词序列看成一个扩展的二元词 位置信息索引 在每个倒排记录中，存储该词在文本中出现的位置 举例如下：","link":"/2020/09/07/information-retrieval-ch2/"},{"title":"chap 3-Dictionaries and tolerant retrieval","text":"本章主要讲解了如何对倒排索引进行优化，分为三个部分：对于词典部分的查询进行优化、对通配符查询的处理、拼写校正的问题以及基于发音的校正技术 词典搜索的数据结构 哈希表 容易产生冲突问题，尤其是词汇表不断变大的情况下 搜索树——二叉树、B树 二叉树：注意二叉树的平衡问题 B树：适用于部分词典常驻磁盘的情况 通配符查询 使用B树和反向B树相结合 单一通配符：对于单词lemon，其中反向B树中的路径就是 root-n-o-m-e-l；对于查询se*mon，可以通过B-树来返回所有前缀为se且后缀非空的词项子集W，再通过反向B-树来返 回所有后缀为mon且前缀非空的词项子集R，然后，对W和R求交集W ∩ R 多通配符：通过穷举法检查返回的集合中的每个元素 轮排索引 此方法是对于正反向B树的改进，即只使用一棵B树 单一通配符：我们在字符集中引入一个新的符号$，用于标识词项结束；对于单词hello的轮排索引的结果如下所示： 处理通配符时的应用：考虑通配符查询 m*n，这里的关键是将查询进行旋转让*号出现 在字符串末尾，即得到 n$m*，此时便可以用正向B树来解决 多通配符：通过穷举法检查返回的集合中的每个元素 K-gram索引 k-gram表示的是将每个单词按照k个字母来划分，分成字母片段对于 castle 来说，所有的 3-gram包括$ca、cas、ast、stl、tle 及 le$ 本方法直接适用于多通配符，此时词汇表变为k-gram形式，而每个倒排记录表则由包含该k-gram的词项组成，而每个单词还会对应一个文章ID的倒排记录表 拼写校正 用于解决在用户输入拼写错误的查询词时，能检测到并还原为正确的查询词，最终返回正确查询词的查询结果。 拼写校正的实现 对于一个拼写错误的查询，在其可能的正确拼写中，选择“距离”最近或者邻近度最小的那个。在距离和邻居度相同的情况下，选择其他用户查询最频繁的 校正方法 词项独立的校正方法 在对查询进行分词之后，只对单个查询词进行校正，此时很难检测到flew form Heathrow中的错误 编辑距离法 编辑距离的定义：将字符串s1转换为字符串s2所需要的最小编辑操作数（删除一个字符、替换一个字符、增加一个字符） 一般通过动态规划来计算，其算法描述为： image-20200909092014035 K-gram法 原理：正确的目标查询词为待匹配词项包含查询 q（拼写错误） 中某个固定数目的 k-gram 即可 举例：假定我们想返回 bord 的 3 个 2-gram 中的至少 2 个词项，对倒排记录表的单遍扫描会返回满足该条件的所有词项，本例当中，这些词项包括 aboard、boardroom 及 border 混合法 首先使用 k-gram 索引返回可能是 q 的潜在正确拼写形式的词项集合，然后计算该集合中的每个元素和 q 之间的编辑距离并选择 具有较小距离的那些词项。 上下文敏感的拼写校正 尝试对短语中的每个词进行替换。比如对于上面 flew form Heathrow 的例子，我们 可能会返回如下短语 fled from Heathrow 和 flew fore Heathrow。对每个替换后的短语，搜索引擎 进行查找并确定最后的返回数目。 基于发音的校正技术 原理：：（1）在名称转录时，元音是可以互换的；（2）发音相似的辅音字母归为一类。这就会导致相关的名称通常有相同的 soundex 编码结果 方法：使用soundex编码方法","link":"/2020/09/08/information-retrieval-ch3/"},{"title":"chap 4-Index construction","text":"基于块的排序索引方法（BSBI） 此处主要使用的算法是外部排序算法和归并算法 主要步骤为： 将文档集分割成几个大小相等的部分（所谓的块） 将每个部分的词项 ID—文档 ID 对排序（将每个词项映射为ID） 将中间产生的临时排序结果存放到磁盘中 将所有的中间文件合并成最终的索引 特点： 需要构建词项-词项ID的映射表 所有的索引结果都需要经过排序 single-pass in-memory indexing（SPIMI） 相比于BSBI，其优点为： 不需要构建词项-词项ID的映射表，节约内存 索引结果不需要排序，节约时间 算法描述： 最后的内存块的合并过程此处省略，与BSBI相同。 特点：将词典直接存入每一个内存块中 分布式索引构建方法 采用MapReduce框架，其图示如下所示： Map 阶段：将输入的数据片映射成键—值对。这个映射过程对应于 BSBI 和 SPIMI 算法中的分析任务，因此也将执行 Map 过程的机器称为分析器（parser） Reduce阶段：将同一键（词项ID）的所有值（文档ID）集中存储，以便快速读取和处理。实现时，将所有的键按照词项区间划分成j个段，并将属于每个段的键—值对写入各自分区文件即可 动态索引构建方法 周期性重建索引：周期性地对文档集从头开始进行索引重构。如果随时间的推移 文档更新的次数不是很多，并且能够接受对新文档检索的一定延迟，再加上如果有足够的资源 能够支持在建立新索引的同时让旧索引继续工作，那么周期性索引重构不失为一种较好的选择 添加辅助索引：要求能够及时检索到新文档，一个是大的主 索引，另一个是小的用于存储新文档信息的辅助索引，后者保存在内存中。检 索时可以同时遍历两个索引并将结果合并。文档的删除记录在一个无效位向量中，在返回结果之前可以利用它过滤掉已删除文档。某篇文档的更新通过先删除后重新 插入来实现。 其他检索类型 排序式检索系统（ranked retrieval）：相比于布尔检索系统，不按照文档ID排序，仅仅按照权重或者影响程度排序 安全性检索系统：添加用户访问权限，构建ACL（access control list，访问控制表）","link":"/2020/09/09/information-retrieval-ch4/"},{"title":"chap 5-Index compression","text":"压缩索引的目的：1.增加cache的利用率；2.加速数据从磁盘到内存的速度 本章介绍的压缩技术均为无损压缩，即压缩之后所有的原始信息都被保留；大小写转换、词干还原、和停用词剔除均是有损压缩技术 信息检索中词项的统计特性 Heaps定律：对于词汇表大小的估计 Zipf定律：对词项分布的建模 词典压缩 使用变长字符串 查询表中只存指针，每个词项均对应一个指针 按块存储 块间二分查找，块内遍历（块内K个词项），减少指针数目 k越大，压缩率越高；但是查找效率越低 倒排记录表的压缩 倒排记录表中的内容为文档的序号 主要采用：1.可变字节码；2.Y编码这两种压缩方式 可变字节编码 利用整数个字节对同一单词的倒排记录表中的相邻文档间的间距进行编码，字节的后七位是间距的有效编码区，第一位是延续位（如果该位为1，表示结尾；否则不是） 举例如下： Y编码 Y编码的组成：将间距G表示成长度和偏移两个部分进行变长编码，G的偏移实际上是G的二进制编码，但是前端的1被去掉；eg：对 13（二进制为 1101）进行编码，其偏移为 101。偏移的长度为3位，长度部分采用一元编码（一开始有连续个1，最后以0结尾，1的个数表示长度）。即为1110. 举例如下：","link":"/2020/09/11/information-retrieval-ch5/"},{"title":"chap 6-Scoring, term weighting, and the vector space model","text":"参数化索引及域索引 元数据：指的是和文档有关的一些具有特定形式的数据，通常包含字段和数值两部分 域数据：同字段的意义相似 图示如下： 另一种实现方式： 在域索引的基础上添加权重 假定每篇文档有 l 个域，其对应的权重分别是\\[g_{1}, \\ldots, g_{l} \\in [0,1]\\]，且满足\\[\\sum_{i=1}^{l} g_{i}=1\\]，其中\\[s_{i}\\]是查询文档和某个域的匹配得分（若匹配，值为1；否则，值为0），则一篇文档的查询得分为\\[\\sum_{i=1}^{l} g_{i} s_{i}\\] 对于权重的学习 考虑一个简单的域加权评分的例子，其中每篇文档只包含 title 和 body 两个域。给定查询 q和文档d，根据 title 及 body 域是否和 q 匹配，利用布尔匹配函数分别计算出布尔变量\\[s_{T}(d, q)\\]和\\[s_{B}(d, q)\\]，主要接下来要确定g的值： \\[\\operatorname{socre}(d, q)=g \\cdot s_{T}(d, q)+(1-g) s_{B}(d, q)\\] 词项频率及权重计算 目前只考虑了词项在文档中出现与否的情况，未考虑词项出现的频率。不同于之前的权重计算方法， 我们认为：如果一个查询词在文档中出现的频率越高，所应该赋予的权重就越大 此处，我们引入词项频率（TF），我们使用词袋模型（直接将出现的次数作为权重），即不在乎词的位置，只在乎出现的次数 我们认为：并不是所有词的重要性都是一样的，应该赋予不同的词以不同的权重（根据它在所有文档中出现的频繁程度，越频繁价值越低） 此处，我们引入逆文档频率（IDF），其中对于文档频率（DF）的定义为：词项在文档集中出现的次数（同一文档出现多次算做一次）；而IDF与DF的关系如下： \\[i d f_{t}=\\log \\frac{N}{d f_{t}}\\] 我们通常将TF和IDF结合起来计算一个文档与查询之间的相关度，即： \\[\\operatorname{tf-idf}_{t, d}=\\operatorname{tf}_{t, d} \\times \\mathrm{idf}_{t}\\] 当查询由多个词组成时，tf和idf用向量来表示，结果用内积的形式来表示；具体可见之前的一篇博客 空间向量模型 一组文档的集合可以看成向量空间中的多个向量，每个词项 对应一个坐标轴。这种表示忽略了词项在文档中的相对次序。即在这种模型表示下，文档 Mary is quicker than John 和 John is quicker than Mary 是等价的 利用向量空间进行文档相似度的计算 此处使用余弦相似度这一计算模型： \\[\\operatorname{sim}\\left(d_{1}, d_{2}\\right)=\\frac{\\vec{V}\\left(d_{1}\\right) \\cdot \\vec{V}\\left(d_{2}\\right)}{\\left|\\vec{V}\\left(d_{1}\\right) \\| \\vec{V}\\left(d_{2}\\right)\\right|}\\] 于是，将查找与 d 最相似的文档这个问题可以归结成寻找和d有最大内积结果的过程。在查询时，一般需要构建查询向量，即将查询文本视为一个短文档，并为它构建向量。 通过向量进行查询结果排序 用于给定查询，从文档集中返回得分最高的k篇文档 计算向量相似度的基本算法如下： 原理：对于每一个查询词项，对于其倒排序表中的每一个文档累加该词项的权重。最后比较所有参与过计算的文档的权重和，选出前K个文档。","link":"/2020/09/16/information-retrieval-ch6/"},{"title":"chap 7-Computing scores in a complete search system","text":"以文档为单位的评分方法：每次计算一篇文档的得分 以词项为单位的评分方法：遇到每个词项时，得分能够逐渐累加 快速评分及排序 第六章介绍的是精确返回前K篇得分最高的文档的办法；此处我们开始关注非精确返回前K篇文档的方法，即前K篇返回的文档与最相关的K篇近似，但又不完全相同。同时用户感受不到返回的前K篇文档间的相关度有所降低，这样做的好处是可以降低运算的复杂度 此处介绍的算法主要包含如下的两个步骤： 找到一个文档集合 A，它包含了参与最后竞争的候选文档，其中\\[K&lt;|A|&lt;&lt;N\\]。A 不必包 含前 K 篇得分最高的文档，但是它应该包含很多和前 K 篇文档得分相近的文档 返回 A 中得分最高的 K 篇文档 索引去除技术 仅考虑查询中词项idf值超过一定阈值的单词所对应的倒排记录表 仅考虑包含多个（K个）查询词项的文档 胜者表 胜者表（champion list），有时也称为优胜表（fancy list）或高分文档（top doc），它的基本 思路是，对于词典中的每个词项 t，预先计算出 r 个最高权重的文档，其中 r 的值需要事先给定。 对于 tf-idf 权重计算机制而言，词项 t 所对应的 tf 值最高的 r 篇文档构成 t 的胜者表 静态得分和排序 很多搜索引擎中，每篇文档 d 往往都有 一个与查询无关的静态得分 g（d）。该得分函数的取值往往在 0 到 1 之间。比如，对于 Web 上 的新闻报道，g（d）可以基于用户的正面评价次数来定义。 可将静态得分和相似度组合得出每个文档的得分，即： \\[\\text { net-score }(q, d)=g(d)+\\frac{\\vec{V}(q) \\cdot \\vec{V}(d)}{|\\vec{V}(q) \\| \\vec{V}(d)|}\\] 簇剪枝方法 主要原理是对所有的文本使用聚类操作，聚类操作如下： 从 N 篇文档组成的文档集中随机选出\\[\\sqrt{N}\\]篇文档，它们称为先导者（leader）集合 对于剩余的\\[N - \\sqrt{N}\\]篇（称为追随者，follower）每篇不属于先导者集合的文档，计算离之最近的先导者 查询操作如下： 给定查询 q，通过与\\[\\sqrt{N}\\]个先导者计算余弦相似度，找出和它最近的先导者 L 候选集合 A 包括 L 及其追随者，然后对 A 中的所有的文档计算余弦相似度 信息检索系统的组成 此处介绍信息检索系统中常用的一些概念，即： 层次型索引 是对索引去除方法的一般化技术：例如，第 1 层索引中的 tf 阈值是 20， 第 2 层阈值是 10。这意味着第 1 层索引只保留 tf 值超过 20 的倒排记录，而第 2 层的记录只保 留 tf 值超过10的倒排记录。 查询词项的邻近性 对于检索中的查询，特别是Web上的自由文本查询来说，用户往往希望返回的文档中大部分或者全部查询词项之间的距离比较近，因为这表明返回文档中具有聚焦用户 查询意图的文本 考虑一个由两个或者多个查询词项构成的查询\\[t_{1}, t_{2}, \\ldots, t_{k}\\]，令文档中包含所有查询词项得最小窗口大小为\\[\\omega\\]，其取值为窗口内词的个数。如果文档中不包含所有的查询词项， 那么此时可以将ω设成一个非常大的数字。直观上讲，ω的值越小，文档d和查询匹配程度更高，即可以根据ω的大小来设计权重 查询分析及文档评分函数的设计 查询分析：将自由文本查询通过查询分析器，转化成带操作符的查询，比如对于rising interest rates的查询的分析如下： 查询rising interest rates这一短语 如果太少，转而分别查询 rising interest 和 interest rates 两个查询短语 如果太少，转而分别查询 rising ，interest 和 rates 这三个查询短语 评分函数的设计：必须融入向量空间计算、静态得分、邻近度加权或其他因素 搜索系统的组成","link":"/2020/09/16/information-retrieval-ch7/"},{"title":"chap 8-Evaluation in information retrieval","text":"信息检索的评价 信息系统测试集的组成： 一个文档集 一组用于测试的信息需求集合，信息需求可以表示为查询（信息系统!=查询词） 一组相关性测试结果，对于每个查询-文档而言，赋予一个二值判断结果（相关、不相关） 对无序检索结果集合的评价 正确率： \\[\\text { Precision }=\\frac{\\text { 返回结果中相关文档的数目 }}{\\text { 返回结果的数目 }}=P(\\text { relevant } \\mid \\text { retrieved })\\] 召回率： \\[\\text { Recall }=\\frac{\\text { 返回结果中相关文档的数目 }}{\\text { 所有相关文档的数目 }}=P(\\text { retrieved } \\mid \\text { relevant })\\] 精确率： \\[\\text { Recall }=\\frac{\\text { 返回结果中真正例+正反例 }}{\\text { 所有被判断的文档的数目 }}\\] 精确率往往导致不准确的结果：绝大多数情况下，信息检索中的数据存在着极度的不均衡性，比如通常情况下，超过 99.9%的文档 都是不相关文档。这样的话，一个简单地将所有的文档都判成不相关文档的系统就会获得非常 高的精确率值，从而使得该系统的效果看上去似乎很好。而即使系统实际上非常好 正确率+召回率（F值）： \\[F=\\frac{1}{\\alpha \\frac{1}{P}+(1-\\alpha) \\frac{1}{R}}\\] 可以通过调整\\[\\alpha\\]来控制正确率和召回率的权重 对有序检索结果的评价 相比于无序检索结果，有序检索结果只对top-K个返回的结果进行处理 正确率-召回率曲线： 随着K的增加，出现锯齿形图案 插值正确率： 起到平滑的作用，具体做法为：对每一个Precision值，使用其右边最大的Precision值替代 11点插值平均正确率： 对平滑后的Precision曲线进行均匀采样出11个点（每个点间隔0.1），然后计算这11个点的平均Precision \\[A P=\\frac{1}{11} \\times\\left(A P_{r}(0)+A P_{r}(0.1)+\\ldots+A P_{r}(1.0)\\right)\\] 平均正确率均值MAP（Mean Average Precision）： 目前普遍使用，具有较好的稳定性和代表性 平均正确率AP：在每个相关文档位置上正确率的平均值 某个查询Q共有6个相关结果，某系统排序 返回了5篇相关文档，其位置分别是第1，第2，第5，第 10，第20位，则AP=(1/1+2/2+3/5+4/10+5/20+0)/6；其中1/1，2/2，3/5等就是平均正确率 平均正确率均值MAP：对一组查询的top-K个返回结果求平均正确率 \\[\\operatorname{MAP}(Q)=\\frac{1}{|Q|} \\sum_{j=1}^{|Q|} \\frac{1}{m_{j}} \\sum_{k=1}^{m_{j}} \\operatorname{Precision}\\left(R_{j k}\\right)\\] 相关性判定 在构建测试集时，需要： 设计用于测试的查询 需要判定文档的相关性 此处主要讨论判定文档的相关性，即考虑雇佣多个人来进行相关性判定，所需要做的是判定多个人之间的判定是否一致，采用kappa统计量，即： \\[\\text {kappa}=\\frac{P(A)-P(E)}{1-P(E)}\\] 其中P(A)是观察到的一致性判断比率，p(E)是比较对象间的随机一致性比率，距离如下： reference [1] 白话mAP [2] 中科大课件","link":"/2020/09/16/information-retrieval-ch8/"},{"title":"chap 10-XML retrieval","text":"XML文本的基本概念 XML文本的主要特点为：具有复杂的树形结构，属性之间还存在嵌套关系 XML文本举例： 转化为树形结构： XML DOM：将元素、属性以及元素内部的文本表示成树的节点 XPath: 是XML文档集中的路径表达式描述标准，也称为XML上下文。路径上前后元素间使用”/“来分割；\"//\"表示中间可以插入多个元素。eg：act/scene 表示选择所有父节点为 act 元素的scene元素；plan//scene表示选择出现在play元素下的所有scene 元素 XML检索中的挑战性问题 结构化检索中的挑战是用户希望返回文档的一部分（即 XML 元素），而不像非结构 化检索那样往往返回整个文档 选择最合适的文档部分的一个准则是：系统应该总是检索出回答查询的最明确最具体的文档部分，即返回信息需求的最小单位 该问题相对应的问题是：“对文档的哪些部分建立索引”，具体方法如下： 将节点分组 使用最大的元素作为索引单位，然后在最大的元素中寻找相关的子元素——自顶向下 先搜索最相关的子节点，然后扩展成更大的单位（父节点）——自底向上 对所有元素建立索引 由此产生的问题，即冗余性增大，同时元素间存在嵌套关系 解决方法：构造元素选择时的限制策略： 忽略所有的小元素 忽略用户不会浏览的所有元素类型（这需要记录当前 XML 检索系统的运行日志信息） 忽略通常被评估者判定为不相关性的元素类型（如果有相关性判定的话） 只保留系统设计人员或图书馆员认定为有用的检索结果所对应的的元素类型 对于剩余的冗余元素，将嵌套元素组合起来，并将查询词高亮显示来吸引用户关注相关段落 对于嵌套问题，还会引起词项统计信息的不准确，解决方法如下： 为XML的每个上下文-词项对计算idf，只考虑目标节点的父节点：比如 author#\"Gates\" 和 section#\"Gates\"","link":"/2020/09/19/information-retrieval-ch10/"},{"title":"chap 9-Relevance feedback and query expansion","text":"相关反馈及伪相关反馈 属于查询优化的局部方法，在一个查询的初始返回结果的基础上进行完善，使得再次返回的结果得到优化 相关反馈的主要思想：在信息检索的过程中通过用户交互来提高最终的检索效果。让用户来判断相关性 具体过程如下： 用户提交一个简短的查询 系统返回初次检索结果 用户对部分结果进行标注，将它们标注为相关或不相关 系统基于用户的反馈计算出一个更好的查询来表示信息需求 利用新查询系统返回新的检索结果 Rocchio 相关反馈算法 基本原理：假定我们要找一个最优查询向量\\(\\vec{q}\\) ，它与相关文档之间的 相似度最大且同时又和不相关文档之间的相似度最小 若\\(C_{r}\\)表示相关文档集，\\(C_{n r}\\)表示不相关文档集，那么我们希望找到的最优的\\(\\vec{q}\\) 是： \\[\\vec{q}_{o p t}=\\underset{\\vec{q}}{\\arg \\max }\\left[\\operatorname{sim}\\left(\\vec{q}, C_{r}\\right)-\\operatorname{sim}\\left(\\vec{q}, C_{n r}\\right)\\right]\\] 另一种定义为： \\[\\vec{q}_{o p t}=\\frac{1}{\\left|C_{r}\\right|} \\sum_{\\bar{d}_{j} \\in C_{r}} \\vec{d}_{j}-\\frac{1}{\\left|C_{n r}\\right|} \\sum_{\\bar{d}_{j} \\in C_{m r}} \\vec{d}_{j}\\] 增加权重后为： \\[\\vec{q}_{m}=\\alpha \\vec{q}_{0}+\\beta \\frac{1}{\\left|D_{r}\\right|} \\sum_{\\vec{d}_{j} \\in D_{r}} \\vec{d}_{j}-\\gamma \\frac{1}{\\left|D_{n r}\\right|} \\sum_{\\vec{d}_{j} \\in D_{n r}} \\vec{d}_{j}\\] 其中定义\\(C_{r}\\)为已知的相关文档集，\\(C_{n r}\\)为已知的不相关文档集 影响相关反馈的因素 初始查询时出问题： 拼写错误，可通过拼写校正技术来解决 跨语言IR 用户的词汇表和文档集的词汇表不同，比如laptop和notebook computer 相关反馈方法的使用条件： 理想条件下，所有相关文档中的词项分布应该与用户标出的相关文档中的词项分布相似，而同时所 有不相关文档中的词项分布与相关文档中的词项分布差别很大。如果相关文档包括多个不同子类，即它们在向量空间中可 以聚成多个簇，那么 Rocchio 方法效果会不太好 对于相关反馈策略的评价 首先计算出原始查询 \\(q_{0}\\) 的正确率—召回率曲线，一轮相关反馈之后，我们计算出修改后的 查询 \\(q_{m}\\) 并再次计算出新的正确率—召回率曲线。这样，反馈前与反馈后我们都可以在所有文档 上对结果进行评价，然后直接进行比较 利用剩余文档集（residual collection，所有文档集中除去用户判定的相关文档 后的文档集）对反馈后的结果进行评价。这种思路看上去更具现实性。不过，性能的度量结果 往往低于原始查询的结果。 伪相关反馈 伪相关反馈（pseudo relevance），也称为盲相关反馈（blind relevance feedback），提供了一 种自动局部分析的方法。它将相关反馈的人工操作部分自动化，因此用户不需要进行额外的交 互就可以获得检索性能的提升。 该方法首先进行正常的检索过程，返回最相关的文档构成初始集，然后假设排名靠前的 k 篇文档是相关的，最后在此假设上像以往一样进行相关反馈 间接相关反馈 在反馈过程中，我们也可以利用间接的资源而不是显式的反馈结果作为反馈的基础。这种 方法也常常称为隐式相关反馈（implicit relevance feedback）。 隐式反馈不如显式反馈可靠，但是 会比没有任何用户判定信息的伪相关反馈更有用 查询扩展 属于查询优化的全局方法，在不考虑查询及其返回文档情况下对初始查询进行扩展和重构的方法 主要思想是使用同义词词典对于查询词t进行自动扩展，其中同义词词典的构建方法共有3种，即： 简单辅助用户进行查询扩展 采用人工词典的方法 自动构建词典的方法","link":"/2020/09/19/information-retrieval-ch9/"},{"title":"ubuntu安装cuda和cudnn指南","text":"cuda，cudnn之间的关系 cuda安装 安装驱动的两种方式 在安装cuda时使用cuda默认的驱动，这时产生的问题是cuda默认的驱动一般不是官网上最新的驱动，在使用较新的显卡时（比如我们这回使用的3090），会导致显卡与驱动不适配的情况。 将驱动与cuda分开安装，先安装相应的驱动，再安装相应的cuda 安装前的下载 本文采用cuda和驱动分开安装，具体流程如下： 安装前先在NVIDIA官网查看Nvidia显卡的型号和驱动的对应情况: 然后在cuda官网查看cuda版本和驱动的对应情况： 然后开始正式安装 正式安装 禁用ubuntu系统自带的nouveau驱动： 1sudo gedit /etc/modprobe.d/blacklist.conf 在文件末尾添加： 1blacklist nouveau 然后执行如下命令跟新系统驱动： 1sudo update-initramfs -u 重启系统： 1sudo reboot now 检查是否禁用成功 1lsmod | grep nouveau 若没有输出则代表禁用成功 关闭图形界面： 首先按CTRL+ALT+F1（1~6均为命令行界面，7为图形界面）进入命令行界面 然后执行如下命令关闭图形界面： 1sudo service lightdm stop 安装驱动： 注意要带参数--no-opengl-files，否则会出现，安装后重启，输入用户名和密码，无法进入桌面，会一直让你输入用户名和密码的情况，网上把这种情况称为循环登录。 完整命令如下： 1sudo sh ./NVIDIA-Linux-x86_64-***.***.run --no-opengl-files 检测安装成果：输入如下指令 1nvidia-smi 若显示显卡则代表安装成功 安装cuda，因为我们已经安装好了驱动，因此在安装cuda时不再勾选安装驱动 打开图形界面： 1sudo service lightdm start 卸载NVIDIA驱动 首先执行官方推荐的卸载 1sudo /usr/bin/nvidia-uninstall 然后卸载相关的包 12sudo apt-get --purge remove nvidia*sudo apt autoremove 卸载cuda 首先执行官方推荐的卸载 1sudo /usr/bin/cuda-uninstall 然后卸载相关的包 12sudo apt-get --purge remove &quot;*cublas*&quot; &quot;cuda*&quot;sudo apt autoremove","link":"/2020/10/03/cuda-ubuntu-install/"},{"title":"对于transformer模型的学习","text":"整体架构 pic-1 encoders中多个堆叠的encoder之间不共享参数 encoder的个数与decoder的个数相同 其中每个encoder的内部构造如下所示： pic-2 self-encoder的作用，帮助编码器在编码特单词时留意其他的单词 其中decoder和encoder之间的连接如下： pic-3 transformer的主要优点在于其并行计算的能力，主要在于feed forward层面 pic-4 通过self-attention之后的词向量，分别经过相同的feed forward network（参数相同）因此可以并行 Encoder解读 self-attention的作用机制 对于句子“The animal didn't cross the street because it was too tired.”，利用self-attention可以将it与the animal产生关联，即通过句子中的其他词更好的解释当前词 pic-5 Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing. self-attention详解 此处介绍的是Encoder中的self-attention，不同于Decoder 对于其中一层的self-attention而言： pic-5 从向量X到向量Z的计算步骤为： 初始化三个权重矩阵\\[ W^Q \\]，\\[ W^K \\]，\\[ W^V \\]（这三个权重矩阵在训练过程中参数会得到训练，在代码中表示为经过了三个线性层），分别将单词的embedding与这三个矩阵相乘，得到每个embedding对应的Query，key，value向量 这一步的主要内容是来计算得分，即当前词和所输入句子中其他词相关性的得分。计算方法为：假设当前单词所处的位置为#1，计算其与#2位置的单词的相关性的方法为：用\\[q_1\\]和\\[k_2\\]进行点乘。举个栗子而言：假设输入的句子为“thinking machines”，其计算如下： 将得分除以8（key vector维度的平方根，paper中定义为64维），目的是为了维持更稳定的梯度。 将当前词与每个单词的得分通过softmax函数处理 根据#1，#2位置的softmax的值，计算加权的#1，#2位置单词对应的结果向量。self-attention执行完毕，接下来将向量送入前馈网络即可 multi-headed attention机制详解 本质是每次计算self-attention模块时，使用多组Query/Key/Value权重矩阵（Transformer使用的是8个），每一组矩阵代表一个头 执行结果如下： 由于feedforword模块只需要输入一个矩阵，因此执行如下操作： 多头注意力机制的可视化：我们可以看出每个attention关注的侧重点不同 对于单词的embedding加入位置和序列信息 之前值注重于考虑词与词之间的关联性信息，对于输入序列的单词在编码时没有考虑其位置信息，在此处加以处理 通过向量，表示出位置和序列信息，与原有信息进行加和。具体如下图所示： 残差连接 其可视化结果如下，在进行残差连接后，经过一个LayerNorm层 关于LayerNorm和BatchNorm之间的区别 Decoders解读 Decoders中含有和Encoders中数量相同的encoder，每一个Decoder的结构包含三层，分别是：self-attention，Encoder-Decoder attention，Feed forward层 其图示如下： transformer decoding Decoders中每一个Decoder的输出会传入上一层作为输入；第一层decoder之前也是加入位置信息的embeding层，如下图所示： Decoders是自回归的，意味着每次Decoders均只预测一个单词，每一次预测的输出都会累加起来重新作为对于下一次预测的输入（此处可以将Decoders看做一个RNN，具体如下图所示）——在输出停止符号&lt;end of sentence&gt;后停止迭代 decoder 比如：输入&lt;start&gt;得到“I” Decodrs的输入 Decoders的输出 &lt;start&gt; I &lt;start&gt; I am &lt;start&gt; I am fine &lt;start&gt; I am fine &lt;end&gt; Decoder中的self-attention不同于之前介绍的encoder中的self-attention，只对序列中的前面的向量计算softmax，而对后面的向量进行mask操作 Decoder和Encoder之间的合作 对于Encoder-Decoder attention的解读 Encoder-Decoder attention的本质还是self-attention，分别将encoder输出的序列经过线性变换得到k和v序列，将第一层经过self-attention层输出的向量作为q向量序列 对于最后的Linear and Softmax Layer的解读 主要作用在于将输出的向量转化为对应的单词 Linear Layer是一个全连接网络，用于将Decoders输出的向量映射到词汇表相应的维度，得到logits向量（即进行归一化前的对应到各个词的可能得分） Softmax layer则将各个维度对应的得分映射到 (0,1] 之间的概率，对应概率最高的维度对应的单词即为目标单词 参考及图片来源 [1] Illustrated Guide to Transformers- Step by Step Explanation [2] The Annotated Transformer harvard的源码解读 [3] The Illustrated Transformer","link":"/2020/10/13/learning-transformer/"},{"title":"对于Bert模型的学习","text":"BERT之前 ELMo的提出 ELMo属于一种word embeding，由于同一个单词可能有不同的含义，因此对于同一个单词应该有多重向量来表示。具体的应用在于，先观察整个句子，然后再为每个单词生成相应的embeding。 具体而言是将整个句子的初始embeding过一遍预训练的Bi-LSTM（language modeling）——本质上而言，也是在句子单侧的编码，为每个单词生成相应的embeding。如下图所示： 随后将这些隐藏层（包括初始的embeding）通过每种方式结合在一起 最后得到相应的embeding，如上图湖蓝色所示 ULM-FiT：明确的将迁移学习引入NLP ULM-FiT介绍了一个语言模型和一种将语言模型进行微调以适应各种其他任务的方法 Transformer：超过LSTMs 在处理长序列方面的能力超过了LSTM OpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling 将Transformer的Decoder改造为可以与训练的语言模型，The OpenAI Transformer通过7000本书来实现预训练 OpenAI提出的Transformer 迁移至下流任务 BERT BERT：From Decoders to Encoders 目标：改变Transformer模型，使其能够学习句子双侧的序列信息 使用Transformer的多层Encoder部分（而不是Decoder部分）来学习句子中词向量的表示，提出BERT的论文的全名是BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding，从其名字可以看出模型的特点：Pre-training、Deep、Bidirectional、Transformer、Language Understanding 由于语言模型的特点是：已知前N个词来预测第N+1个词，因此以往的语言模型均是单向学习的语言模型。而BERT提出遮蔽语言模型（masked language model，MLM），即在训练时随机MASK掉15%的单词，对于一个被MASK的单词： 有80%的概率用“[mask]”标记来替换 有10%的概率用随机采样的一个单词来替换 有10%的概率不做替换（虽然不做替换，但是还是要预测哈） 具体模型如下： bert-base-bert-large-encoders 即进行多层Transformer中的Encoder的堆叠，论文中的模型参数为： BERTBASE (L=12, H=768, A=12, Total Parameters=110M) BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M). BERT模型的训练特点 共分为两步进行训练： 对于被MASK的词汇进行预测：进行语言模型的学习 BERT-language-modeling-masked-lm 进行下一句预测：学习句子间的关系 其中[seq]为句子分隔符，每次输入两个句子，训练模型用来判断第二个句子是否是第一个句子的下一个 此后得到的模型为预训练的模型，可以作为后期词向量embeding的生成器 BERT模型的应用 Sentence Classification bert-classifier 在句子前面加入[CLS]标致，通过最后一层输出的[CLS]的向量表示来进行句子分类。因为该向量已经学得了整个句子的信息。因此像RNN一样不用考虑其他位置单词的信息。直接判断即可 Reference [1] The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) [2] NLP的游戏规则从此改写？从word2vec, ELMo到BERT [3] NLP必读：十分钟读懂谷歌BERT模型 [4] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","link":"/2020/10/18/learning-Bert/"},{"title":"无题","text":"我愿 升入青空 坠入深海 只愿 成长的忧伤 追不上我轻灵的魂魄","link":"/2020/10/25/poem/"},{"title":"新闻推荐现有工作的调研","text":"新闻推荐的特点 具有较强的时效性，相比于其它推荐任务，不能使用基于ID的MF方法，可用的用户交互数据较少 新闻文章往往简洁、准确。便于使用NLP模型进行处理 新闻中包含很多实体，往往成为文章的keyword；不同于之前的基于item的推荐，往往一个item只对应一个实体 新闻推荐的基本思路 对新闻进行分类，具体类别用向量来表示；对于用户的兴趣点采用相同向量空间来表示；最后将两者点乘得到数值表示用户对目标新闻的感兴趣的概率——即分别对news和user学得一个相同向量空间向量表示（不同向量空间则不能使用点乘，可用全连接网络） 相关论文 我们对新闻推荐的方法主要采取了两种分类方法，即是否使用知识图谱，是否使用时间序列。本文所包含的论文汇总如下： 题目 时间 会议 包含时间序列 包含知识图谱 TEKGR 2020 CIKM 是 KRED 2020 RecSys 是 LSTUR 2019 ACL 是 NPA 2019 KDD DKN 2018 WWW 是 是 DRN 2018 WWW NPA: LSTUR: 目前新闻推荐的缺点 将用户未点击均视为负样例，容易陷入信息茧房 利用知识图谱对news进行embedding时，有时候title中的entity并不能表示表示新闻的类别以及全部内容 知识图谱对entity进行embedding时，向外发散到其他实体时，目前两种主要做法是： 无差别发散； 加入权重进行发散，即GAT； 但是没有对知识图谱进行裁剪，噪音还是很大的","link":"/2020/12/05/news-recommend-papers/"},{"title":"实体消歧(Entity Disambiguation)","text":"为了将新闻标题中的实体与知识图谱中的实体相对应，我们需要先进行实体识别（NER），再进行实体与知识图谱的映射。在这个过程中受导师指点，需要考虑实体消歧（Entity Disambiguation）的问题。因此进行调研。 对于实体消歧问题的描述 在一段文本中，由于文本和语言的多态性，一个语言字段在不考虑上下文的情况下可能对应多个实体，为了将文中的实体与知识图谱中相应的实体正确对应，提出了实体消歧问题。 注：在文本中需要与知识图谱中的entity相对应的文字段称为mention 实体链接的步骤 一共分为两个步骤： 对于每一个mention产生一个候选实体集（candidate），实体集中的实体全部来自于知识图谱 对于候选实体集中的实体进行实体消歧，通过排序选出相关度最高的那个实体作为mention的对应实体 相关论文 题目 时间 会议 Collective Entity Linking in Web Text: A Graph-Based Method 2011 SIGIR NeuPL 2017 CIKM Pair-Linking for Collective Entity Disambiguation: Two Could Be Better Than All 2018 TKDE Collective Entity Linking in Web Text: A Graph-Based Method 创新点 设计了全局协同推理方法Collective Entity Linking，如下图例子所示： 即认为单独只依赖上下文standout career at Bulls, [] also acts in the movie无法直接推断出Jordan就是Michael Jordan，需要借助其他的mention对应的entity，即Chicago Bulls and Space Jam来进行推断，此种方法称为协同实体链接。 方法 对于文本During his standout career at Bull, Jordan also acts in the movie Space Jam. 构建Referent Graph，如下所示： 图中包含两种关系，即 Local Mention-to-Entity Compatibility: mention与entity之间的连接 Semantic Relation between Entities: entity之间的连接 NeuPL: Attention-based Semantic Matching and Pair-Linking for Entity Disambiguation Intro 本篇论文所需要解决的两个问题是： 充分利用上下文信息来消除歧义 增强linked entity之间的一致性 创新点 目标函数： \\[\\Gamma^{*}=\\underset{\\Gamma}{\\arg \\max }\\left[\\sum_{i=1}^{N} \\phi\\left(m_{i}, t_{i}\\right)+\\psi(\\Gamma)\\right]\\] 本文将目标函数拆成两个部分（模型）进行优化，即： local model 作用及含义：根据mention的context来推断mention与哪一个entity相对应，计算得到Local confidence or local score \\(\\phi\\left(m_{i}, t_{i}\\right)\\)反映了\\(m_{i} \\longmapsto t_{i}\\)的概率 现存的方法：用DNN对mentions的context进行建模，此种方法存在一些问题，没有充分利用local context的信息，即： 上下文中可能包含多个mention，DNN无法确定应该关注于哪一个mention 忽视了上下文中单词间的顺序 本文的改进： 通过两个LSTM来学习目标mention两侧的上下文信息 同时为了消除上下文的噪音以及学习对于不同mention的权重，引入了attention机制来提取信息 具体模型如下所示： global model 作用及含义：在一篇文章中通过多个实体间的关系来确定待确定的实体的对应 传统的方法： 构建整篇文章的指代实体图（Referent Graph），传统的collective entity linking方法在图中实体个数过多时会消耗过多的计算时间。 同时，也不是一篇文章中的所有实体都可以进行相互推断，比如The Sun and The Times reported that Greece will have to leave the Euro soon. 这句话中的关系如下： 观察发现，并不是所有实体间都有联系 本文的改进：论文中提出了Pair-Linking (PL)算法，特点是，只使用一次迭代便可以完成一篇文章中所有的mentions之间的collective linking，具体的做法见下面这篇文章中介绍 Pair-Linking for Collective Entity Disambiguation: Two Could Be Better Than All 2-4行：计算任意两个mention对应的的candidate entity set之间的最小语义相似距离（构建图中的边） 7-9行：类似Kruskal算法，每次选取confident最高（语义相似距离最小）的边 10-13行：一旦一个candidate entity set中的entity被选中，则此set中的其他entity被从图中抹去，更新剩余的集合间语义距离 可以图示如下： 算法停止时间：所有的实体集中都有一个实体被选中，不需要形成最小生成树","link":"/2020/11/01/Entity-Disambiguation/"},{"title":"推荐系统的评测标准","text":"hit@k 定义如下： In a nutshell, it is the count of how many positive triples are ranked in the top-n positions against a bunch of synthetic negatives. 具体计算方法为： In the following example, pretend the test set includes two ground truth positive only: 12Jack born_in ItalyJack friend_with Thomas Let's assume such positive triples (identified by * below) are ranked against four synthetic negatives each. Now, assign a score to each of the positives and its synthetic negatives using your pre-trained embedding model. Then, sort the triples in descending order. In the example below, the first triple ranks 2nd, and the other triple ranks first (against their respective synthetic negatives): 12345678910111213s p o score rankJack born_in Ireland 0.789 1Jack born_in Italy 0.753 2 *Jack born_in Germany 0.695 3Jack born_in China 0.456 4Jack born_in Thomas 0.234 5s p o score rankJack friend_with Thomas 0.901 1 *Jack friend_with China 0.345 2Jack friend_with Italy 0.293 3Jack friend_with Ireland 0.201 4Jack friend_with Germany 0.156 5 Then, count how many positives occur in the top-1 or top-3 positions, and divide by the number of triples in the test set (which in this example includes 2 triples): 12Hits@3= 2/2 = 1.0Hits@1= 1/2 = 0.5 ndcg@k 全名Normalized Discounted Cumulative Gain，是一个测量排序质量的指标。我们将会分成三个步骤来介绍，即： Cumulative Gain(CG) Discounted Cumulative Gain(DCG) Normalized Discounted Cumulative Gain(NDCG) CG 每一个待推荐的item都有一个相关性的得分，所有相关性得分总和即为CG，对于推荐系统给出的已排序（按照推荐算法）序列A，其中相关性得分（与推荐系统的推荐顺序无关）和CG分别如下所示： DCG 对于推荐系统给出的两个有序推荐序列A和B，虽然B的结果比A的要好（按相关性从大到小进行排序），但是按照CG进行评测两者表现效果相同，因此单纯依靠CG存在很大的缺陷 因此我们提出DCG，希望能够测量推荐系统能否将待推荐的item按照相关性的降序进行排列，其公式如下： $$DCG=\\sum_{i=1}^{n} \\frac{2^{r e l e v a n c e_{i}}-1}{\\log _{2}(i+1)}$$ 或者 $$DCG=\\sum_{i=1}^{n} \\frac{\\text {relevance}_{i}}{\\log _{2}(i+1)}$$ 其中第一个公式对于具有较高相关性但在推荐序列中排名较后的item具有很大的惩罚，可以作为对于推荐系统排序能力进行测量的工具。对于前面举例子的集合A，B，其DCG的得分如下： 如果相关性用0/1来表示，则两个公式的效果相同 NDCG 由于不同推荐系统对于不同用户给出的待推荐序列的长度不同，相关度的范围不同，因此DCG无法作为一个通用的衡量标准，为了使其通用化，我们引入了NDCG，对于每一个推荐系统生成的序列，其计算方法如下： 按照推荐系统给出的顺序计算DCG 按照相关性排序的顺序计算DCG，带到iDCG 求出两者的比率DCG/iDCG，取值范围应该在0,1之间 举例如下，对于推荐系统给出的序列： 按照相关性排序可以得到： 两个序列DCG分别计算如下： 因此该序列NDCG为： referrence [1] How is hits@k calculated and what does it mean in the context of link prediction in knowledge bases [2] Evaluate your Recommendation Engine using NDCG","link":"/2020/11/06/performance-measure-of-resys/"},{"title":"Graph Transformer Networks 论文阅读","text":"目前挑战 目前GNN被设计来在固定和同质图上学习节点表示，在以下情况下会存在缺陷： 图中的连接存在错误 包含不同类型节点和边的异质图中 目前的解决办法是使用两阶段方法： 手动设计meta-path，需要专家和领域知识 通过meta-path将异质图转化为同质图，然后使用GNN 本文是对上述方法的改进 主要贡献 提出Graph Transformer Networks（GTN），其特点是：能够产生新的图结构，即识别出原本未连接的节点间的有用连接，从而学得更好的节点表示，不需要依赖领域知识 新图的生成是可解释的，自动生成meta-path，不需要人为设定，meta-path的生成更加有效 先置概念 meta-path： 对于关系：\\(A \\stackrel{A P}{\\longrightarrow} P \\stackrel{P C}{\\longrightarrow} C\\)，其meta-path为\\(A_{A P}A_{P C}\\) 模型 异质图中存在多种关系，用一个邻接矩阵表示一种关系，整个异质图用K个邻接矩阵来表示（一共有K种关系） Graph Transformer (GT) Layer 核心部分是Graph Transformer (GT) Layer，由两个步骤组成： GT layer从邻接矩阵集合\\(\\mathbb{A}\\)中软选择两个邻接矩阵Q1,Q2 通过两个关系矩阵Q1,Q2，学到新的图结构 对于权重向量\\(\\boldsymbol{W}_{\\phi}^{1}\\)和\\(\\boldsymbol{W}_{\\phi}^{2}\\)（1*1*k）分别进行softmax操作，然后作为1*1的卷积核对异构邻接矩阵集合中的全部K个邻接矩阵进行卷积操作。最后分别得到两个新的邻接矩阵Q1,Q2，将两个矩阵相乘得到二阶meta-path图\\(A^{(1)}\\)。每进行一次新矩阵间的相乘阶数都会升高一阶。 整体模型 相比于上面介绍的Graph Transformer (GT) Layer，整体模型使用multi-channel（使用了C个通道），这样最后得到的\\(\\mathbb{A}^{(l)}\\)就是由C个邻接矩阵组成的图集和。对于每一个矩阵分别进行卷积，得到C个向量表示，将这C个向量表示拼接起来得到目标node的表示。 为了得到L-hop间的信息，一共设计了L个multi-channel矩阵，这样就可以考虑到L-hop范围内的信息。 反思 可以尝试使用到知识图谱中，对知识图谱进行剪裁，前提是知识图谱中的关系的种类是有限的 reference","link":"/2020/11/09/graph-transformer-net/"},{"title":"对于GPT-2模型的学习(未完待续)","text":"GPT-2与BERT都是Transformer模型的衍生物，其中BERT是仅仅对Transformer模型中的encoder部分进行改造；而GPT-2是对Transformer模型中的decoder部分进行改造 对于Transfoemer的理解可以看之前看这里，BERT的理解可以看这里，接下来我们将详细介绍GPT-2，我们将分为如下几个部分介绍： 整体模型：大致了解模型的整体结构 Decoder详解：详解单个decoder中的计算原理 GPT-2的应用：详解模型如何在实际中进行应用 整体模型 GPT-2 使用了Transformer的decoder部分，将多个decoder堆叠构成了decoder栈","link":"/2020/11/14/learning-gpt2/"},{"title":"Leveraging Demonstrations for Reinforcement Recommendation Reasoning over Knowledge Graphs 论文阅读","text":"Leveraging Demonstrations for Reinforcement Recommendation Reasoning over Knowledge Graphs 模型整体是基于user-item之间的path来提升推荐的准确性和可解释性的。 整个方法分类两个部分，首先构建一些不完整、不全面的path，来引导随后的ADAC模型来学习一些path；随后通过ADAC模型来学习path Demonstration Extraction 用来通过Meta-heuristics产生一些样例，可供后面的模型进行学习 首先，我们在设计元启发规则时，应当考虑三个方面的特性： Accessibility：通过最小的标签努力可以获得样例 Explainability：要比随机采样的path更具有解释性 Accuracy：样例要能够连接已观测到的user-item之间的交互 我们得到的启发式规则如下： Shortest path：多条路径时，使用Dijkstra算法寻找最短路径 Meta-path：设计几条meta-path用于之后的识别 Path of interest：通过path中用户感兴趣的entity的个数；来判断 meta-path的理解： \\(B o b \\stackrel{\\text { Purchase }}{\\longrightarrow}\\) Revolution 5 Running Shoe\\(\\frac{\\text {Produced_By}}{\\longrightarrow}\\) Nike \\(\\frac{\\text {Produce}}{\\longrightarrow}\\) Acalme Sneaker User \\(\\stackrel{\\text {Purchase}}{\\longrightarrow}\\)Item\\(\\frac{\\text {Produced}_{-} \\mathrm{By}}{\\longrightarrow}\\) Brand \\(\\stackrel{\\text {Produce}}{\\longrightarrow}\\)Item Adversarial Actor-Critic for Path Finding 这个算法将actor-critic-based reinforcement learning和adversarial imitation learning相结合 MDP环境 用于通知actor它现在的搜索状态，以及可能采取的行为，同时对actor产生奖励 其定义为一个四元组\\((\\mathcal{S}, \\mathcal{A}, \\delta, \\rho)\\)， state 只考虑K步以内的entity和relation作为状态 action 所有能够与当前实体连接的下一个实体的集合 transition \\(s_{t+1}=\\delta\\left(s_{t}, a_{t}\\right)\\) reward 只对最终状态进行奖励，对于中间的寻路过程不进行奖励 Actor 用于通过全连接网络来学习根据当前状态\\(s_{t}\\)以及当前所有的行动空间\\(\\mathcal{A}_{t}\\)，采取每一种行动的概率\\(p\\left(a_{t} \\mid s_{t}, \\mathcal{A}_{t}\\right)\\)，用\\(\\pi_{\\theta}\\left(a_{t}, s_{t}, \\mathcal{A}_{t}\\right)\\)来表示 学习过程如下： \\(h_{\\theta}=\\operatorname{ReLU}\\left(W_{\\theta, 1} s_{t}\\right)\\) \\(p\\left(a_{t} \\mid s_{t}, \\mathcal{F}_{t}\\right)=\\pi_{\\theta}\\left(a_{t}, s_{t}, \\mathcal{F}_{t}\\right)=\\frac{\\mathbf{a}_{t} \\cdot \\operatorname{ReLU}\\left(W_{\\theta, 2} h_{\\theta}\\right)}{\\sum a_{i} \\in \\mathcal{A}_{t} \\mathbf{a}_{i} \\cdot \\operatorname{ReLU}\\left(W_{\\theta, 2} h_{\\theta}\\right)}\\) 其中\\(\\mathbf{s}_{t}=\\mathbf{u} \\oplus \\mathbf{e}_{t-K} \\oplus \\ldots \\mathbf{e}_{t-1} \\oplus \\mathbf{r}_{t} \\oplus \\mathbf{e}_{t}\\)，不够k，用padding补齐 Adversarial Imitation Learning 由两个识别器组成，分别是 a path discriminator and a meta-path discriminator，分别用来识别actor产生的路径是否与path，以及meta-path相似 Path discriminator：用于识别path形成过程中的每一步是否与样本的path相似，计算公式如下： \\(h_{p}=\\tanh \\left(\\mathbf{s}_{t} \\oplus \\mathbf{a}_{p, t}\\right)\\) \\(D_{p}\\left(s_{t}, a_{t}\\right)=\\sigma\\left(\\beta_{p}^{T} \\tanh \\left(W_{p} h_{p}\\right)\\right)\\)，我们训练\\(D_{p}\\left(s_{t}, a_{t}\\right)\\)，使得它的可以识别当前path片段与样例的相似程度 meta-path discriminator：用于在path最终形成时，识别当前path是否与样例的meta-path相似 对于meta-path的embedding\\(\\mathbf{M}=\\mathbf{r}_{1} \\oplus \\mathbf{r}_{2} \\oplus \\ldots \\mathbf{r}_{T}\\)，其学习公式如下： \\(\\boldsymbol{h}_{m}=\\tanh \\left(\\boldsymbol{W}_{m, 1} \\mathbf{M}\\right)\\) \\(D_{m}(\\mathbf{M})=\\sigma\\left(\\boldsymbol{\\beta}_{m}^{T} \\tanh \\left(\\boldsymbol{W}_{m, 2} \\boldsymbol{h}_{m}\\right)\\right)\\) Critic 用于建模来自MDP和两个识别器对于actor的奖励 实验结果","link":"/2020/11/23/ADAC-model/"},{"title":"知识图谱与推荐系统","text":"论文阅读 本文所包含的论文汇总： 题目 时间 会议 包含时间序列 包含知识图谱 数据集 CKAN 2020 SIGIR 是 KARN 2020 AAAI 是 是 Amazon review dataset KNI 2020 DLP-KDD 是 MovieLens-20M，Book-Crossing，Last.FM KGAT 2019 KDD 是 KGCN 2019 WWW 是 RippleNet 2018 CIKM 是 对于使用知识图谱的推荐系统，可以被分为两类，即： 利用知识图谱进行knowledge graph embedding (KGE)，从而得到包含更多语义信息的item的embedding 利用知识图谱中的路径进行path-based methods，提取meta-path/meta-graph CKAN: Collaborative Knowledge-aware Attentive Network for Recommender Systems Intro 文章认为现有的基于KG的方法存在着一些问题，即： 只利用了知识图谱中entity之间的连接信息来丰富item的语义信息，没有利用collaborative signals 对此本文提出新的模型Collaborative Knowledge-aware Attentive Network (CKAN)，用于将collaborative signals和knowledge associations结合起来 collaborative signals: which assumes that users with similar behaviors may have similar appetites for items, 创新点 Heterogeneous propagation: composed of collaboration propagation and knowledge graph propagation, views interaction and knowledge as information in two different spaces and combines them in a natural way. Knowledge-aware attentive embedding: a brand new knowledge-aware neural attention mechanism proposed for learning different weights of neighbors in KG when they are in different conditions. 模型 Heterogeneous Propagation Layer 用于融合知识图谱和协同信号 Collaboration Propagation 对于item的neighbor的确定： \\[ \\mathcal{V}_{v}=\\left\\{v_{u} \\mid u \\in\\left\\{u \\mid y_{u v}=1\\right\\} \\text { and } y_{u v_{u}}=1\\right\\} \\] \\[ \\mathcal{E}_{v}^{0}=\\left\\{\\mathrm{e} \\mid\\left(v_{u}, \\mathrm{e}\\right) \\in \\mathcal{A} \\text { and } v_{u} \\in \\mathcal{V}_{v}\\right\\} \\] 对于user的neighbor的确定： \\[ \\mathcal{E}_{u}^{0}=\\left\\{\\mathrm{e} \\mid(v, \\mathrm{e}) \\in \\mathcal{A} \\text { and } v \\in\\left\\{v \\mid y_{u v}=1\\right\\}\\right\\} \\] Knowledge Graph Propagation 对于上一阶段得到的seed set，在知识图谱中逐层传播，每一层得到的entity作为一个集合保存 $$ \\mathcal{S}_{o}^{l}=\\left\\{(h, r, t) \\mid(h, r, t) \\in \\mathcal{G} \\text { and } h \\in \\mathcal{E}_{o}^{l-1}\\right\\}, \\quad l=1,2, \\ldots, L $$ Knowledge-aware Attentive Embedding Layer 对于每一个扩展出来的entity，使用attention进行计算，attention机制使用两层全连接网络实现： \\[ \\begin{aligned} \\mathrm{z}_{0} &amp;=\\operatorname{ReLU}\\left(\\mathrm{W}_{0}\\left(\\mathrm{e}_{i}^{h} \\| \\mathrm{r}_{i}\\right)+\\mathrm{b}_{0}\\right) \\\\ \\pi\\left(\\mathrm{e}_{i}^{h}, \\mathrm{r}_{i}\\right) &amp;=\\sigma\\left(\\mathrm{W}_{2} \\operatorname{ReLU}\\left(\\mathrm{W}_{1} \\mathrm{z}_{0}+\\mathrm{b}_{1}\\right)+\\mathrm{b}_{2}\\right) \\end{aligned} \\] 最后经过softmax得到如下： \\[ \\pi\\left(\\mathrm{e}_{i}^{h}, \\mathrm{r}_{i}\\right)=\\frac{\\exp \\left(\\pi\\left(\\mathrm{e}_{i}^{h}, \\mathrm{r}_{i}\\right)\\right)}{\\sum_{\\left(h^{\\prime}, r^{\\prime}, t^{\\prime}\\right) \\in \\mathcal{S}_{o}^{l}} \\exp \\left(\\pi\\left(\\mathrm{e}_{i}^{h^{\\prime}}, \\mathrm{r}_{i}^{\\prime}\\right)\\right)} \\] 其中第0层的计算略有不同，因为第0层的贡献都很大，因此采用如下的方法： \\[ \\mathrm{e}_{o}^{(0)}=\\frac{\\sum_{\\mathrm{e} \\in \\mathcal{E}_{o}^{0}} \\mathrm{e}}{\\left|\\mathcal{E}_{o}^{0}\\right|} \\] 对于item而言： \\[ \\mathrm{e}_{v}^{(\\text {origin})}=\\frac{\\sum_{\\mathrm{e} \\in\\{\\mathrm{e} \\mid(\\mathrm{e}, v) \\in \\mathcal{A}\\}} \\mathrm{e}}{|\\{\\mathrm{e} \\mid(\\mathrm{e}, v) \\in \\mathcal{A}\\}|} \\] \\[ \\mathcal{T}_{v}=\\left\\{\\mathrm{e}_{v}^{(\\text {origin})}, \\mathrm{e}_{v}^{(0)}, \\mathrm{e}_{v}^{(1)}, \\ldots, \\mathrm{e}_{v}^{(L)}\\right\\} \\] 对于user而言： \\[ \\mathcal{T}_{u}=\\left\\{\\mathrm{e}_{u}^{(0)}, \\mathrm{e}_{u}^{(1)}, \\ldots, \\mathrm{e}_{u}^{(L)}\\right\\} \\] Model Prediction Layer Sum aggregator：将上一个模块的每一层向量直接相加 \\[ \\operatorname{agg}_{s u m}^{(o)}=\\sigma\\left(\\mathrm{W}_{a} \\cdot \\sum_{e_{o} \\in \\mathcal{T}_{o}} \\mathrm{e}_{o}+\\mathrm{b}_{a}\\right) \\] Pooling aggregator：去上一模块中所有层的向量中每一维度选取最大的 \\[ a g g_{\\text {pool}}^{(o)}=\\sigma\\left(\\mathrm{W}_{a} \\cdot \\text {pool}_{\\max }\\left(\\mathcal{T}_{o}\\right)+\\mathrm{b}_{a}\\right) \\] Concat aggregator：将每一层向量进行拼接 \\[ a g g_{\\text {concat}}^{(o)}=\\sigma\\left(\\mathrm{W}_{a} \\cdot\\left(\\mathrm{e}_{o}^{\\left(i_{1}\\right)}\\left\\|\\mathrm{e}_{o}^{\\left(i_{2}\\right)}\\right\\| \\ldots \\| \\mathrm{e}_{o}^{\\left(i_{n}\\right)}\\right)+\\mathrm{b}_{a}\\right) \\] 点击率的预测函数： \\[ \\hat{y}_{u v}=\\mathrm{e}_{u}^{\\top} \\mathrm{e}_{v} \\] 实验结果 KARN: A Knowledge-Aware Attentional Reasoning Network for Recommendation Intro 之前的工作要么只考虑使用知识图谱来解释user与item在知识图谱中之间path的合理性，要么只考虑通过用户的历史点击序列来预测用户的兴趣。本文将结合两者进行预测，这种预测方式将提高预测的准确率，具体如下： 用户Jark的历史记录为3-&gt;2-&gt;1，如果不考虑历史记录，只根据1和知识图谱进行推荐，则由于Titantic的导演是James Cameron，因此可能推荐他执导的另外两部电影Piranha2和Aliens，但是通过历史记录可以得到Jark喜欢的电影类型更偏向于Adventure类型的电影，因此更有可能推荐Aliens而不是Piranha2 创新点 提出KARN将用户的历史点击序列和知识图谱中的路径信息相结合 对于item进行向量表示时，不仅考虑了textual（tittle）的信息，还考虑了contextual（its one-hop neighbors in KGs）的信息 对于item进行向量表示时，不仅考虑用户的历史点击序列，还通过user和item之间的路径考虑了user对于该item的潜在意图 模型 对于item的表示 其中 对于tittle中的每一个词的embedding组成的矩阵用F个卷积核进行卷积操作，用于提取局部语义信息（多个词组成的phrase的含义），最后经过一层变换得到与KG中实体维度相同向量V‘ 对于该商品在KG中对应的entity的embedding用V来表示 对于该商品1-hop之内的neighbors entity采用平均的方法来得到向量V’‘ 则item的表示最终为[V, V' , V''] 对于user history的表示 对于历史点击的item，通过SRA网络来学习，SRA将RNN+attention得到的结果与RNN最后一个隐状态进行拼接来表示user history interest User potential intent 即用户对于目标商品的潜在意图，通过所有在知识图谱中由用户到商品的路径来计算得出 对于每一条路径，e表示实体，r表示关系： 我们将（ei,ri）视为一个整体用一个向量来表示，在路径最后补上空关系rq，随后将将整条路径用多个这样的向量来表示，最后通过SRA学得整个路径的表示。将所有路径通过attention net得到User对于item的potential intent 整体网络 实验结果 反思 实验存在缺陷，没有与其他使用知识图谱的模型进行对比，即KGCN，KGAT，RippleNet 无法应用于新闻推荐，新闻的待推荐item的数量太多，无法每一个都参与user potential intent的计算 KGCN: Knowledge Graph Convolutional Networks for Recommender Systems Intro 每个item都有很多属性，这些属性之间能够相互联系，从而形成一张知识图谱。利用这张知识图谱，我们可以发现item之间的高阶语义信息和结构信息。从而做出更加准确的推荐。对于知识图谱的处理我们提出了KGCN的方法，即将GCN运用到知识图谱中，用以形成对于每个entity（item）的表示。 模型 如上图所示，KGCN在本文中的应用是：规定感受野的跳数H（图中为2），以及每个实体单跳感受野中要考虑的节点个数（图中为2）。则KGCN的计算顺序是：不学习最外层的节点，先学习红色的节点，后学习蓝色的节点。 整体算法如下所示： 注意：当计算一个实体的上下文感受野信息时，对于单跳感受野中每个实体的权重设置不同 算法图示如下： 实验结果 与其他baseline的对比结果如下： PER需要人工设计meta-path，因此准确性会低很多 同时通过实验探究了K，H，dimension的大小对于实验准确性的影响 反思 对于感受野中固定实体的选择采用的是同一的方法，未来可以根据实体的重要性进行选择 将user和item分隔开，只对item端使用KG进行表示学习，没有尝试加入user的信息或者对user也使用KG进行表示学习 KNI: An End-to-End Neighborhood-based Interaction Model for Knowledge-enhanced Recommendation 创新点 针对之前的模型，我们提出了其存在的early summarization problem，针对这一问题，我们采用Neighborhood Interaction (NI) model，相比较早期仅仅只用item的向量和user的向量计算点积率。我们这里使用item的neighborhood和user的neighborhood进行计算，可以捕捉珍贵和复杂的结构信息 “early summarization”：仅将user/item自身及周围的邻居信息全部汇聚到一个向量，这种方式的缺点是没有利用到item/user的local structures，即item neighborhoods与user neighborhoods之间的联系没有捕捉到。只捕捉到了目标item与user之间这一条边的关系 为了进一步丰富节点间的连接信息和高阶结构化信息，我们引入了知识图谱，用来增加节点间的连接。因此我们称这种方法为KNI 模型 NI模型 计算user对于item期望值的模型 \\[ \\begin{aligned} \\text { Attention: } \\hat{y}_{u, v} &amp;=\\left\\langle\\sum_{i \\in N_{u}} \\alpha_{u, i} \\mathbf{x}_{i}, \\sum_{j \\in N_{v}} \\alpha_{v, j} \\mathbf{x}_{j}\\right\\rangle \\\\ &amp;=\\sum_{i \\in N_{u}} \\sum_{j \\in N_{v}} \\alpha_{u, i} \\alpha_{v, j}\\left\\langle\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right\\rangle \\end{aligned} \\] 其中\\(N_{u}\\)为user的neighborhoods，\\(N_{\\mathcal{V}}\\)为item的neighborhoods。简写如下： \\[ \\hat{y}=\\mathrm{~A} \\odot \\mathrm{Z} \\] 其中： \\[ \\sum_{i, j} \\mathrm{~A}_{i, j}=1, \\mathrm{Z}_{i, j}=\\left\\langle\\mathrm{x}_{i}, \\mathrm{x}_{j}\\right\\rangle \\] 对于其中权重的计算，论文中提出如下bi-attention network： \\[ \\alpha_{i, j}=\\operatorname{softmax}_{i, j}\\left(\\mathbf{w}^{\\top}\\left[\\mathbf{x}_{u}, \\mathbf{x}_{i}, \\mathbf{x}_{v}, \\mathbf{x}_{j}\\right]+b\\right) \\] node embedding模型 通过GCN，GAT来学到对于user/item的embedding，用学得的embedding来替代NI模型中的\\(\\mathbf{X}_{i}, \\mathbf{X}_{j}\\) 假设当前计算的是node u的表示，则i是所有与u在one-hop之内相连的节点，最外层的节点\\(\\mathbf{X}_{j}\\)用初始向量进行计算，讲过一次卷积得到node i的表示 \\[ \\mathbf{x}_{i}^{1}=\\sigma\\left(\\frac{1}{\\left|N_{i}\\right|} \\sum_{j \\in N_{i}} \\mathbf{w}^{1} \\mathbf{x}_{j}+\\mathbf{b}^{1}\\right) \\] 将node i的表示带入node u的卷积计算中，得到node u的表示 \\[ \\mathbf{x}_{u}^{2}=\\sigma\\left(\\frac{1}{\\left|N_{u}\\right|} \\sum_{i \\in N_{u}} \\mathbf{w}^{2} \\mathbf{x}_{i}^{1}+\\mathbf{b}^{2}\\right) \\] #### 引入知识图谱的NI模型 即KNI模型 在user和item之间加入KG的entity来扩充图中的关系和实体 整体计算流程 通过GNN逐层传递周围2-hop内的信息到目标节点，计算两个节点的embedding 通过计算到的embedding，计算目标节点的neighborhood之间的期望得分 实验结果 实验证明NI模型的计算是有用的 反思 将user-item之间的交互转化为neighbor-neighbor之间的交互，摆脱了双塔模型，相当于基于图的计算 KGAT: Knowledge Graph Attention Network for Recommendation RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems Intro 现在的基于知识图谱的推荐系统基本上分成两种方法，即： embedding-based methods: 利用知识图谱中目标entity周围的entity的embedding的信息来丰富单个entity的信息，一般使用GCN算法或者GAT算法 path-based methods: 一般需要人工设计meta-path/meta-graph，因此模型学习效果的好坏很大程度依赖于meta-path/meta-graph的设计，并且meta-path/meta-graph很难在学习的过程中被优化 而RippelNet将两者相结合，既考虑了embedding的问题，又考虑了path的问题。解决这一问题的方法被称为preference propagation，借助KG，更加有利于发现用户的高层潜在兴趣。 For each user, RippleNet treats his historical interests as a seed set in the KG, then extends the user’s interests iteratively along KG links to discover his hierarchical potential interests with respect to a candidate item. 模型 对于一次点击历史中的一个实体，其知识图谱中的rippel如下所示： 对于每一次迭代的entity的集合定义如下： \\[ \\mathcal{E}_{u}^{k}=\\left\\{t \\mid(h, r, t) \\in \\mathcal{G} \\text { and } h \\in \\mathcal{E}_{u}^{k-1}\\right\\}, \\quad k=1,2, \\ldots, H \\] 其中： \\[ \\mathcal{E}_{u}^{0}=\\mathcal{V}_{u}=\\left\\{v \\mid y_{u v}=1\\right\\} \\] 即一开始的seed集合是一个用户所有点击的历史记录中的item对应的entity 以第一轮rippel传播为例 每一个传播到的entity对应的权重的计算方式如下： \\[ p_{i}=\\operatorname{softmax}\\left(\\mathrm{v}^{\\mathrm{T}} \\mathbf{R}_{i} \\mathbf{h}_{i}\\right)=\\frac{\\exp \\left(\\mathrm{v}^{\\mathrm{T}} \\mathbf{R}_{i} \\mathbf{h}_{i}\\right)}{\\sum_{(h, r, t) \\in \\mathcal{S}_{u}^{1}} \\exp \\left(\\mathrm{v}^{\\mathrm{T}} \\mathrm{Rh}\\right)} \\] 每一轮传播过后得到的用户表示（局部）为： \\[ \\mathbf{o}_{u}^{1}=\\sum_{\\left(h_{i}, r_{i}, t_{i}\\right) \\in \\mathcal{S}_{u}^{1}} p_{i} \\mathbf{t}_{i} \\] 最后通过局部用户表示得到的全局用户表示为： \\[ \\mathbf{u}=\\mathbf{o}_{u}^{1}+\\mathbf{o}_{u}^{2}+\\ldots+\\mathbf{o}_{u}^{H} \\] 点击率预测方法为： \\[ \\hat{y}_{u v}=\\sigma\\left(\\mathbf{u}^{\\mathrm{T}} \\mathbf{v}\\right) \\] 特殊 使用了Memory Networks 实验结果 对于baseline的实验 对于hop number的实验 case study future work further investigate the methods of characterizing entity-relation interactions; design non-uniform samplers during preference propagation to better explore users’ potential interests and improve the performance.","link":"/2020/12/07/recsys-with-kg/"},{"title":"(DKN)Deep Knowledge-Aware Network for News Recommendation 论文阅读","text":"目前新闻推荐的痛点 时效性强，无法使用传统的基于ID的协同过滤的方法 用户的兴趣往往涉及许多，我们需要动态的测量用户的兴趣 新闻中往往包含许多实体和常识，可以借助来进行知识图谱上的推理 创新点 learn user representations based on the relevance between the representations of clicked and candidate news News encoder 对于每一个news tittle一共要学习三个层面得embedding，分别是word embedding, entity embedding以及contextual entity embedding，其中entity embedding和contextual entity embedding的学习如下： Entity embedding 利用知识图谱丰富文本表示信息，即知识蒸馏的过程 Illustration of knowledge distillation process 先对Tittle中的文本进行entity linking，得到一些实体 将识别出来的实体映射到原始的知识图谱中，丰富实体间的关系，并提取出子图 通过TransE，TransH，TransR，TransD中的一种得到每一个entity的embedding Contextual entity embedding Illustration of context of an entity in a knowledge graph 考虑每个entity在知识图谱中one-hop范围内的所有entity，采用如下方法作为目标entity的Contextual entity embedding： \\(\\operatorname{context}(e)=\\left\\{e_{i} \\mid\\left(e, r, e_{i}\\right) \\in \\mathcal{G}\\right.\\) or \\(\\left.\\left(e_{i}, r, e\\right) \\in \\mathcal{G}\\right\\}\\) \\(\\overline{\\mathrm{e}}=\\frac{1}{\\mid \\text {context}(e) \\mid} \\sum_{e_{i} \\in \\operatorname{context}(e)} \\mathbf{e}_{i}\\) 模型 Illustration of the DKN framework 输入：one piece of candidate news and one piece of a user’s clicked news 输出：click-through rate (CTR) prediction 此处的user embedding对于不同的candidate news是不同的，因为这里对不同的candidate news使用了注意力机制，对于不同的用户历史记录使用了不同的权重 核心组件：KCNN 模型源自于Kim CNN，如下图所示： A typical architecture of CNN for sentence repre-sentation learning 扩展到本模型中： KCNN 特点： 多通道：将word embedding, entity embedding, and contextual entity embedding of news作为news编码的三个通道 word-entity-aligned：将每一个word与相应的entity对齐（疑问：有些单词没有entity与之对应，需要检查代码） 使用多个filter User encoder: Attention-based User Interest Extraction user encoder 我们认为用户的每一次历史点击对于预测用户是否对于candidate news感兴趣的贡献度不同；因此使用attention机制来学习不同的贡献度，其计算方法如下： \\[ s_{t_{k}^{i}, t_{j}}=\\operatorname{softmax}\\left(\\mathcal{H}\\left(\\mathbf{e}\\left(t_{k}^{i}\\right), \\mathbf{e}\\left(t_{j}\\right)\\right)\\right)=\\frac{\\exp \\left(\\mathcal{H}\\left(\\mathbf{e}\\left(t_{k}^{i}\\right), \\mathbf{e}\\left(t_{j}\\right)\\right)\\right)}{\\sum_{k=1}^{N_{i}} \\exp \\left(\\mathcal{H}\\left(\\mathbf{e}\\left(t_{k}^{i}\\right), \\mathbf{e}\\left(t_{j}\\right)\\right)\\right)} \\] \\(t_{k}^{i}\\)表示用户i的第k次点击；\\(t_{j}\\)表示candidate news；\\(\\mathcal{H}\\)表示DNN网络 实验结果 对比其他模型 消融实验","link":"/2021/03/01/newsRecsys-DKN/"},{"title":"(KRED)Knowledge-Aware Document Representation for News 论文阅读","text":"Intro 正如文章的名字Knowledge-Aware Document Representation for News Recommendations，这篇文章更注重的是创建一个位于任务上游的news representation，为位于下游的新闻推荐子任务提供类似BERT在NLU领域的工具。 创新点 认为现存的文档理解模型要么是没有考虑知识图谱信息，比如BERT；要么是依赖于特殊的文章编码模型，以至于缺乏泛化能力和效率，比如DKN。本篇文章将知识图谱与BERT相结合 对于每一个实体在经过知识图谱表示层后，又加入了上下文信息，即context embedding层 模型 整体模型分为三层，分别是： Entity Representation Layer：使用Knowledge Graph Attention (KGAT) Network，将知识图谱中当前实体周围实体的信息汇聚与当前实体，用于增强对于当前实体的表示。其具体公式如下： \\[ \\mathbf{e}_{\\mathcal{N}_{h}}=\\operatorname{ReLU}\\left(\\mathbf{W}_{0}\\left(\\mathbf{e}_{h} \\oplus \\sum_{(h, r, t) \\in \\mathcal{N}_{h}} \\pi(h, r, t) \\mathbf{e}_{t}\\right)\\right) \\] 其中Nh表示以h为头部实体的三元组，\\(\\pi(h, r, t)\\)表示注意力权重，用来控制周围节点传播多少信息量到中心节点，其计算方式如下： \\[ \\pi_{0}(h, r, t)=\\mathbf{w}_{2} \\operatorname{Re} L U\\left(\\mathbf{W}_{1}\\left(\\mathbf{e}_{h} \\oplus \\mathbf{e}_{r} \\oplus \\mathbf{e}_{t}\\right)+\\mathbf{b}_{1}\\right)+b_{2} \\] \\[ \\pi(h, r, t)=\\frac{\\exp \\left(\\pi_{0}(h, r, t)\\right)}{\\sum_{\\left(h, r^{\\prime}, t^{\\prime}\\right) \\in \\mathcal{N}_{h}} \\exp \\left(\\pi_{0}\\left(h, r^{\\prime}, t^{\\prime}\\right)\\right)} \\] Context Embedding Layer：为每一个实体向量加入位置、频率、类别这三种信息，通过wise-to-wise的直接相加 Information Distillation Layer：使用类似self-attention的机制来得到news representation 模型 随后，为了提升这个上游模型的性能，对该模型进行Multi-Task的学习（包含Category Classification、Popularity Prediction、Local News Detection、Item Recommendation、Item-to-item Recommendation），一共分为两个阶段： stage1：we alternately train different tasks every few mini-batches. stage2：we only include the target task’s data to finalize a task-specific model. 反思 对于知识图谱的使用：其中对于知识图谱的使用也是在news representation的阶段，处于整体模型的第一层","link":"/2021/03/01/newsRecsys-KRED/"},{"title":"(TEKGR)News Recommendation with Topic-Enriched Knowledge Graphs 论文阅读","text":"Intro 之前基于知识图谱的推荐模型的存在者不足，即：仅仅根据新闻标题中被识别的实体来表示新闻，但往往存在错误。如下表所示，通过训练集可以看出用户感兴趣的是Marijuana Stock，而DKN模型通过实体“Cannabis” or “Marijuana”因此会将用户的兴趣点识别为Politics and Crime，而用户关心的往往是“Prices” or “Stocks”无法被识别为特殊实体。 为了解决这个问题，我们给每条新闻加入类别信息，不同于传统的人工加类别信息，我们通过神经网络和知识图谱来学习类别信息 模型结构 整体模型分为三层，即： KG-Based News Modeling Layer（核心） Attention Layer Scoring Layer KG-Based News Modeling Layer（news embedding） 如下所示： 主要由三个encoder组成，即： Word-Level News Encoder 一共有三层组成，即 embedding层 Bi-GRU层，学习共同出现的单词间的联系，而不孤立的提取单个单词的表示 attention层，每一个单词的权重不同 最终得到word-level的news representation vector Knowledge Encoder 为了提取新闻的类别信息，引入了concept的概念，即知识图谱中\"is A\"的relationship，举例如下：对于新闻标题“Donald Trump vs. Madonna, Everything We Know”，通过知识图谱中的\"is A\"关系，我们可以得到：Donald Trump is a politician and a CEO while Madonna is a singer and a feminist. 通过这些关系，有助于我们判定该新闻为政治类新闻。在这一层中我们先提取出每一个实体的多个\"is A\"关系对应的实体，称之为\"concept\"，然后对这些concept进行embedding，最后利用attention机制学习相应的权重。在这个例子中就会为“politician”和“feminist”设置较高的权重 最终得到concept vector KG-Level News Encoder 分为三个步骤： 根据标题提取出实体，将相应的实体映射到知识图谱中 将实体向外延伸2-hop，并从原始的知识图谱中根据这些实体得到相应的子图 为tittle中的各个实体添加topical relation，得到一张新的子图，在图中使用GNN算法得到news的表示 最终得到KG-level的news representation vector 最终得到的news embedding为，word-level和KG-level的news representation vector相结合的结果 Attention Layer（user embedding） 通过attention学习目标news与user的历史点击的news的相关程度，最终得到user的embedding Scoring Layer 将user embedding和news embedding进行点击，得到点击率 实验结果","link":"/2021/03/01/newsRecsys-TEKGR/"},{"title":"Hexo博客迁移记录","text":"起因 原本blog是暑假搭建在我的笔记本上的，但是上学后更多的用的是实验室的台式机，笔记本太重了经常放在寝室，导致我上学期的后半段几乎没有跟新博客（又在给自己找理由了hhh）。趁着刚开学有空，就想将原本笔记本上的博客内容迁移到实验室的主机上来，实现笔记本和实验室主机都能写博客，同时还能相互间同步。说道免费同步，当然是利用github~ 于是诞生了这篇围绕如何利用github实现多终端都可以撰写博客的博文 方法 在笔记本上的操作（原电脑） 给 username.github.io博客仓库创建hexo分支，并设为默认分支 随便一个目录下，命令行执行 git clone git@github.com:username/username.github.io.git 把仓库 clone 到本地（默认是hexo分支） 显示所有隐藏文件和文件夹，进入刚才 clone 到本地的仓库，删掉除了 .git 文件夹以外的所有内容 命令行 cd 到 clone 的仓库，执行如下命令： 123git add -Agit commit -m &quot;--&quot;git push origin hexo 把刚才删除操作引起的本地仓库变化更新到远程，此时刷新下 github 端博客hexo分支，应该已经被清空了 将上述 .git 文件夹复制到本机本地博客根目录下（即含有 themes、source 等文件夹的那个目录），现在可以把上述 clone 的本地仓库删掉了，因为它已经没有用了，本机博客目录已经变成可以和 hexo 分支相连的仓库了 将博客目录下 themes 文件夹下每个主题文件夹里面的 .git .gitignore 删掉 cd 到博客目录，执行如下命令 123git add -Agit commit -m &quot;--&quot;git push origin hexo 将博客目录下所有文件更新到 hexo 分支。如果上一步没有删掉 .git .gitignore，主题文件夹下内容将传不上去。 在实验室主机上的操作（新电脑） 在新电脑上操作，先把新电脑上环境安装好，node.js、git、hexo，ssh key 也创建和添加好 选好博客安装的目录， 执行 1git clone git@github.com:username/username.github.io.git cd 到博客目录，npm install、hexo g &amp;&amp; hexo s，安装依赖，生成和启动博客服务。正常的话，浏览器打开 localhost:4000 可以看到博客了。至此新电脑操作完毕 同步方法 上传 123git add -Agit commit –m &quot;xx&quot;git push 在已经执行过git clone的电脑上进行下载 1git pull 引用 [1] 使用hexo，如果换了电脑怎么更新博客？","link":"/2021/03/01/migrate-blog/"},{"title":"(DeepCoNN)Joint Deep Modeling of Users and Items Using Reviews for Recommendation 论文阅读","text":"摘要 ⽤户所写的评论中存在很多可以发掘的有⽤的信息，这些信息可以⽤来减轻数据的稀疏性问题并且提升 推荐系统的质量，然⽽⽬前很多推荐系统都忽略了这个评论中的信息。因此我们提出了DeepCoNN模 型，包含两个平⾏的神经⽹络，这两个⽹络在最后⼀层产⽣交互。其中⼀个⽹络通过⽤户写过的所有评 论来学习⽤户的向量表示；另⼀个⽹络通过学习所有针对同⼀商品的评论来学习商品的向量表示。最后 的共享层⽤来学习商品向量和⽤户向量的交互，其交互⽅式采⽤的分解机技术。实验结果证明 DeepCoNN在过个baseline中都表现的很好。 创新 我们提出的DeepCoNN模型是第⼀个通过评论信息来共同建模⽤户⾏为和商品属性，额外的位于两 个神经⽹络之上的共享层链接了两个神经⽹络，通过结合两个平⾏神经⽹络的预测结果来预测⽤户 的评分。 此篇⽂章的模型使⽤预训练的word embedding来表示⽤户的评论。实验证明这种表示⽅式下的语 义信息和情感态度相⽐于传统的bag-of-word模型更能提升预测准确性。 利⽤评论不仅可以减轻稀疏性问题，还能显著提⾼系统的整体表现。 模型 整体的模型结果如下图所示： 其中左侧为⽤于学习⽤户向量表示的神经⽹络；右侧为⽤于学习item向量表示的神经⽹络。左右侧的模 型结构完全⼀样，只是输⼊的数据不⼀样，因此此处着重讲解左侧的模型。左侧模型⼀共由四个layer组 成，即：Look-up layer，Convolution layer，Max-pooling layer，Fully-connected layer。接下来将 分别介绍这⼏个layer的作⽤： Look-up layer：⽤于将每个⽂字转化为对应的word embedding，进⽽将⼀段⽂本转化为⼀个矩 阵。此处不同于之前的bag-of-word，转化为向量矩阵之后依旧保持着原先word之间的顺序。即⾸ 先将⼀个⽤户过往的所有评论组成⼀个document，\\(d_{1: n}^{u}\\)，然后将⽂档转化为矩阵： \\[ V_{1: n}^{u}=\\phi\\left(d_{1}^{u}\\right) \\oplus \\phi\\left(d_{2}^{u}\\right) \\oplus \\phi\\left(d_{3}^{u}\\right) \\oplus \\ldots \\oplus \\phi\\left(d_{n}^{u}\\right) \\] 其中\\(\\phi\\left(d_{k}^{u}\\right)\\)为look-up函数 Convolution layer：通过多个卷积核来提取不同的局部语义信息，即 \\[ z_{j}=f\\left(V_{1: n}^{u} * K_{j}+b_{j}\\right) \\] 其中\\(K_{j} \\in \\Re^{c \\times t}\\)为卷积核，*为卷积操作， 为激活函数Rectified Linear Units (ReLUs)，即： \\[ f(x)=\\max \\{0, x\\} \\] 每个卷积核会依序遍历⼀遍整个document，得到卷积结果： \\[ z_{1}, z_{2}, \\ldots, z_{(n-t+1)} \\] Max-pooling layer：对于每⼀个卷积核的卷积结果，选择⼀个最⼤的作为当前卷积核在当前 document下的特征。因此pooling函数的表达式如下： \\[ o_{j}=\\max \\left\\{z_{1}, z_{2}, \\ldots, z_{(n-t+1)}\\right\\} \\] 此处的n是整个⽂章的单词⻓度，t是窗⼝的⼤⼩。对于所有卷积核做完如上操作得到的结果为： \\[ O=\\left\\{o_{1}, o_{2}, o_{3}, \\ldots, o_{n_{1}}\\right\\} \\] 其中\\(n_{1}\\)表示卷积核的个数 Fully-connected layer：将Max-pooling layer的结果做⼀次全连接的映射，得到的结果即为： \\[ \\mathbf{x}_{\\mathbf{u}}=f(W \\times O+g) \\] 通过如上的模型可以得到：user和item的向量表示。然后将user和item的向量表示输⼊最后⼀层交互层 可以得到⽤于对于某个商品的评分，交互层使⽤了分解机（Factorization Machine) 机制，公式如下： \\[ J=\\hat{w}_{0}+\\sum_{i=1}^{|\\hat{z}|} \\hat{w}_{i} \\hat{z}_{i}+\\sum_{i=1}^{|\\hat{z}|} \\sum_{j=i+1}^{|\\hat{z}|}\\left\\langle\\hat{\\mathbf{v}}_{i}, \\hat{\\mathbf{v}}_{j}\\right\\rangle \\hat{z}_{i} \\hat{z}_{j} \\] 其重要作⽤是对⼀阶特征和⼆阶特征进⾏融合，⽤来预测评分。其中\\(\\left\\langle\\hat{\\mathbf{v}}_{i}, \\hat{\\mathbf{v}}_{j}\\right\\rangle\\)⽤来确定⼆阶特征的权重。 训练 ⽬标函数为： \\[ J=\\hat{w}_{0}+\\sum_{i=1}^{|\\hat{z}|} \\hat{w}_{i} \\hat{z}_{i}+\\sum_{i=1}^{|\\hat{z}|} \\sum_{j=i+1}^{|\\hat{z}|}\\left\\langle\\hat{\\mathbf{v}}_{i}, \\hat{\\mathbf{v}}_{j}\\right\\rangle \\hat{z}_{i} \\hat{z}_{j} \\] 其⽬标就是对于\\(-\\hat{z}_{i}\\)最⼩化\\(J\\)，从⽽通过链式法则实现反向传播，即： \\[ \\frac{\\partial J}{\\partial \\hat{z}_{i}}=\\hat{w}_{i}+\\sum_{j=i+1}^{|\\hat{z}|}\\left\\langle\\hat{\\mathbf{v}}_{i}, \\hat{\\mathbf{v}}_{j}\\right\\rangle \\hat{z}_{j} \\] 在这⾥我们使⽤RMSprop，它是梯度下降的⾃适应版本，会根据梯度的绝对值⾃适应的控制步⻓。具体 的做法如下⾯公式所示： \\[ r_{t} \\leftarrow 0.9\\left(\\frac{\\partial J}{\\partial \\theta}\\right)^{2}+0.1 r_{t-1} \\] \\[ \\theta \\leftarrow \\theta-\\left(\\frac{\\lambda}{\\sqrt{r_{t}}+\\epsilon}\\right) \\frac{\\partial J}{\\partial \\theta} \\] 实验 数据集 Yelp：⼀个对于饭店评论的数据集，包含1 W条评论和打分 Amazon：包含来意与亚⻢逊⽹站的对于商品的评论和打分，包含21类商品和143.7 million条评 论，是⽬前最⼤的针对商品评论的数据库 Beer：这是⼀个对于啤酒的评论数据库 实验设置 进⾏卷积时，卷积核的个数\\(n_{1}\\)，会从取10-400进⾏实验，最终设置为100 最终进⾏分解机前潜在因⼦的数⽬，即\\(\\left|\\mathbf{x}_{\\mathbf{u}}\\right|\\)和\\(\\left|\\mathbf{y}_{\\mathbf{i}}\\right|\\)的⼤⼩会取5-100进⾏实验，最终设置为50 CNN的窗⼝⼤⼩设置为3 batch size设置为100 学习率设置为0.002 实验的评价标准：采⽤MSE作为评价标准，即： \\[ M S E=\\frac{1}{N} \\sum_{n=1}^{N}\\left(r_{n}-\\hat{r}_{n}\\right)^{2} \\] 实验结果 通过⽐较可以发现，我们的模型在各个数据库⽅⾯均优于之前的模型，同样我们可以得出如下结论： 所有的模型在Beer数据库的表现都⽐其他两个数据库好，是因为其他两个数据库⽐较稀疏 考虑了评论的模型⽐没有考虑评论的模型（PMF，MF）表现要好 同时考虑了评论和rating的模型（CTR，HFT）表现要⽐只考虑评论的模型（LDA）取得的效果好 模型分析 为了验证我们的提出的模型的创新点，即 相互协作的两个神经⽹络能否通过合作有效的从评论中学习特征? 对此我们提出了如下的实验，DeepCoNN-User and DeepCoNN-Item，即分别替换了item和user 直接为⼀组vector进⾏训练，实验结果证明DeepCoNN的效果最好 word embedding是否有效的挖掘了语义信息和情感信息？ 对此我们提出了如下的实验，DeepCoNN-TFIDF and DeepCoNN-Random，分别以TFIDF和 random作为模型的输⼊来训练模型，最终实验表明word embedding的效果最好 共享层对于提升预测的准确性有多⼤的帮助？ 对此我们做出如下的实验，设计了DeepCoNN-DP模型，即⽤⼀个更简单的⽬标函数来替换分解机 模型，实验证明分解机模型的效果更好","link":"/2021/03/05/reviewRecsys-DeepCoNN/"},{"title":"(FAN)Fairness-aware News Recommendation with Decomposed Adversarial Learning 论文阅读","text":"Fairness介绍 新闻推荐中Fairness的问题 用论文中的图片来解释：这里我们以性别为例子来解释新闻推荐中的fairness问题，通过用户的过往点击历史，模型可以学到用户A,B是女性，用户C,D是男性。如果对用户D进行新闻推荐时，因为用户是男性，模型会根据大多数男性的兴趣为用户进行推荐，不会为该用户推荐左侧的新闻。而左侧新闻有关Fashion，正好是用户D的兴趣点。 创新点 本文提出了fairness-aware news recommendation approach with decomposed adversarial learning and orthogonality regularization这一方法，即该方法由解离对抗学习，正交正则化这两种子方法来实现对于fairness的去除。 解离对抗学习：以往的新闻推荐方法都是分别通过user model和news model来学习user embedding和news embedding，FAN为了解离bias，于是将user model分为两个部分，分别是bias-aware user model和bias-free user model。对这两个user model进行对抗式的训练，使得bias-aware user model能准确的预测sensitive attributes，而bias-free user model无法预测sensitive attributes。 正交正则化：是得bias-aware user model得到的user embedding和bias-free user model得到的user embedding尽量能够相互正交。进一步解离bias 模型 user Model部分news Model部分与NRMS模型采用同样的建模方式 其余主要划分为三个模块： Attribute Prediction： 输入bias-aware user model产生的bias-aware user embedding，通过如下公式对于sensitive attributes进行预测 \\[ \\hat{z}=\\operatorname{softmax}\\left(\\mathbf{W}^{b} \\mathbf{u}^{b}+\\mathbf{b}^{b}\\right) \\] 要求预测产生的loss越小越好 \\[ \\mathcal{L}_{G}=-\\frac{1}{U} \\sum_{j=1}^{U} \\sum_{i=1}^{C} z_{i}^{j} \\log \\left(\\hat{z}_{i}^{j}\\right) \\] Adversarial Learning 输入bias-free user model产生的bias-free user embedding，通过如下公式对于sensitive attributes进行预测 \\[ \\tilde{z}=\\operatorname{softmax}\\left(\\mathrm{W}^{d} \\mathrm{u}^{d}+\\mathrm{b}^{d}\\right) \\] 要求预测产生的loss越大越好，即预测越不准确越好 \\[ \\mathcal{L}_{A}=-\\frac{1}{U} \\sum_{j=1}^{U} \\sum_{i=1}^{C} z_{i}^{j} \\log \\left(\\tilde{z}_{i}^{j}\\right) \\] Orthogonality regularization 产生的loss如下： \\[ \\mathcal{L}_{D}=-\\frac{1}{U} \\sum_{i=1}^{U} \\mid \\frac{\\mathbf{u}_{i}^{b} \\cdot \\mathbf{u}_{i}^{d}}{\\left\\|\\mathbf{u}_{i}^{b}\\right\\| \\cdot\\left\\|\\mathbf{u}_{i}^{d}\\right\\|} \\] 训练时总的Loss \\[ \\mathcal{L}=\\mathcal{L}_{R}+\\lambda_{G} \\mathcal{L}_{G}+\\lambda_{D} \\mathcal{L}_{D}-\\lambda_{A} \\mathcal{L}_{A} \\] 其中\\(\\mathcal{L}_{R}\\)表示推荐系统本身的loss，其计算公式如下： \\[ \\mathcal{L}_{R}=-\\frac{1}{N_{c}} \\sum_{i=1}^{N_{c}} \\log \\left[\\frac{\\exp \\left(\\hat{y}_{i}\\right)}{\\exp \\left(\\hat{y}_{i}\\right)+\\sum_{j=1}^{T} \\exp \\left(\\hat{y}_{i, j}\\right)}\\right. \\] 模型训练 对于每一个样本，通过负采样进行训练，负样本来自于同一对话中未被点击的新闻 训练时使用的user embedding是将bias-free user embedding和bias-aware user embedding相加得到的 Fairness的测量标准 实验结果 消融实验 表明模型中每一部分都是有作用的","link":"/2021/03/06/newsRecsys-FAN/"},{"title":"(KGIN)Learning Intents behind Interactions with Knowledge Graph for Recommendation 论文阅读","text":"创新点 User Intents：将用户与商品之间的交互通过一个细粒度的角度考虑，即用户意图这个角度。一共存在n个用户意图，这些用户意图从不同的角度决定了用户对于某一件商品的兴趣。最终决定用户对于一件商品的兴趣，即user vector。通过对于不同的intent vector赋予不同权重的加和。关于intent的图示化具体如下所示： Relational Paths：加入知识图谱的过程中，考虑了知识图谱中的relation关系。即更加重视路径的源起和结尾，对于知识图谱中的结构化知识的利用更加充分。图示如下： 整体模型 主要分为两个部分： User Intent Modeling Relational Path-aware Aggregation User Intent Modeling 为了增强对于intent的建模的可解释性，对于intent的建模是对知识图谱中所有relation的embedding的注意力加权。具体公式如下： \\[ \\mathbf{e}_{p}=\\sum_{r \\in \\mathcal{R}} \\alpha(r, p) \\mathbf{e}_{r} \\] \\[ \\alpha(r, p)=\\frac{\\exp \\left(w_{r p}\\right)}{\\sum_{r^{\\prime} \\in \\mathcal{R}} \\exp \\left(w_{r^{\\prime} p}\\right)} \\] 为了防止intent vector之间重复度过高，需要对intent vector进行Independence Modeling of Intents（正交分解）。从而增强其独立性，是得每一个intent vector都有其存在的意义：使用的是Distance correlation，具体公式如下： \\[ \\mathcal{L}_{\\mathrm{IND}}=\\sum_{p, p^{\\prime} \\in \\mathcal{P}, p \\neq p^{\\prime}} d \\operatorname{Cor}\\left(\\mathbf{e}_{p}, \\mathbf{e}_{p^{\\prime}}\\right) \\] \\[ d \\operatorname{Cor}\\left(\\mathbf{e}_{p}, \\mathbf{e}_{p^{\\prime}}\\right)=\\frac{d \\operatorname{Cov}\\left(\\mathbf{e}_{p}, \\mathbf{e}_{p^{\\prime}}\\right)}{\\sqrt{d \\operatorname{Var}\\left(\\mathbf{e}_{p}\\right) \\cdot d \\operatorname{Var}\\left(\\mathbf{e}_{p^{\\prime}}\\right)}} \\] Relational Path-aware Aggregation 通过关系感知的路径构建，我们将得到user和item的向量表示，即： 通过公式，我们可以看出： Relational Path的构建是通过多层item representation来表示的 user representation通过item representation来接入relational path 第0层的user, item的表示均是随机初始化的。 得到第一层user的具体过程为： \\[ \\mathbf{e}_{u}^{(1)}=f_{\\mathrm{IG}}\\left(\\left\\{\\left(\\mathbf{e}_{u}^{(0)}, \\mathbf{e}_{p}, \\mathbf{e}_{i}^{(0)}\\right) \\mid(p, i) \\in \\mathcal{N}_{u}\\right\\}\\right) \\] \\[ \\mathbf{e}_{u}^{(1)}=\\frac{1}{\\left|\\mathcal{N}_{u}\\right|} \\sum_{(p, i) \\in \\mathcal{N}_{u}} \\beta(u, p) \\mathbf{e}_{p} \\odot \\mathbf{e}_{i}^{(0)} \\] 其中\\(\\beta(u, p)\\)的作用是，每个user对于不同的intent有不同的权重 \\[ \\beta(u, p)=\\frac{\\exp \\left(\\mathbf{e}_{p}^{\\top} \\mathbf{e}_{u}^{(0)}\\right)}{\\sum_{p^{\\prime} \\in \\mathcal{P}} \\exp \\left(\\mathbf{e}_{p^{\\prime}}^{\\top} \\mathbf{e}_{u}^{(0)}\\right)} \\] \\[ \\mathbf{e}_{i}^{(1)}=f_{\\mathrm{KG}}\\left(\\left\\{\\left(\\mathbf{e}_{i}^{(0)}, \\mathbf{e}_{r}, \\mathbf{e}_{v}^{(0)}\\right) \\mid(r, v) \\in \\mathcal{N}_{i}\\right\\}\\right) \\] 得到第一层item的具体过程为： \\[ \\mathbf{e}_{i}^{(1)}=f_{\\mathrm{KG}}\\left(\\left\\{\\left(\\mathbf{e}_{i}^{(0)}, \\mathbf{e}_{r}, \\mathbf{e}_{v}^{(0)}\\right) \\mid(r, v) \\in \\mathcal{N}_{i}\\right\\}\\right) \\] \\[ \\mathbf{e}_{i}^{(1)}=\\frac{1}{\\left|\\mathcal{N}_{i}\\right|} \\sum_{(r, v) \\in \\mathcal{N}_{i}} \\mathbf{e}_{r} \\odot \\mathbf{e}_{v}^{(0)} \\] 最终为了使路径延伸L层，需要进行L次迭代，具体公式如下： \\[ \\begin{array}{l} \\mathbf{e}_{u}^{(l)}=f_{\\mathrm{IG}}\\left(\\left\\{\\left(\\mathbf{e}_{u}^{(l-1)}, \\mathbf{e}_{p}, \\mathbf{e}_{i}^{(l-1)}\\right) \\mid(p, i) \\in \\mathcal{N}_{u}\\right\\}\\right) \\\\ \\mathbf{e}_{i}^{(l)}=f_{\\mathrm{KG}}\\left(\\left\\{\\left(\\mathbf{e}_{i}^{(l-1)}, \\mathbf{e}_{r}, \\mathbf{e}_{v}^{(l-1)}\\right) \\mid(r, v) \\in \\mathcal{N}_{i}\\right\\}\\right) \\end{array} \\] 为了展示路径的延伸，将上式中计算\\(\\mathbf{e}_{i}^{(l)}\\)的过程可以展开成如下结果： \\[ \\mathbf{e}_{i}^{(l)}=\\sum_{s \\in \\mathcal{N}_{i}^{l}} \\frac{\\mathbf{e}_{r_{1}}}{\\left|\\mathcal{N}_{s_{1}}\\right|} \\odot \\frac{\\mathbf{e}_{r_{2}}}{\\left|\\mathcal{N}_{s_{2}}\\right|} \\odot \\cdots \\odot \\frac{\\mathbf{e}_{r_{l}}}{\\left|\\mathcal{N}_{s_{l}}\\right|} \\odot \\mathbf{e}_{s_{l}}^{(0)} \\] 模型训练 模型预测 为了防止过平滑，需要将每一层的计算结构进行叠加： \\[ \\mathbf{e}_{u}^{*}=\\mathbf{e}_{u}^{(0)}+\\cdots+\\mathbf{e}_{u}^{(L)}, \\quad \\mathbf{e}_{i}^{*}=\\mathbf{e}_{i}^{(0)}+\\cdots+\\mathbf{e}_{i}^{(L)} \\] 最后的预测指标为向量间的点积： \\[ \\hat{y}_{u i}=\\mathbf{e}_{u}^{* \\top} \\mathbf{e}_{i}^{*} \\] 模型优化 总的Loss为： \\[ \\mathcal{L}_{\\mathrm{KGIN}}=\\mathcal{L}_{\\mathrm{BPR}}+\\lambda_{1} \\mathcal{L}_{\\mathrm{IND}}+\\lambda_{2}\\|\\Theta\\|_{2}^{2} \\] 其中预测结果使用BPR Loss（使得正样本和负样本之间的差距尽可能大）： \\[ \\mathcal{L}_{\\mathrm{BPR}}=\\sum_{(u, i, j) \\in O}-\\ln \\sigma\\left(\\hat{y}_{u i}-\\hat{y}_{u j}\\right) \\] IND Loss为之前提到的intent 正交化的Loss： \\[ \\mathcal{L}_{\\mathrm{IND}}=\\sum_{p, p^{\\prime} \\in \\mathcal{P}, p \\neq p^{\\prime}} d \\operatorname{Cor}\\left(\\mathbf{e}_{p}, \\mathbf{e}_{p^{\\prime}}\\right) \\] 实验结果 消融实验 参数敏感性实验","link":"/2021/03/10/itemRecsys-KGIN/"}],"tags":[{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"Hexo,Icarus","slug":"Hexo-Icarus","link":"/tags/Hexo-Icarus/"},{"name":"datawhale","slug":"datawhale","link":"/tags/datawhale/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"cuda","slug":"cuda","link":"/tags/cuda/"},{"name":"deep learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"news recommendation","slug":"news-recommendation","link":"/tags/news-recommendation/"},{"name":"entity disambiguation","slug":"entity-disambiguation","link":"/tags/entity-disambiguation/"},{"name":"GNN","slug":"GNN","link":"/tags/GNN/"},{"name":"knowledge graph","slug":"knowledge-graph","link":"/tags/knowledge-graph/"},{"name":"RL","slug":"RL","link":"/tags/RL/"},{"name":"item recommendation","slug":"item-recommendation","link":"/tags/item-recommendation/"},{"name":"review-based recommendation","slug":"review-based-recommendation","link":"/tags/review-based-recommendation/"}],"categories":[{"name":"博客搭建","slug":"博客搭建","link":"/categories/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"},{"name":"Datawhale学习手记","slug":"Datawhale学习手记","link":"/categories/Datawhale%E5%AD%A6%E4%B9%A0%E6%89%8B%E8%AE%B0/"},{"name":"pytorch-learning","slug":"pytorch-learning","link":"/categories/pytorch-learning/"},{"name":"information retrieval","slug":"information-retrieval","link":"/categories/information-retrieval/"},{"name":"环境配置","slug":"环境配置","link":"/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"随想","slug":"随想","link":"/categories/%E9%9A%8F%E6%83%B3/"},{"name":"topic research","slug":"topic-research","link":"/categories/topic-research/"},{"name":"recsys","slug":"recsys","link":"/categories/recsys/"},{"name":"paper reading","slug":"paper-reading","link":"/categories/paper-reading/"}]}