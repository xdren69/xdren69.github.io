---
title: itemRecsys-GBDT+LR
date: 2021-04-14 19:00:12
tags:
---



>   参考资料：
>
>   [1] [深入浅出理解决策树算法（一）-核心思想](https://zhuanlan.zhihu.com/p/26703300)
>
>   [2] [深入浅出理解决策树算法（二）-ID3算法与C4.5算法](https://zhuanlan.zhihu.com/p/26760551)



## 决策树



主要分为：

-   分类树
-   回归树

### 分类树



#### 特征选择



#### 树的生成



#### 剪枝





### 回归树

#### 参考资料

[1] []()









## 集成学习

指构建多个弱分类器对数据集进行预测，然后用某种策略将多个分类器预测的结果集成起来，作为最终预测结果。

集成学习的那两大流派：Boosting和Bagging：

-   Boosting：各分类器之间有依赖关系，必须串行，比如Adaboost、GBDT(Gradient Boosting Decision Tree)、Xgboost

    >   同一个问题，划分成串行的子问题， 先由一个人解决一部分，解决不了的，后面的人再来

-   Bagging：各分类器之间没有依赖关系，可各自并行，比如随机森林（Random Forest）

    >   同一个问题，把问题划分成不相干的子问题，然后分派给不同的人各干各的



### 加法模型与前向分布算法

由于Adaboost和GBDT都属于加法模型，针对于加法模型，从前向后，一步只学习一个基函数及其系数



## AdaBoost

### 参考资料

[1] [（十三）通俗易懂理解——Adaboost算法原理](https://zhuanlan.zhihu.com/p/41536315)



### 主要特点

>   经过一个分类器，就根据分类的结果：1. 利用前一轮计算的误差率改变每个样本的概率分布（权重）2. 计算出这个弱分类器在最终加法模型中的系数
>
>   最终，将多个弱分类器根据分类结果的好坏进行加权，得到一个新的强分类器

$$
f(x)=\sum_{i=1}^{n} \alpha_{i} G_{i}(x)
$$

其中$ \alpha_{i} $是根据当前的分类错误率（带权重）得到，即：
$$
\alpha_{i}=\frac{1}{2} \log \frac{1-e_{i}}{e_{i}}
$$



### Adaboost计算步骤

（1）首先，是初始化训练数据的权值分布D1。假设有N个训练样本数据，则每一个训练样本最开始时，都被赋予相同的权值：w1=1/N。
（2）然后，训练弱分类器hi。具体训练过程中是：如果某个训练样本点，被弱分类器hi准确地分类，那么在构造下一个训练集中，它对应的权值要减小；相反，如果某个训练样本点被错误分类，那么它的权值就应该增大。权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。
（3）最后，将各个训练得到的弱分类器组合成一个强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。



#### Adaboost 特点

优点：

1.  随着迭代次数的增多，错误率会下降
2.  不会出现过拟合问题

缺点：

1.  错误分类的样本的权重会随着迭代次数呈现指数式增长
2.  弱分类器的训练时间比较长



