---
title: ML-GBDT
date: 2021-04-21 11:37:58
tags:
---

## GBDT（梯度提升决策树）

>   不论我们处理的是分类任务还是回归任务，都是将CART树（回归树）作为基学习器，相比于串行的回归树，这种拟合残差的并行回归树能够利用更少的特征来防止过拟合



### 参考资料

[1] [GBDT算法](https://zhuanlan.zhihu.com/p/39354380)



### 学习方式：梯度提升

对于回归树中不使用平方误差作为损失函数的回归任务，我们使用损失函数的负梯度在当前模型的值作为残差的近似值，用于下一次的拟合任务——每一次建立模型是在之前建立模型损失函数的梯度下降方向

>   GBDT中的学习率

![](https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/image-20210420085120980.png)



### GBDT二分类



#### 参考资料

[1] [深入理解GBDT二分类算法](https://blog.csdn.net/program_developer/article/details/103060416?spm=1001.2014.3001.5501)



#### 每一次建树前需要拟合的目标

>   之所以每一次需要学习的目标仍然是真实值与目标值之间的残差，是因为：

1.  每一次优化的loss函数是：
    $$
    L(\theta)=\log (l(\theta))=-\sum_{i=1}^{N}\left[y_{i} \log h_{\theta}\left(x_{i}\right)+\left(1-y_{i}\right) \log \left(1-h_{\theta}\left(x_{i}\right)\right)\right]
    $$
    其中将$h_{\theta}\left(x_{i}\right)$替换为：
    $$
    h_{\theta}= sigmoid(F_{M}(x))= \frac{1}{1+e^{-F_M(x)}} = \frac{1}{1+e^{-\sum_{m=1}^{M} f_{m}(x)}}
    $$

2.  每一次需要拟合的新的负梯度为：
    $$
    -\frac{\partial L}{\partial F_{M}}=y_{i}-\frac{1}{1+e^{-F_{M}(x)}}=y_{i}-\hat{y}_{i}
    $$

3.  综上，我们可以得出每一次需要优化的负梯度，即为残差



注：

1.  每一次划分区域时，代表那一区域的值仍然是：那个区域所有样本的**平均值**
2.  区域划分的选择：依据是使用各自不同的Loss损失函数，对所有样本计算损失函数，选取损失函数最小的划分





### GBDT多分类

>   一次学习多棵CART树，最终得到多个GBDT，然后使用softmax



### GBDT回归

1. 目标是每次去拟合损失函数对于当前值的负梯度，对于MSE和Logistic函数，负梯度都是残差





## 利用GBDT构建新特征

根本方法还是GBDT用于二分类的方式，将多属性的训练样本\[x1, x2, x3...., xn, y\]输入GBDT，其中y的取值为0,1；gbdt会构建多棵树去拟合当前值，我们需要提取出当前样本在每棵树的叶子节点中的位置，然后利用0，1编码作为新的组合出的特征，输入随后的MLP或者LR模型。



>   其中XGBoost和lightGBM是GBDT的不同的实现方式







## XGBoost





## lightGBM

