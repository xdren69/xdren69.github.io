---
title: (KNN)k-近邻算法
date: 2021-04-19 11:40:31
toc: true
categories:
  - machine learning
excerpt: 本文记录了机器学习中第一个算法——k-近邻算法，属于有监督学习中的分类算法
---



## 原理

根据KNN的全称——k-近邻算法，我们可以get到KNN算法的原理，即：**当预测一个新的值x的时候，根据它距离最近的K个点是什么类别来判断x属于哪个类别**



具体而言：

>   本书讲解的第一个机器学习算法是k-近邻算法（KNN),它的工作原理是：存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。

一般距离的计算公式为：欧氏距离公式，即：
$$
d(x, y):=\sqrt{\left(x_{1}-y_{1}\right)^{2}+\left(x_{2}-y_{2}\right)^{2}+\cdots+\left(x_{n}-y_{n}\right)^{2}}=\sqrt{\sum_{i=1}^{n}\left(x_{i}-y_{i}\right)^{2}}
$$
算法图示如下：

![](https://pic1.zhimg.com/80/v2-eea2fdf7d0008e024e62f35690e90a94_720w.jpg)

k=3时，有两个蓝色，一个红色，待预测目标判定为蓝色；

![](https://pic2.zhimg.com/80/v2-136a496fcd752d1b72ae7d958a256b41_720w.jpg)

k=5时，有两个蓝色，三个红色，待预测目标判定为红色



## 特点

KNN是一种**非参的**，**惰性**的算法模型。

**非参**的意思并不是说这个算法不需要参数，而是意味着这个模型不会对数据做出任何的假设，与之相对的是线性回归（我们总会假设线性回归是一条直线）。也就是说KNN建立的模型结构是根据数据来决定的，这也比较符合现实的情况，毕竟在现实中的情况往往与理论上的假设是不相符的。

**惰性**又是什么意思呢？想想看，同样是分类算法，逻辑回归需要先对数据进行大量训练（tranning），最后才会得到一个算法模型。而KNN算法却不需要，它没有明确的训练数据的过程，或者说这个过程很快。



## 优点

1.  简单易用，相比其他算法，KNN算是比较简洁明了的算法
2.  不需要花费训练时间
3.  对异常值不敏感



## 缺点

1.  对内存要求较高，因为该算法存储了所有训练数据
2.  预测阶段可能很慢，要计算目标数据和全部训练数据间的关系
3.  对不相关的功能和数据规模敏感，即不相关的数据不能太多



## Reference

[1] [深入浅出KNN算法（一） 介绍篇](https://zhuanlan.zhihu.com/p/61341071)