<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Xdren&#039;s blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Xdren&#039;s blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Xdren&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Xdren&#039;s blog"><meta property="og:url" content="https://xdren69.github.io/"><meta property="og:site_name" content="Xdren&#039;s blog"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://xdren69.github.io/img/og_image.png"><meta property="article:author" content="Xdren"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xdren69.github.io"},"headline":"Xdren's blog","image":["https://xdren69.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Xdren"},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?14e57ad6d6de6c90603b6bea2182917a";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-170595403-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-170595403-1');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo-1.svg" alt="Xdren&#039;s blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-04-30T14:32:17.000Z" title="2021/4/30 下午10:32:17">2021-04-30</time>发表</span><span class="level-item"><time dateTime="2021-04-30T14:32:17.075Z" title="2021/4/30 下午10:32:17">2021-04-30</time>更新</span><span class="level-item">11 分钟读完 (大约1667个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/04/30/itemRecsys-DICE/">(DICE) Disentangling User Interest and Conformity for Recommendation with Causal Embedding 论文阅读</a></h1><div class="content"><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202103/24/105111-436109.png" /></p>
<blockquote>
<p>Conformity这里可以理解为屈从性，即用户对于流行商品的追求性。</p>
</blockquote>
<h2 id="创新点">创新点</h2>
<p>这里的主要思路是：通过解离特征表示的方法。增强模型在不同I.I.D.分布的数据集上的表现。</p>
<p>当前面临的主要的三个挑战分别为：</p>
<ol type="1">
<li>conformity同时依赖于用户和商品，相同用户对不同商品的conformity是不同的，不同用户对同一商品的conformity是不同的</li>
<li>在无监督的环境下学习解离表示本身是困难的</li>
<li>一个交互可能来自于（interest，conformity）中的一个或两个原因，因此我们需要巧妙设计来结合多个原因</li>
</ol>
<h2 id="模型">模型</h2>
<p>主要由以下三个部分组成：</p>
<h3 id="causal-embedding">Causal Embedding</h3>
<p>我们使用embedding而不是标量值来建模用户的interest和conformity（causes），用来解决conformity会随着item和user变动的问题。我们认为用户的点击来自于两个方面的原因，即：</p>
<ol type="1">
<li>the user’s interest in the item’s characteristics</li>
<li>the user’s conformity towards the item’s popularity</li>
</ol>
<p>我们构建的<strong>causal graph</strong>为：</p>
<p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202103/24/115834-404097.png" style="zoom:67%;" /></p>
<p>相应的causal model为： <span class="math display">\[
S_{\mathrm{ui}}=S_{u i}^{\text {interest }}+S_{u i}^{\text {conformity }} \tag{1}
\]</span> 相应的SCM（structural causal model）为： <span class="math display">\[
\begin{array}{l}
X_{u i}^{i n t}:=f_{1}\left(u, i, N^{i n t}\right) \\
X_{u i}^{c o n}:=f_{2}\left(u, i, N^{c o n}\right) \\
Y_{u i}^{\text {click }}:=f_{3}\left(X_{u i}^{i n t}, X_{u i}^{c o n}, N^{\text {click }}\right)
\end{array}  \tag{2}
\]</span> 在这里我们引入Causal Embedding，为每个用户设计相应的interest embedding和conformity embedding；与之相对应的，为每个商品也设计对应的characteristic embedding（在这里叫做interest）和popularity embedding（在这里叫做conformity）</p>
<p>使用Causal Embedding将SCM细化得到： <span class="math display">\[
\begin{array}{l}
s_{u i}^{i n t}=\left\langle u^{(\text {int })}, i^{(\text {int })}\right\rangle , \\ 
s_{u i}^{c o n}=\left\langle u^{(\text {con })}, i^{(\text {con })}\right\rangle , \\
s_{u i}^{\text {click }}=s_{u i}^{i n t}+s_{u i}^{c o n} , 
\end{array}  \tag{3}
\]</span></p>
<h3 id="disentangled-representation-learning">Disentangled Representation Learning</h3>
<p>我们将训练数据根据不同的causes（user’s interest，user’s conformity）分为不同的cause-specific parts，为每种原因训练不同的embedding。</p>
<p>但是根据数据集，我们只能知道用户是否点击了某个商品，无法知道是因为具体哪个cause而进行点击的。为了获得cause-specific的训练数据，我们引入了colliding effect这个概念！</p>
<blockquote>
<p><strong>colliding effect :</strong></p>
<p>In fact, the two causes of a collider are independent variables. However, if we condition on the collider, the two causes become correlated with each other, and we call it colliding effect.</p>
</blockquote>
<p>根据这一概念我们将采样两类数据（其中<span class="math inline">\(M^{I}\)</span>， <span class="math inline">\(M^{C}\)</span>均是<span class="math inline">\(\mathbb{R}^{M \times N}\)</span>类型的矩阵，分别表示<strong>interest matching score</strong>和<strong>conformity matching score</strong>）：</p>
<ul>
<li><p>Case 1: 负样本的流行性低于正样本 <span class="math display">\[
\begin{array}{l}
M_{u a}^{C}&gt;M_{u b}^{C} \\
M_{u a}^{I}+M_{u a}^{C}&gt;M_{u b}^{I}+M_{u b}^{C}
\end{array}  \tag{4}
\]</span></p></li>
<li><p>Case 2: 负样本的流行性高于正样本 <span class="math display">\[
\begin{array}{l}
M_{u c}^{I}&gt;M_{u d}^{I} \\
M_{u c}^{C}&lt;M_{u d}^{C} \\
M_{u c}^{I}+M_{u c}^{C}&gt;M_{u d}^{I}+M_{u d}^{C}
\end{array}  \tag{5}
\]</span></p></li>
</ul>
<div id="1.2">


<p>首先我们组装training instance为一个三元组 （u，i，j） 即一个正样本对应特定个数个负样本（）。将所有的training instance的集合称为<span class="math inline">\(O\)</span>，然后根据以上两个Case，将<span class="math inline">\(O\)</span>分为<span class="math inline">\(O_{1}\)</span> 和 <span class="math inline">\(O_{2}\)</span></p>
<p>接下来，将采取结合矩阵分解和<strong>BPR Loss</strong>的方法来建模以上的<strong>不等式关系</strong>，借此来学习矩阵<span class="math inline">\(M^{I}\)</span>， <span class="math inline">\(M^{C}\)</span>。我们将整个学习过程化为四个步骤：</p>
<ol type="1">
<li><p><strong>Conformity Modeling</strong>：</p>
<blockquote>
<p>用于通过BPR Loss建模公式(4)，(5)中的关于conformity的不等式关系</p>
</blockquote>
<p><span class="math display">\[
\begin{aligned}
L_{\text {conformity }}^{O_{1}} &amp;=\sum_{(u, i, j) \in O_{1}} \operatorname{BPR}\left(\left\langle\boldsymbol{u}^{(\text {con })}, \boldsymbol{i}^{(\text {con })}\right\rangle,\left\langle\boldsymbol{u}^{(\text {con })}, \boldsymbol{j}^{(\text {con })}\right\rangle\right), \\
L_{\text {conformity }}^{O_{2}} &amp;=\sum_{(u, i, j) \in O_{2}}-\mathrm{BPR}\left(\left\langle\boldsymbol{u}^{(\mathrm{con})}, \boldsymbol{i}^{(\mathrm{con})}\right\rangle,\left\langle\boldsymbol{u}^{(\mathrm{con})}, \boldsymbol{j}^{(\mathrm{con})}\right\rangle\right), \\
L_{\text {conformity }}^{O_{1}+O_{2}} &amp;=L_{\text {conformity }}^{O_{1}}+{L}_{\text {conformity }}^{O_{2}} .
\end{aligned}  \tag{6}
\]</span></p></li>
<li><p><strong>Interest Modeling</strong>：</p>
<blockquote>
<p>用于通过BPR Loss建模公式(5)中的关于interest的不等式关系</p>
</blockquote>
<p><span class="math display">\[
L_{\text {interest }}^{O_{2}}=\sum_{(u, i, j) \in O_{2}} \operatorname{BPR}\left(\left\langle u^{(\text {int })}, i^{(\text {int })}\right\rangle,\left\langle u^{(\text {int })}, j^{(\text {int })}\right\rangle\right)  \tag{7}
\]</span></p></li>
<li><p><strong>Estimating Clicks</strong>：</p>
<blockquote>
<p>用于通过BPR Loss建模公式(4)，(5)中的关于最终点击率的不等式关系</p>
</blockquote>
<p><span class="math display">\[
L_{\text {click }}^{O_{1}+O_{2}}=\sum_{(u, i, j) \in O} \operatorname{BPR}\left(\left\langle\boldsymbol{u}^{t}, \boldsymbol{i}^{t}\right\rangle,\left\langle\boldsymbol{u}^{t}, \boldsymbol{j}^{t}\right\rangle\right)  \tag{8}
\]</span> 其中<span class="math inline">\(\boldsymbol{u}^{t}, \boldsymbol{i}^{t}\)</span> 和 <span class="math inline">\(\boldsymbol{j}^{t}\)</span>分别表示为： <span class="math display">\[
\boldsymbol{u}^{t}=\boldsymbol{u}^{(\mathrm{int})}\left\|\boldsymbol{u}^{(\mathrm{con})}, \boldsymbol{i}^{t}=\boldsymbol{i}^{(\mathrm{int})}\right\| \boldsymbol{i}^{(\mathrm{con})}, \boldsymbol{j}^{t}=\boldsymbol{j}^{(\mathrm{int})} \| \boldsymbol{j}^{(\mathrm{con})}  \tag{9}
\]</span></p></li>
<li><p><strong>Discrepancy Task</strong>：</p>
<blockquote>
<p>目标是使得同一user/item之间的interest embedding和conformity embedding之间的区别加大，减小两者间共同包含的信息</p>
</blockquote>
<p>这里我们使用：distance correlation (dCor)来增强相应embedding之间的距离</p></li>
</ol>
<h3 id="multi-task-curriculum-learning">Multi-task Curriculum Learning</h3>
<blockquote>
<p>多任务学习：把多个相关（related）的任务放在一起学习，同时学习多个任务。</p>
<p><strong>课程学习</strong>策略：进行从易到难的学习。</p>
</blockquote>
<p>对之前分解的四个子任务进行多任务学习，总的Loss为： <span class="math display">\[
L=L_{\text {click }}{\mathrm{O}_{1}+\mathrm{O}_{2}}+\alpha\left(L_{\text {interest }}{\mathrm{O}_{2}}+{L}_{\text {conformity }}{\mathrm{O}_{1}+\mathrm{O}_{2}}\right)+\beta L_{\text {discrepancy }}  \tag{10}
\]</span> 对于<a href="#1.2">之前</a>的training instance的组装，我们采用特殊的负采样方式，即<code>Popularity based Negative Sampling with Margin (PNSM)</code>，即：</p>
<blockquote>
<p>If the popularity of the positive sample is p, then we will sample negative instances from items with popularity larger than p + m<sub>up</sub>, or lower than p − m<sub>down</sub> , where m<sub>up</sub> and m<sub>down</sub> are positive margin values.</p>
</blockquote>
<p>这里我们通过动态调整超参数来实现<strong>课程学习</strong>策略，为了让模型的学习变得由易到难，整个过程展示如下：</p>
<ul>
<li>先让模型进行简单学习，即将m<sub>up</sub> and m<sub>down</sub> 设置的较大，使得正样本和负样本之间有很大的区分，使模型去学习具有高可信度的样例，同时将<span class="math inline">\(\alpha\)</span>的值设置较大</li>
<li>随后逐渐增加难度，即按照0.9的比率对m<sub>up</sub>，m<sub>down</sub>和<span class="math inline">\(\alpha\)</span>进行衰退</li>
</ul>
<h2 id="数据集构造">数据集构造</h2>
<ol type="1">
<li>为了构造和训练集I.I.D.不相同的测试集，我们按照流行性的倒数对测试集中的交互记录进行抽样</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-04-30T07:58:19.000Z" title="2021/4/30 下午3:58:19">2021-04-30</time>发表</span><span class="level-item"><time dateTime="2021-04-30T14:31:14.808Z" title="2021/4/30 下午10:31:14">2021-04-30</time>更新</span><span class="level-item">15 分钟读完 (大约2188个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/04/30/recall-ESAM/">(ESAM)Discriminative Domain Adaptation with Non-Displayed Items to Improve Long-Tail Performance 论文笔记</a></h1><div class="content"><blockquote>
<p>本文主要想要解决的问题是，在 <strong>召回层</strong> 中，由于 <strong>sample selection bias</strong> 导致的曝光样本与未曝光样本之间的 <strong>domain shift</strong> 问题</p>
</blockquote>
<h2 id="sample-selection-bias">Sample selection bias</h2>
<p>定义为：用于分析的数据使用了不能保证随机化得方法获得，因此从样本中分析的结果不能代表总体特征；</p>
<p>比如：</p>
<ol type="1">
<li>在大学中开展社会调查，实际上是condition on受访者上过大学</li>
<li>使用某个在线网站上的数据做分析，实际上是condition on用户会访问该网站</li>
<li>分析从战场返回的战斗机上的弹孔位置，实际上是condition on飞机没有被击落</li>
</ol>
<p>用论文中的一张图来进行说明：</p>
<p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/20210430164602.png" /></p>
<p>可以看出曝光的商品中82.7%是hot的，也就是说82.7%的商品是经常出现在各个用户的曝光商品中的；而未曝光的商品中有85.2%的商品是长尾的，即几乎始终处于各个用户的未曝光列表中的。这样会导致我们的召回模型能够很好的学到已曝光商品的向量表征，但是无法学到未曝光商品的向量表征。目前采取的做法有：</p>
<ol type="1">
<li>（正向）纠正曝光样本与未曝光样本之间的 <strong>domain shift</strong> 问题</li>
<li>（反向）在训练模型时，增强负采样对于负样本的选择，增强模型的学习难度，保证了模型对于负样本的向量表征，一般使用的是 <strong>hard negative</strong></li>
</ol>
<h2 id="召回策略">召回策略</h2>
<p>目前的召回策略均是基于embedding的，整体流程如下： <span class="math display">\[
\begin{array}{l}
\boldsymbol{v}_{q}=f_{q}(q),\quad \boldsymbol{v}_{d}=f_{d}(d) 
\end{array}
\]</span></p>
<p><span class="math display">\[
S c_{q, d}=f_{s}\left(v_{q}, v_{d}\right)
\]</span></p>
<p>根据 <strong>domain shift</strong>，我们将问题归结为 <span class="math inline">\(f_{d}\)</span> 在处理未曝光样本时的问题</p>
<h2 id="domain-shift">Domain shift</h2>
<p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/20210430223112.png" /></p>
<p>即曝光商品的向量表征空间与未曝光商品的向量表征空间不统一</p>
<h2 id="本文的解决方法">本文的解决方法</h2>
<blockquote>
<p>我们称曝光的item组成的域为source domain，未曝光的item组成的域为target domain</p>
</blockquote>
<p>整体的解决方法一共分为三步：</p>
<ol type="1">
<li><p><strong>Domain Adaptation with Attribute Correlation Alignment</strong>：通过一种 <strong><em>间接</em></strong> 的特征对齐，将曝光商品与未曝光商品的向量表征空间统一起来，这时候source domain与target domain会发生重叠，即：</p>
<p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/20210430211715.png" /></p>
<blockquote>
<p>此时我们会发现，由于target domain中的item没有标签，此时虽然domain发生了重叠，但是很多红色的target item都分类错误了；接下来我们的任务就主要集中在对target item进行正确的分类；一共分类两步！</p>
</blockquote></li>
<li><p><strong>Center-Wise Clustering for Source Clustering</strong>：通过第一步，我们可以使得target domain的分布屈从于source domain的分布，这一步我们使得source domain中的同一标签的item之间更加内聚，距离分类边界更远，即“<strong><em>高内聚，低耦合</em></strong>”，这样target也会屈从于这样的趋势。此时分布图会变成如下：</p>
<p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/20210430214323.png" /></p></li>
<li><p><strong>Self-Training for Target Clustering</strong>：我们通过上图可以看出，虽然相互间的分界线很明显，但是target item还是会存在分类错误的情况！原因依旧是因为大部分的target domain的商品没有标签！这一步我们会为每一个item打上 <strong><em>伪标签</em></strong>，通过伪标签对模型进行训练。经过最后一步，总体分布会变成下图：</p>
<p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/20210430215819.png" /></p></li>
</ol>
<h2 id="具体算法">具体算法</h2>
<blockquote>
<p>注意：我们模型可以看做是一个 <strong><em>插件</em></strong> ，是对原本召回模型中的未曝光商品训练的加强</p>
</blockquote>
<h4 id="domain-adaptation-with-attribute-correlation-alignment">Domain Adaptation with Attribute Correlation Alignment</h4>
<p>我们首先寻找特征与特征之间的相互关系，制造特征间的协方差矩阵。</p>
<blockquote>
<p>举个低维特征间相似性的例子，一个商品牌子越大，价格就越高；而一个商品的牌子和材料之间的关联程度就较弱，因此我们可以得到下图（a）：</p>
<p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/20210430221052.png" /></p>
<p>我们认为可以将这一地位特征间的相关性推导到高维特征，得到图（b）</p>
</blockquote>
<p>然后通过拟合source domain的<strong>协方差矩阵</strong>和target domain之间的<strong>协方差矩阵</strong>，实现source domain和target domain之间特征空间的 <strong><em>伪对齐</em></strong>。具体做法为：</p>
<p>假设对于每个query "q"</p>
<ul>
<li>我们收集了n个“曝光item”(来自source domain)和用户对它们的反馈。这n个item经过<span class="math inline">\(f_{d}\)</span>映射，映射成<span class="math inline">\(D^{s} \in R^{n * L}\)</span>，L是embedding的维度</li>
<li>我们再随机抽样n个“未曝光item”（来自target domain），它们经过<span class="math inline">\(f_{d}\)</span>映射，映射成<span class="math inline">\(D^{s} \in R^{n * L}\)</span></li>
<li>我们要求：的L维高阶特征的两两之间（<span class="math inline">\(h_{j^{\prime}}^{s}\)</span>和<span class="math inline">\(h_{k}^{s}\)</span>）的协方差，与的L维高阶特征的两两之间（<span class="math inline">\(h_{j^{\prime}}^{t}\)</span>和<span class="math inline">\(h_{k}^{t}\)</span>）的协方差，两者之间的差值尽可能小</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
L_{D A} &amp;=\frac{1}{L^{2}} \sum_{(j, k)}\left(\boldsymbol{h}_{j}^{s \top} \boldsymbol{h}_{k}^{s}-\boldsymbol{h}_{j}^{t \top} \boldsymbol{h}_{k}^{t}\right)^{2} \\
&amp;=\frac{1}{L^{2}}\left\|\operatorname{Cov}\left(D^{s}\right)-\operatorname{Cov}\left(D^{t}\right)\right\|_{F}^{2}
\end{aligned}
\]</span></p>
<h4 id="center-wise-clustering-for-source-clustering">Center-Wise Clustering for Source Clustering</h4>
<p>为了实现source domain内的“高耦合，低内聚”，我们加入如下Loss： <span class="math display">\[
\begin{aligned}
L_{D C}^{c} &amp;=\sum_{j=1}^{n} \max \left(0,\left\|\frac{\boldsymbol{v}_{d_{j}^{s}}}{\left\|\boldsymbol{v}_{d_{j}^{s}}\right\|}-\mathbf{c}_{q}^{y_{j}^{s}}\right\|_{2}^{2}-m_{1}\right) +\sum_{k=1}^{n_{y}} \sum_{u=k+1}^{n_{y}} \max \left(0, m_{2}-\left\|\mathbf{c}_{q}^{k}-\mathbf{c}_{q}^{u}\right\|_{2}^{2}\right)
\end{aligned}
\]</span> 其中： <span class="math display">\[
\mathrm{c}_{q}^{k}=\frac{\sum_{j=1}^{n}\left(\delta\left(y_{j}^{s}=Y_{k}\right) \cdot \frac{v_{d}^{s}}{\left\|\boldsymbol{v}_{j_{j}^{s}}\right\|}\right)}{\sum_{j=1}^{n} \delta\left(y_{j}^{s}=Y_{k}\right)}
\]</span> 第一个公式的理解如下：</p>
<ul>
<li>公式中的第1项是为了达到“同一类feedback中的item embedding”要“高内聚”的目标，其中 <span class="math inline">\(c_{q}^{y_{j}^{s}}\)</span> 代表属于某个feedback的所有item embedding的平均，作为这一类feedback的item embedding的中心</li>
<li>公式中的第2项是为了达到“不同类feedback的item embedding”要“低耦合”的目标，表现在不同feedback的item embedding的中心要尽可能远</li>
</ul>
<h4 id="self-training-for-target-clustering">Self-Training for Target Clustering</h4>
<p>添加伪标签的具体做法如下：</p>
<ul>
<li>训练中，如果根据当前模型，query "q"已经与某“未曝光item”的匹配得分相当低了（小于阈值p1），我们就认为这对&lt;&gt;是负样本，接下来的训练中，<span class="math inline">\(&lt;q, d^{t}&gt;\)</span>得分要变得越来越小，趋近0</li>
<li>训练中，如果根据当前模型，query "q"已经与某“未曝光item”的匹配得分相当高了（大于阈值p2），我们就认为这对&lt;&gt;是正样本，接下来的训练中，<span class="math inline">\(&lt;q, d^{t}&gt;\)</span>得分要变得越来越大，趋近1</li>
</ul>
<p><strong>具体实现时，ESAM并不需要动态修改label。因为修改label的目的，仅仅是为了接下来的优化指明方向</strong>，所以ESAM使用如下辅助task达到同样的效果： <span class="math display">\[
L_{D C}^{p}=-\frac{\sum_{j=1}^{n} \delta\left(S c_{q, d_{j}^{t}}&lt;p_{1} \mid S c_{q, d_{j}^{t}}&gt;p_{2}\right) S c_{q, d_{j}^{t}} \log S c_{q, d_{j}^{t}}}{\sum_{j=1}^{n} \delta\left(S c_{q, d_{j}^{t}}&lt;p_{1} \mid S c_{q, j}^{t}&gt;p_{2}\right)}
\]</span> 其中<span class="math inline">\(\delta\)</span>是condition function，表示<strong>这种"伪label"只在预估的匹配得分太高或太低，即模型有充分自信时，才添加</strong>，之所要设置p1，p2这两个阈值，是因为：模型一开始无法得到很好的训练，对于target item的打分存在一定的容错，因此我们需要保护 [p1, p2] 中间这一段不做分类！</p>
<h2 id="final">Final</h2>
<p>总的Loss最终设置如下： <span class="math display">\[
L_{a l l}=L_{s}+\lambda_{1} L_{D A}+\lambda_{2} L_{D C}^{c}+\lambda_{3} L_{D C}^{p}
\]</span> 梯度下降的方式如下： <span class="math display">\[
\Theta^{t+1}=\Theta^{t}-\eta \frac{\partial\left(L_{s}+\lambda_{1} L_{D A}+\lambda_{2} L_{D C}^{c}+\lambda_{3} L_{D C}^{p}\right)}{\partial \Theta}
\]</span> 最终提升效果如下：</p>
<p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/20210430223054.png" /></p>
<h2 id="reference">Reference</h2>
<p>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.10545">ESAM: Discriminative Domain Adaptation with Non-Displayed Items to Improve Long-Tail Performance</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/qLBYIkevmBpZ84hTSlmh5g">阿里ESAM：用迁移学习解决召回中的样本偏差</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-04-26T11:29:42.000Z" title="2021/4/26 下午7:29:42">2021-04-26</time>发表</span><span class="level-item"><time dateTime="2021-04-26T12:41:10.275Z" title="2021/4/26 下午8:41:10">2021-04-26</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/recsys/">recsys</a></span><span class="level-item">5 分钟读完 (大约806个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/04/26/learning-to-rank/">learning to rank中的Listwise，Pairwise和Pointwise</a></h1><div class="content">本文介绍了目前三种基于排序的loss生成方式，即Listwise，Pairwise，Pointwise</div><a class="article-more button is-small is-size-7" href="/2021/04/26/learning-to-rank/#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-04-24T01:38:00.000Z" title="2021/4/24 上午9:38:00">2021-04-24</time>发表</span><span class="level-item"><time dateTime="2021-04-24T07:07:17.277Z" title="2021/4/24 下午3:07:17">2021-04-24</time>更新</span><span class="level-item">10 分钟读完 (大约1460个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/04/24/recall-PinnerSage/">recall-PinnerSage</a></h1><div class="content"><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202104/24/094135-426175.png" /></p>
<p>推荐系统的架构：召回层（粗排）、排序层（精排）、重排序</p>
<h2 id="召回层">召回层</h2>
<blockquote>
<p>以下均是从 <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzA4NTUxNTE4Ng==&amp;mid=2247491310&amp;idx=1&amp;sn=39e4d96cfaa2d006148d06f6214fc6d4&amp;chksm=9fd79033a8a01925107bc959afb0cd14d97a7d5432b5fc666059bbac8e4559405bfa6f169d53&amp;scene=21">负样本为王: 评Facebook的向量化召回算法</a>摘录出来的笔记，</p>
<p>相关论文：</p>
<p>[1] <a href="">Embedding-based Retrieval in Facebook Search</a></p>
<p>[2] <a target="_blank" rel="noopener" href="http://research.baidu.com/Public/uploads/5d12eca098d40.pdf">MOBIUS: Towards the Next Generation of Query-Ad Matching in Baidu’s Sponsored Search</a></p>
</blockquote>
<blockquote>
<p>如果说排序是特征的艺术，那么召回就是样本的艺术，特别是<strong><em>负样本</em></strong>的艺术</p>
</blockquote>
<h3 id="如何选择负样本">如何选择负样本</h3>
<ol type="1">
<li><p>排序层的目标是目标是“从用户可能喜欢的当中挑选出用户最喜欢的”，是为了优中选优，因此排序是非常讲究所谓的“真负”样本的，即必须拿“曝光未点击”的样本做负样本</p></li>
<li><p>召回层的目标是“是将用户可能喜欢的，和海量对用户根本不靠谱的，分隔开”，而<strong>最不靠谱的，往往不是“曝光未点击”的样本</strong>，因为这里牵扯到一个推荐系统里常见的bias，就是我们从线上日志获得的训练样本，已经是上一版本的召回、粗排、精排替用户筛选过的，即已经是对用户“<strong>比较靠谱</strong>”的样本了。拿这样的样本训练出来的模型做召回，<strong>一叶障目，只见树木，不见森林</strong>。</p>
<blockquote>
<p>因此目前召回层的做法是：<strong>拿点击样本做正样本，拿随机采样做负样本</strong>。因为线上召回时，候选库里大多数的物料是与用户八杆子打不着的，随机抽样能够很好地模拟这一分布。</p>
</blockquote></li>
<li><p>随机采样不是指<strong>在整个候选库里等概率抽样</strong>，由于推荐系统中存在二八定律，因此：</p></li>
</ol>
<pre><code>-   当热门物料做正样本时，要降采样，减少对正样本集的绑架。比如，某物料成为正样本的概率如下，其中z(wi)是第i个物料的曝光或点击占比
    $$
    P_&#123;\text &#123;pos &#125;&#125;=\left(\sqrt&#123;\frac&#123;z\left(w_&#123;i&#125;\right)&#125;&#123;0.001&#125;&#125;+1\right) \frac&#123;0.001&#125;&#123;z\left(w_&#123;i&#125;\right)&#125;
    $$

-   当热门物料做负样本时，要适当过采样，抵消热门物料对正样本集的绑架；同时，也要保证冷门物料在负样本集中有出现的机会。比如，某物料成为负样本的概率如下，其中n(w)是第i个物料的出现次数，而一般取0.75
    $$
    P_&#123;\text &#123;neg &#125;&#125;=\frac&#123;n\left(w_&#123;i&#125;\right)^&#123;\alpha&#125;&#125;&#123;\sum_&#123;j&#125; n\left(w_&#123;j&#125;\right)^&#123;\alpha&#125;&#125;
    $$</code></pre>
<h3 id="如何使用hard-negative">如何使用hard Negative</h3>
<h4 id="增强样本">增强样本</h4>
<p>我们可以将召回模型的召回样例分为三部分，即：</p>
<ul>
<li>匹配度最高的，是以用户点击为代表的，那是正样本</li>
<li>匹配度最低的，那是随机抽取的。能被一眼看穿，没难度，所谓的easy negative，达不到锻炼模型的目的</li>
<li>所以要选取一部分匹配度适中的作为负样本，能够增加模型在训练时的难度，让模型能够关注细节，这就是所谓的<strong>hard negative</strong></li>
</ul>
<p>如何选取<strong>hard negative</strong>：</p>
<ul>
<li>根据业务逻辑来选取</li>
<li><strong>用上一版本的召回模型筛选出“ 没那么相似 ”的对，作为额外负样本，训练下一版本召回模型</strong></li>
</ul>
<h4 id="模型融合">模型融合</h4>
<p>用不同难度的negative训练不同难度的模型，再做多模型的融合</p>
<ul>
<li>串行：不同难度的模型独立打分，最终取Top K的分数依据是多模型打分的加权和（各模型的权重是超参，需要手工调整）</li>
<li>并行：<strong>其实就是粗排，</strong>候选物料先经过easy model的初筛，再经过hard model的二次筛选，剩余结果再交给下游，更复杂粗排或是精排。</li>
</ul>
<h3 id="全链路优化">全链路优化</h3>
<ul>
<li>全链路优化的目标：Ranker是在已有召回算法筛选过的数据的基础上训练出来的，日积月累，也强化了某种刻板印象，即&lt;user,doc&gt;之间的匹配模式，只有符合某个老召回描述的匹配模式，才能得到Ranker的青睐，才有机会曝光给用户，才能排在容易被用户点击的好位置。</li>
<li>新召回所引入的&lt;user,doc&gt;之间的匹配模式，突破了ranker的传统认知，曝光给用户的机会就不多。即使曝光了，也会被ranker排在不讨喜的位置，不太容易被用户点击</li>
</ul>
<h2 id="graphsage">GraphSAGE</h2>
<blockquote>
<p>[1] <a target="_blank" rel="noopener" href="https://towardsdatascience.com/an-intuitive-explanation-of-graphsage-6df9437ee64f">An Intuitive Explanation of GraphSAGE</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/79637787">【Graph Neural Network】GraphSAGE: 算法原理，实现和应用</a></p>
</blockquote>
<h2 id="pinsage">PinSAGE</h2>
<h2 id="pinnersage">PinnerSAGE</h2>
<blockquote>
<p>论文解读不是简单的翻译（这年头，谁还看不懂个英文呢?），而是结合自己的亲身实践，把论文中一笔带过的地方讲透，补充论文中忽略的细节，列举针对相同问题业界其他的解决方案，算是论文原文的一个补充。</p>
</blockquote>
<blockquote>
<p>Faiss是Facebook AI团队开源的针对聚类和相似性搜索库，为稠密向量提供高效相似度搜索和聚类，支持十亿级别向量的搜索，是目前最为成熟的近似近邻搜索库。</p>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-04-23T12:53:34.000Z" title="2021/4/23 下午8:53:34">2021-04-23</time>发表</span><span class="level-item"><time dateTime="2021-04-26T11:57:12.193Z" title="2021/4/26 下午7:57:12">2021-04-26</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/leetcode/">leetcode</a></span><span class="level-item">8 分钟读完 (大约1227个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/04/23/leetcode-permutation-combination/">leetcode刷题笔记-排列组合</a></h1><div class="content">本文记录了leetcode中常见的排列组合问题</div><a class="article-more button is-small is-size-7" href="/2021/04/23/leetcode-permutation-combination/#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-04-22T02:29:14.000Z" title="2021/4/22 上午10:29:14">2021-04-22</time>发表</span><span class="level-item"><time dateTime="2021-04-26T13:47:48.210Z" title="2021/4/26 下午9:47:48">2021-04-26</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/machine-learning/">machine learning</a></span><span class="level-item">4 分钟读完 (大约604个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/04/22/ML-SVM/">(SVM)支持向量机算法</a></h1><div class="content">本文记录了机器学习中第三个算法——支持向量机算法，属于有监督学习中的分类算法</div><a class="article-more button is-small is-size-7" href="/2021/04/22/ML-SVM/#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-04-21T03:37:58.000Z" title="2021/4/21 上午11:37:58">2021-04-21</time>发表</span><span class="level-item"><time dateTime="2021-04-26T13:43:43.671Z" title="2021/4/26 下午9:43:43">2021-04-26</time>更新</span><span class="level-item">5 分钟读完 (大约700个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/04/21/ML-GBDT/">ML-GBDT</a></h1><div class="content"><h2 id="gbdt梯度提升决策树">GBDT（梯度提升决策树）</h2>
<blockquote>
<p>不论我们处理的是分类任务还是回归任务，都是将CART树（回归树）作为基学习器，相比于串行的回归树，这种拟合残差的并行回归树能够利用更少的特征来防止过拟合</p>
</blockquote>
<h3 id="参考资料">参考资料</h3>
<p>[1] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/39354380">GBDT算法</a></p>
<h3 id="学习方式梯度提升">学习方式：梯度提升</h3>
<p>对于回归树中不使用平方误差作为损失函数的回归任务，我们使用损失函数的负梯度在当前模型的值作为残差的近似值，用于下一次的拟合任务——每一次建立模型是在之前建立模型损失函数的梯度下降方向</p>
<blockquote>
<p>GBDT中的学习率</p>
</blockquote>
<p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/image-20210420085120980.png" /></p>
<h3 id="gbdt二分类">GBDT二分类</h3>
<h4 id="参考资料-1">参考资料</h4>
<p>[1] <a target="_blank" rel="noopener" href="https://blog.csdn.net/program_developer/article/details/103060416?spm=1001.2014.3001.5501">深入理解GBDT二分类算法</a></p>
<h4 id="每一次建树前需要拟合的目标">每一次建树前需要拟合的目标</h4>
<blockquote>
<p>之所以每一次需要学习的目标仍然是真实值与目标值之间的残差，是因为：</p>
</blockquote>
<ol type="1">
<li><p>每一次优化的loss函数是： <span class="math display">\[
L(\theta)=\log (l(\theta))=-\sum_{i=1}^{N}\left[y_{i} \log h_{\theta}\left(x_{i}\right)+\left(1-y_{i}\right) \log \left(1-h_{\theta}\left(x_{i}\right)\right)\right]
\]</span> 其中将<span class="math inline">\(h_{\theta}\left(x_{i}\right)\)</span>替换为： <span class="math display">\[
h_{\theta}= sigmoid(F_{M}(x))= \frac{1}{1+e^{-F_M(x)}} = \frac{1}{1+e^{-\sum_{m=1}^{M} f_{m}(x)}}
\]</span></p></li>
<li><p>每一次需要拟合的新的负梯度为： <span class="math display">\[
-\frac{\partial L}{\partial F_{M}}=y_{i}-\frac{1}{1+e^{-F_{M}(x)}}=y_{i}-\hat{y}_{i}
\]</span></p></li>
<li><p>综上，我们可以得出每一次需要优化的负梯度，即为残差</p></li>
</ol>
<p>注：</p>
<ol type="1">
<li>每一次划分区域时，代表那一区域的值仍然是：那个区域所有样本的<strong>平均值</strong></li>
<li>区域划分的选择：依据是使用各自不同的Loss损失函数，对所有样本计算损失函数，选取损失函数最小的划分</li>
</ol>
<h3 id="gbdt多分类">GBDT多分类</h3>
<blockquote>
<p>一次学习多棵CART树，最终得到多个GBDT，然后使用softmax</p>
</blockquote>
<h3 id="gbdt回归">GBDT回归</h3>
<ol type="1">
<li>目标是每次去拟合损失函数对于当前值的负梯度，对于MSE和Logistic函数，负梯度都是残差</li>
</ol>
<h2 id="利用gbdt构建新特征">利用GBDT构建新特征</h2>
<p>根本方法还是GBDT用于二分类的方式，将多属性的训练样本[x1, x2, x3...., xn, y]输入GBDT，其中y的取值为0,1；gbdt会构建多棵树去拟合当前值，我们需要提取出当前样本在每棵树的叶子节点中的位置，然后利用0，1编码作为新的组合出的特征，输入随后的MLP或者LR模型。</p>
<blockquote>
<p>其中XGBoost和lightGBM是GBDT的不同的实现方式</p>
</blockquote>
<h2 id="xgboost">XGBoost</h2>
<h2 id="lightgbm">lightGBM</h2>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-04-20T13:44:52.000Z" title="2021/4/20 下午9:44:52">2021-04-20</time>发表</span><span class="level-item"><time dateTime="2021-04-30T01:52:32.108Z" title="2021/4/30 上午9:52:32">2021-04-30</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/machine-learning/">machine learning</a></span><span class="level-item">9 分钟读完 (大约1374个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/04/20/ML-decisonTree/">决策树算法</a></h1><div class="content">本文记录了机器学习中第二个算法——决策树算法，属于有监督学习中的分类算法，此处的决策树特指分类树，回归树会在后面单独出现</div><a class="article-more button is-small is-size-7" href="/2021/04/20/ML-decisonTree/#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-04-19T03:40:31.000Z" title="2021/4/19 上午11:40:31">2021-04-19</time>发表</span><span class="level-item"><time dateTime="2021-04-27T01:54:43.789Z" title="2021/4/27 上午9:54:43">2021-04-27</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/machine-learning/">machine learning</a></span><span class="level-item">5 分钟读完 (大约820个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/04/19/ML-KNN/">(KNN)k-近邻算法</a></h1><div class="content">本文记录了机器学习中第一个算法——k-近邻算法，属于有监督学习中的分类算法</div><a class="article-more button is-small is-size-7" href="/2021/04/19/ML-KNN/#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-04-14T11:00:12.000Z" title="2021/4/14 下午7:00:12">2021-04-14</time>发表</span><span class="level-item"><time dateTime="2021-04-26T13:45:46.765Z" title="2021/4/26 下午9:45:46">2021-04-26</time>更新</span><span class="level-item">5 分钟读完 (大约811个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/04/14/itemRecsys-GBDT-LR/">itemRecsys-GBDT+LR</a></h1><div class="content"><h2 id="集成学习">集成学习</h2>
<p>指构建多个弱分类器对数据集进行预测，然后用某种策略将多个分类器预测的结果集成起来，作为最终预测结果。</p>
<p>集成学习的那两大流派：Boosting和Bagging：</p>
<ul>
<li><p>Boosting：各分类器之间有依赖关系，必须串行，比如Adaboost、GBDT(Gradient Boosting Decision Tree)、Xgboost</p>
<blockquote>
<p>同一个问题，划分成串行的子问题， 先由一个人解决一部分，解决不了的，后面的人再来</p>
</blockquote></li>
<li><p>Bagging：各分类器之间没有依赖关系，可各自并行，比如随机森林（Random Forest）</p>
<blockquote>
<p>同一个问题，把问题划分成不相干的子问题，然后分派给不同的人各干各的</p>
</blockquote></li>
</ul>
<h3 id="加法模型与前向分布算法">加法模型与前向分布算法</h3>
<p>由于Adaboost和GBDT都属于加法模型，针对于加法模型，从前向后，一步只学习一个基函数及其系数</p>
<h2 id="adaboost">AdaBoost</h2>
<h3 id="参考资料">参考资料</h3>
<p>[1] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/41536315">（十三）通俗易懂理解——Adaboost算法原理</a></p>
<h3 id="主要特点">主要特点</h3>
<blockquote>
<p>经过一个分类器，就根据分类的结果：1. 利用前一轮计算的误差率改变每个样本的概率分布（权重）2. 计算出这个弱分类器在最终加法模型中的系数</p>
<p>最终，将多个弱分类器根据分类结果的好坏进行加权，得到一个新的强分类器</p>
</blockquote>
<p><span class="math display">\[
f(x)=\sum_{i=1}^{n} \alpha_{i} G_{i}(x)
\]</span></p>
<p>其中$ _{i} $是根据当前的分类错误率（带权重）得到，即： <span class="math display">\[
\alpha_{i}=\frac{1}{2} \log \frac{1-e_{i}}{e_{i}}
\]</span></p>
<h3 id="adaboost计算步骤">Adaboost计算步骤</h3>
<p>（1）首先，是初始化训练数据的权值分布D1。假设有N个训练样本数据，则每一个训练样本最开始时，都被赋予相同的权值：w1=1/N。 （2）然后，训练弱分类器hi。具体训练过程中是：如果某个训练样本点，被弱分类器hi准确地分类，那么在构造下一个训练集中，它对应的权值要减小；相反，如果某个训练样本点被错误分类，那么它的权值就应该增大。权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。 （3）最后，将各个训练得到的弱分类器组合成一个强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。</p>
<h4 id="adaboost-特点">Adaboost 特点</h4>
<p>优点：</p>
<ol type="1">
<li>随着迭代次数的增多，错误率会下降</li>
<li>不会出现过拟合问题</li>
</ol>
<p>缺点：</p>
<ol type="1">
<li>错误分类的样本的权重会随着迭代次数呈现指数式增长</li>
<li>弱分类器的训练时间比较长</li>
</ol>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">上一页</a></div><div class="pagination-next"><a href="/page/2/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/6/">6</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.png" alt="xdren"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">xdren</p><p class="is-size-6 is-block">master student in computer science</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Wu han, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">58</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">11</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">18</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xdren69" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xdren69"><i class="fab fa-github"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li><li><a class="level is-mobile" href="http://github.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">GitHub</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Datawhale%E5%AD%A6%E4%B9%A0%E6%89%8B%E8%AE%B0/"><span class="level-start"><span class="level-item">Datawhale学习手记</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/information-retrieval/"><span class="level-start"><span class="level-item">information retrieval</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/leetcode/"><span class="level-start"><span class="level-item">leetcode</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-learning/"><span class="level-start"><span class="level-item">machine learning</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/paper-reading/"><span class="level-start"><span class="level-item">paper reading</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/pytorch-learning/"><span class="level-start"><span class="level-item">pytorch-learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/recsys/"><span class="level-start"><span class="level-item">recsys</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/topic-research/"><span class="level-start"><span class="level-item">topic research</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"><span class="level-start"><span class="level-item">博客搭建</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"><span class="level-start"><span class="level-item">环境配置</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-30T14:32:17.000Z">2021-04-30</time></p><p class="title"><a href="/2021/04/30/itemRecsys-DICE/">(DICE) Disentangling User Interest and Conformity for Recommendation with Causal Embedding 论文阅读</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-30T07:58:19.000Z">2021-04-30</time></p><p class="title"><a href="/2021/04/30/recall-ESAM/">(ESAM)Discriminative Domain Adaptation with Non-Displayed Items to Improve Long-Tail Performance 论文笔记</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-26T11:29:42.000Z">2021-04-26</time></p><p class="title"><a href="/2021/04/26/learning-to-rank/">learning to rank中的Listwise，Pairwise和Pointwise</a></p><p class="categories"><a href="/categories/recsys/">recsys</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-24T01:38:00.000Z">2021-04-24</time></p><p class="title"><a href="/2021/04/24/recall-PinnerSage/">recall-PinnerSage</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-23T12:53:34.000Z">2021-04-23</time></p><p class="title"><a href="/2021/04/23/leetcode-permutation-combination/">leetcode刷题笔记-排列组合</a></p><p class="categories"><a href="/categories/leetcode/">leetcode</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">四月 2021</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/03/"><span class="level-start"><span class="level-item">三月 2021</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/02/"><span class="level-start"><span class="level-item">二月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">十二月 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">十一月 2020</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">十月 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/09/"><span class="level-start"><span class="level-item">九月 2020</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/08/"><span class="level-start"><span class="level-item">八月 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/07/"><span class="level-start"><span class="level-item">七月 2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/06/"><span class="level-start"><span class="level-item">六月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Data-Structure/"><span class="tag">Data Structure</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GNN/"><span class="tag">GNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo-Icarus/"><span class="tag">Hexo,Icarus</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RL/"><span class="tag">RL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/binary-search/"><span class="tag">binary search</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cuda/"><span class="tag">cuda</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/datawhale/"><span class="tag">datawhale</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deep-learning/"><span class="tag">deep learning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/disjoint-set/"><span class="tag">disjoint set</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/entity-disambiguation/"><span class="tag">entity disambiguation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/item-recommendation/"><span class="tag">item recommendation</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/knowledge-graph/"><span class="tag">knowledge graph</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/news-recommendation/"><span class="tag">news recommendation</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/review-based-recommendation/"><span class="tag">review-based recommendation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/text-representation/"><span class="tag">text representation</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo-1.svg" alt="Xdren&#039;s blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 Xdren</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":true},"react":{"opacity":0.7},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/"});</script></body></html>