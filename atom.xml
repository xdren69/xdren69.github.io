<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Xdren&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xdren69.github.io/"/>
  <updated>2020-12-03T02:16:41.150Z</updated>
  <id>https://xdren69.github.io/</id>
  
  <author>
    <name>Xdren</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Leveraging Demonstrations for Reinforcement Recommendation Reasoning over Knowledge Graphs论文阅读</title>
    <link href="https://xdren69.github.io/2020/11/23/ADAC-model/"/>
    <id>https://xdren69.github.io/2020/11/23/ADAC-model/</id>
    <published>2020-11-23T10:53:36.000Z</published>
    <updated>2020-12-03T02:16:41.150Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Leveraging-Demonstrations-for-Reinforcement-Recommendation-Reasoning-over-Knowledge-Graphs"><a href="#Leveraging-Demonstrations-for-Reinforcement-Recommendation-Reasoning-over-Knowledge-Graphs" class="headerlink" title="Leveraging Demonstrations for Reinforcement Recommendation Reasoning over Knowledge Graphs"></a>Leveraging Demonstrations for Reinforcement Recommendation Reasoning over Knowledge Graphs</h1><p>模型整体是基于user-item之间的path来提升推荐的准确性和可解释性的。</p><p>整个方法分类两个部分，首先构建一些不完整、不全面的path，来引导随后的ADAC模型来学习一些path；随后通过ADAC模型来学习path</p><h2 id="Demonstration-Extraction"><a href="#Demonstration-Extraction" class="headerlink" title="Demonstration Extraction"></a>Demonstration Extraction</h2><blockquote><p>  用来通过Meta-heuristics产生一些样例，可供后面的模型进行学习</p></blockquote><p>首先，我们在设计元启发规则时，应当考虑三个方面的特性：</p><ul><li>Accessibility：通过最小的标签努力可以获得样例</li><li>Explainability：要比随机采样的path更具有解释性</li><li>Accuracy：样例要能够连接已观测到的user-item之间的交互</li></ul><p>我们得到的启发式规则如下：</p><ul><li>Shortest path：多条路径时，使用Dijkstra算法寻找最短路径</li><li>Meta-path：设计几条meta-path用于之后的识别</li><li>Path of interest：通过path中用户感兴趣的entity的个数；来判断</li></ul><blockquote><p>  meta-path的理解：</p><p>  $B o b \stackrel{\text { Purchase }}{\longrightarrow}$ Revolution 5 Running Shoe$\frac{\text {Produced_By}}{\longrightarrow}$ Nike $\frac{\text {Produce}}{\longrightarrow}$ Acalme Sneaker</p><p>  User $\stackrel{\text {Purchase}}{\longrightarrow}$Item$\frac{\text {Produced}_{-} \mathrm{By}}{\longrightarrow}$ Brand $\stackrel{\text {Produce}}{\longrightarrow}$Item</p></blockquote><h2 id="Adversarial-Actor-Critic-for-Path-Finding"><a href="#Adversarial-Actor-Critic-for-Path-Finding" class="headerlink" title="Adversarial Actor-Critic for Path Finding"></a>Adversarial Actor-Critic for Path Finding</h2><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/23/190506-333537.png" alt=""></p><p>这个算法将<strong>actor-critic-based reinforcement learning</strong>和<strong>adversarial imitation learning</strong>相结合</p><h3 id="MDP环境"><a href="#MDP环境" class="headerlink" title="MDP环境"></a>MDP环境</h3><blockquote><p>  用于通知actor它现在的搜索状态，以及可能采取的行为，同时对actor产生奖励</p></blockquote><p>其定义为一个四元组$(\mathcal{S}, \mathcal{A}, \delta, \rho)$，</p><h4 id="state"><a href="#state" class="headerlink" title="state"></a>state</h4><p>只考虑K步以内的entity和relation作为状态</p><h4 id="action"><a href="#action" class="headerlink" title="action"></a>action</h4><p>所有能够与当前实体连接的下一个实体的集合</p><h4 id="transition"><a href="#transition" class="headerlink" title="transition"></a>transition</h4><p>$s_{t+1}=\delta\left(s_{t}, a_{t}\right)$</p><h4 id="reward"><a href="#reward" class="headerlink" title="reward"></a>reward</h4><p>只对最终状态进行奖励，对于中间的寻路过程不进行奖励</p><h3 id="Actor"><a href="#Actor" class="headerlink" title="Actor"></a>Actor</h3><p>用于通过全连接网络来学习根据当前状态$s_{t}$以及当前所有的行动空间$\mathcal{A}<em>{t}$，采取每一种行动的概率$p\left(a</em>{t} \mid s_{t}, \mathcal{A}<em>{t}\right)$，用$\pi</em>{\theta}\left(a_{t}, s_{t}, \mathcal{A}_{t}\right)$来表示</p><p>学习过程如下：</p><p>$h_{\theta}=\operatorname{ReLU}\left(W_{\theta, 1} s_{t}\right)$</p><p>$p\left(a_{t} \mid s_{t}, \mathcal{F}<em>{t}\right)=\pi</em>{\theta}\left(a_{t}, s_{t}, \mathcal{F}<em>{t}\right)=\frac{\mathbf{a}</em>{t} \cdot \operatorname{ReLU}\left(W_{\theta, 2} h_{\theta}\right)}{\sum a_{i} \in \mathcal{A}<em>{t} \mathbf{a}</em>{i} \cdot \operatorname{ReLU}\left(W_{\theta, 2} h_{\theta}\right)}$</p><p>其中$\mathbf{s}<em>{t}=\mathbf{u} \oplus \mathbf{e}</em>{t-K} \oplus \ldots \mathbf{e}<em>{t-1} \oplus \mathbf{r}</em>{t} \oplus \mathbf{e}_{t}$，不够k，用padding补齐</p><h3 id="Adversarial-Imitation-Learning"><a href="#Adversarial-Imitation-Learning" class="headerlink" title="Adversarial Imitation Learning"></a>Adversarial Imitation Learning</h3><p>由两个识别器组成，分别是 a path discriminator and a meta-path discriminator，分别用来识别actor产生的路径是否与path，以及meta-path相似</p><ul><li><p>Path discriminator：用于识别path形成过程中的每一步是否与样本的path相似，计算公式如下：</p><p>$h_{p}=\tanh \left(\mathbf{s}<em>{t} \oplus \mathbf{a}</em>{p, t}\right)$</p><p>$D_{p}\left(s_{t}, a_{t}\right)=\sigma\left(\beta_{p}^{T} \tanh \left(W_{p} h_{p}\right)\right)$，我们训练$D_{p}\left(s_{t}, a_{t}\right)$，使得它的可以识别当前path片段与样例的相似程度</p></li><li><p>meta-path discriminator：用于在path最终形成时，识别当前path是否与样例的meta-path相似</p><p>对于meta-path的embedding$\mathbf{M}=\mathbf{r}<em>{1} \oplus \mathbf{r}</em>{2} \oplus \ldots \mathbf{r}_{T}$，其学习公式如下：</p><p>$\boldsymbol{h}<em>{m}=\tanh \left(\boldsymbol{W}</em>{m, 1} \mathbf{M}\right)$</p><p>$D_{m}(\mathbf{M})=\sigma\left(\boldsymbol{\beta}<em>{m}^{T} \tanh \left(\boldsymbol{W}</em>{m, 2} \boldsymbol{h}_{m}\right)\right)$</p></li></ul><h3 id="Critic"><a href="#Critic" class="headerlink" title="Critic"></a>Critic</h3><p>用于建模来自MDP和两个识别器对于actor的奖励</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/23/203313-202458.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      这是一篇对于推荐系统领域与知识图谱相结合的工作的调研
    
    </summary>
    
    
      <category term="论文阅读" scheme="https://xdren69.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="knowledge graph" scheme="https://xdren69.github.io/tags/knowledge-graph/"/>
    
      <category term="RL" scheme="https://xdren69.github.io/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>对于GPT-2模型的学习</title>
    <link href="https://xdren69.github.io/2020/11/14/learning-gpt2/"/>
    <id>https://xdren69.github.io/2020/11/14/learning-gpt2/</id>
    <published>2020-11-14T02:46:22.000Z</published>
    <updated>2020-11-16T12:26:53.609Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>  GPT-2与BERT都是Transformer模型的衍生物，其中BERT是仅仅对Transformer模型中的encoder部分进行改造；而GPT-2是对Transformer模型中的decoder部分进行改造</p></blockquote><p>对于Transfoemer的理解可以看之前看<a href="https://xdren69.github.io/2020/10/13/learning-transformer/">这里</a>，BERT的理解可以看<a href="https://xdren69.github.io/2020/10/18/learning-Bert/">这里</a>，接下来我们将详细介绍GPT-2，我们将分为如下几个部分介绍：</p><ol><li>整体模型：大致了解模型的整体结构</li><li>Decoder详解：详解单个decoder中的计算原理</li><li>GPT-2的应用：详解模型如何在实际中进行应用</li></ol><h2 id="整体模型"><a href="#整体模型" class="headerlink" title="整体模型"></a>整体模型</h2><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/15/161733-313323.png" alt="GPT-2"></p><p>使用了Transformer的decoder部分，将多个decoder堆叠构成了decoder栈</p>]]></content>
    
    <summary type="html">
    
      这是一篇对于GPT-2模型的学习笔记
    
    </summary>
    
    
      <category term="NLP" scheme="https://xdren69.github.io/categories/NLP/"/>
    
    
      <category term="deep learning" scheme="https://xdren69.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Graph Transformer Networks论文阅读</title>
    <link href="https://xdren69.github.io/2020/11/09/graph-transformer-net/"/>
    <id>https://xdren69.github.io/2020/11/09/graph-transformer-net/</id>
    <published>2020-11-09T09:04:13.000Z</published>
    <updated>2020-12-03T02:16:55.995Z</updated>
    
    <content type="html"><![CDATA[<h3 id="目前挑战"><a href="#目前挑战" class="headerlink" title="目前挑战"></a>目前挑战</h3><p>目前GNN被设计来在<strong>固定和同质图</strong>上学习节点表示，在以下情况下会存在缺陷：</p><ol><li>图中的连接存在错误</li><li>包含不同类型节点和边的异质图中</li></ol><p>目前的解决办法是使用两阶段方法：</p><ol><li><strong>手动设计</strong>meta-path，需要专家和领域知识</li><li>通过meta-path将异质图转化为同质图，然后使用GNN</li></ol><p>本文是对上述方法的改进</p><h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ol><li>提出Graph Transformer Networks（GTN），其特点是：能够产生新的图结构，即识别出原本未连接的节点间的有用连接，从而学得更好的节点表示，<strong>不需要依赖领域知识</strong></li><li>新图的生成是可解释的，自动生成meta-path，不需要人为设定，meta-path的生成更加有效</li></ol><h3 id="先置概念"><a href="#先置概念" class="headerlink" title="先置概念"></a>先置概念</h3><p>meta-path：</p><p>对于关系：$A \stackrel{A P}{\longrightarrow} P \stackrel{P C}{\longrightarrow} C$，其meta-path为$A_{A P}A_{P C}$</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>异质图中存在多种关系，用一个邻接矩阵表示一种关系，整个异质图用K个邻接矩阵来表示（一共有K种关系）</p><h4 id="Graph-Transformer-GT-Layer"><a href="#Graph-Transformer-GT-Layer" class="headerlink" title="Graph Transformer (GT) Layer"></a>Graph Transformer (GT) Layer</h4><p>核心部分是Graph Transformer (GT) Layer，由两个步骤组成：</p><ol><li>GT layer从邻接矩阵集合$\mathbb{A}$中软选择两个邻接矩阵Q<sub>1</sub>,Q<sub>2</sub></li><li>通过两个关系矩阵Q<sub>1</sub>,Q<sub>2</sub>，学到新的图结构</li></ol><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/10/083427-987240.png" alt=""></p><p>对于权重向量$\boldsymbol{W}<em>{\phi}^{1}$和$\boldsymbol{W}</em>{\phi}^{2}$（1*1*k）分别进行softmax操作，然后作为1*1的卷积核对异构邻接矩阵集合中的全部K个邻接矩阵进行卷积操作。最后分别得到两个新的邻接矩阵<strong>Q<sub>1</sub>,Q<sub>2</sub></strong>，将两个矩阵相乘得到二阶meta-path图$A^{(1)}$。每进行一次新矩阵间的相乘阶数都会升高一阶。</p><h4 id="整体模型"><a href="#整体模型" class="headerlink" title="整体模型"></a>整体模型</h4><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/10/084807-669776.png" alt=""></p><ul><li><p>相比于上面介绍的Graph Transformer (GT) Layer，整体模型使用multi-channel（使用了C个通道），这样最后得到的$\mathbb{A}^{(l)}$就是由C个邻接矩阵组成的图集和。对于每一个矩阵分别进行卷积，得到C个向量表示，将这C个向量表示拼接起来得到目标node的表示。</p></li><li><p>为了得到L-hop间的信息，一共设计了L个multi-channel矩阵，这样就可以考虑到L-hop范围内的信息。</p></li></ul><h3 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h3><ol><li>可以尝试使用到知识图谱中，对知识图谱进行剪裁，前提是<strong>知识图谱中的关系的种类是有限的</strong></li></ol><h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3>]]></content>
    
    <summary type="html">
    
      这是一篇关于图自动剪裁的论文，通过soft selection剪裁节点间无用的联系，新增有用的联系
    
    </summary>
    
    
      <category term="论文阅读" scheme="https://xdren69.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="deep learning" scheme="https://xdren69.github.io/tags/deep-learning/"/>
    
      <category term="GNN" scheme="https://xdren69.github.io/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>知识图谱与推荐系统</title>
    <link href="https://xdren69.github.io/2020/11/06/recsys-with-kg/"/>
    <id>https://xdren69.github.io/2020/11/06/recsys-with-kg/</id>
    <published>2020-11-06T14:03:20.000Z</published>
    <updated>2020-12-03T02:19:46.215Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/07/103427-972429.png" alt=""></p><p>本文所包含的论文汇总：</p><table><thead><tr><th align="center">题目</th><th align="center">时间</th><th align="center">会议</th><th align="center">包含时间序列</th><th align="center">包含知识图谱</th><th>数据集</th></tr></thead><tbody><tr><td align="center">KARN</td><td align="center">2020</td><td align="center">AAAI</td><td align="center">是</td><td align="center">是</td><td>Amazon review dataset</td></tr><tr><td align="center">KNI</td><td align="center">2020</td><td align="center">DLP-KDD</td><td align="center"></td><td align="center">是</td><td>MovieLens-20M，Book-Crossing，Last.FM</td></tr><tr><td align="center">KGAT</td><td align="center">2019</td><td align="center">KDD</td><td align="center"></td><td align="center">是</td><td></td></tr><tr><td align="center">KGCN</td><td align="center">2019</td><td align="center">WWW</td><td align="center"></td><td align="center">是</td><td></td></tr><tr><td align="center">RippleNet</td><td align="center">2018</td><td align="center">CIKM</td><td align="center"></td><td align="center">是</td><td></td></tr></tbody></table><h2 id="KARN-A-Knowledge-Aware-Attentional-Reasoning-Network-for-Recommendation"><a href="#KARN-A-Knowledge-Aware-Attentional-Reasoning-Network-for-Recommendation" class="headerlink" title="KARN: A Knowledge-Aware Attentional Reasoning Network for Recommendation"></a>KARN: A Knowledge-Aware Attentional Reasoning Network for Recommendation</h2><h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>之前的工作要么只考虑使用知识图谱来解释user与item在知识图谱中之间path的合理性，要么只考虑通过用户的历史点击序列来预测用户的兴趣。本文将结合两者进行预测，这种预测方式将提高预测的准确率，具体如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/07/094120-752547.png" alt=""></p><p>用户Jark的历史记录为3-&gt;2-&gt;1，如果不考虑历史记录，只根据<strong>1和知识图谱</strong>进行推荐，则由于Titantic的导演是James Cameron，因此可能推荐他执导的另外两部电影Piranha2和Aliens，但是通过<strong>历史记录</strong>可以得到Jark喜欢的电影类型更偏向于Adventure类型的电影，因此更有可能推荐Aliens而不是Piranha2</p><h3 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h3><ol><li>提出KARN将用户的历史点击序列和知识图谱中的路径信息相结合</li><li>对于item进行向量表示时，不仅考虑了textual（tittle）的信息，还考虑了contextual（its one-hop neighbors in KGs）的信息</li><li>对于item进行向量表示时，不仅考虑用户的历史点击序列，还通过user和item之间的路径考虑了<strong>user对于该item的潜在意图</strong></li></ol><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><h4 id="对于item的表示"><a href="#对于item的表示" class="headerlink" title="对于item的表示"></a><strong>对于item的表示</strong></h4><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/07/103758-690420.png" alt=""></p><p>其中</p><ul><li>对于tittle中的每一个词的embedding组成的矩阵用F个卷积核进行卷积操作，用于提取局部语义信息（多个词组成的phrase的含义），最后经过一层变换得到与KG中实体维度相同向量V‘</li><li>对于该商品在KG中对应的entity的embedding用V来表示</li><li>对于该商品1-hop之内的neighbors entity采用<strong>平均的方法</strong>来得到向量V’‘</li></ul><p>则item的表示最终为[V, V’ , V’’]</p><h4 id="对于user-history的表示"><a href="#对于user-history的表示" class="headerlink" title="对于user history的表示"></a><strong>对于user history的表示</strong></h4><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/07/105331-105180.png" alt=""></p><p>对于历史点击的item，通过SRA网络来学习，SRA将RNN+attention得到的结果与RNN最后一个隐状态进行拼接来表示user history interest</p><h4 id="User-potential-intent"><a href="#User-potential-intent" class="headerlink" title="User potential intent"></a><strong>User potential intent</strong></h4><p>即用户对于目标商品的潜在意图，通过所有在知识图谱中由用户到商品的路径来计算得出</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/07/105941-300239.png" alt=""></p><p>对于每一条路径，e表示实体，r表示关系：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/07/110015-781535.png" alt=""></p><p>我们将（e<sub>i</sub>,r<sub>i</sub>）视为一个整体用一个向量来表示，在路径最后补上空关系r<sub>q</sub>，随后将将整条路径用多个这样的向量来表示，最后通过SRA学得整个路径的表示。将所有路径通过attention net得到User对于item的potential intent</p><h4 id="整体网络"><a href="#整体网络" class="headerlink" title="整体网络"></a><strong>整体网络</strong></h4><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/07/110840-705446.png" alt=""></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><strong>实验结果</strong></h3><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/07/111124-888823.png" alt=""></p><h3 id="反思"><a href="#反思" class="headerlink" title="反思"></a><strong>反思</strong></h3><ol><li>实验存在缺陷，没有与其他使用知识图谱的模型进行对比，即KGCN，KGAT，RippleNet</li><li>无法应用于新闻推荐，新闻的待推荐item的数量太多，无法每一个都参与user potential intent的计算</li></ol><h2 id="KGCN-Knowledge-Graph-Convolutional-Networks-for-Recommender-Systems"><a href="#KGCN-Knowledge-Graph-Convolutional-Networks-for-Recommender-Systems" class="headerlink" title="KGCN: Knowledge Graph Convolutional Networks for Recommender Systems"></a>KGCN: Knowledge Graph Convolutional Networks for Recommender Systems</h2><h3 id="Intro-1"><a href="#Intro-1" class="headerlink" title="Intro"></a>Intro</h3><p>每个item都有很多属性，这些属性之间能够相互联系，从而形成一张知识图谱。利用这张知识图谱，我们可以发现item之间的高阶语义信息和结构信息。从而做出更加准确的推荐。对于知识图谱的处理我们提出了KGCN的方法，即将GCN运用到知识图谱中，用以形成对于每个entity（item）的表示。</p><h3 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h3><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/13/100654-5848.png" alt=""></p><p>如上图所示，KGCN在本文中的应用是：规定感受野的跳数H（图中为2），以及每个实体单跳感受野中要考虑的节点个数（图中为2）。则KGCN的计算顺序是：<strong>不学习最外层的节点，先学习红色的节点，后学习蓝色的节点</strong>。</p><p>整体算法如下所示：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/13/101319-125780.png" alt=""></p><blockquote><p>  注意：当计算一个实体的上下文感受野信息时，对于单跳感受野中每个实体的权重设置不同</p></blockquote><p>算法图示如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/13/101544-573368.png" alt=""></p><h3 id="实验结果-1"><a href="#实验结果-1" class="headerlink" title="实验结果"></a>实验结果</h3><p>与其他baseline的对比结果如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/14/085000-230172.png" alt=""></p><ul><li>PER需要人工设计meta-path，因此准确性会低很多</li></ul><p>同时通过实验探究了K，H，dimension的大小对于实验准确性的影响</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/13/102721-583322.png" alt=""></p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/13/102741-163332.png" alt=""></p><h3 id="反思-1"><a href="#反思-1" class="headerlink" title="反思"></a>反思</h3><ol><li>对于感受野中固定实体的选择采用的是同一的方法，未来可以根据实体的重要性进行选择</li><li>将user和item分隔开，只对item端使用KG进行表示学习，没有尝试加入user的信息或者对user也使用KG进行表示学习</li></ol><h2 id="KNI-An-End-to-End-Neighborhood-based-Interaction-Model-for-Knowledge-enhanced-Recommendation"><a href="#KNI-An-End-to-End-Neighborhood-based-Interaction-Model-for-Knowledge-enhanced-Recommendation" class="headerlink" title="KNI: An End-to-End Neighborhood-based Interaction Model for Knowledge-enhanced Recommendation"></a>KNI: An End-to-End Neighborhood-based Interaction Model for Knowledge-enhanced Recommendation</h2><h3 id="创新点-1"><a href="#创新点-1" class="headerlink" title="创新点"></a>创新点</h3><ul><li><p>针对之前的模型，我们提出了其存在的early summarization problem，针对这一问题，我们采用Neighborhood Interaction (NI) model，相比较早期<strong>仅仅只用</strong>item的向量和user的向量计算点积率。我们这里使用<strong>item的neighborhood</strong>和<strong>user的neighborhood</strong>进行计算，可以捕捉珍贵和复杂的结构信息</p><blockquote><p>  “early summarization”：仅将user/item自身及周围的邻居信息全部汇聚到一个向量，这种方式的缺点是没有利用到item/user的local structures，即item neighborhoods与user neighborhoods之间的联系没有捕捉到。只捕捉到了目标item与user之间这一条边的关系</p></blockquote></li><li><p>为了进一步丰富节点间的连接信息和高阶结构化信息，我们引入了知识图谱，用来增加节点间的连接。因此我们称这种方法为KNI</p></li></ul><h3 id="具体模型"><a href="#具体模型" class="headerlink" title="具体模型"></a>具体模型</h3><h4 id="NI模型"><a href="#NI模型" class="headerlink" title="NI模型"></a>NI模型</h4><blockquote><p>  计算user对于item期望值的模型</p></blockquote><div>$$\begin{aligned}\text { Attention: } \hat{y}_{u, v} &=\left\langle\sum_{i \in N_{u}} \alpha_{u, i} \mathbf{x}_{i}, \sum_{j \in N_{v}} \alpha_{v, j} \mathbf{x}_{j}\right\rangle \\&=\sum_{i \in N_{u}} \sum_{j \in N_{v}} \alpha_{u, i} \alpha_{v, j}\left\langle\mathbf{x}_{i}, \mathbf{x}_{j}\right\rangle\end{aligned}$$</div><p>其中$N_{u}$为user的neighborhoods，$N_{\mathcal{V}}$为item的neighborhoods。简写如下：</p><div>$$\hat{y}=\mathrm{~A} \odot \mathrm{Z}$$</div><p>其中：</p><div>$$\sum_{i, j} \mathrm{~A}_{i, j}=1, \mathrm{Z}_{i, j}=\left\langle\mathrm{x}_{i}, \mathrm{x}_{j}\right\rangle$$</div><p>对于其中权重的计算，论文中提出如下<strong>bi-attention network</strong>：</p><div>$$\alpha_{i, j}=\operatorname{softmax}_{i, j}\left(\mathbf{w}^{\top}\left[\mathbf{x}_{u}, \mathbf{x}_{i}, \mathbf{x}_{v}, \mathbf{x}_{j}\right]+b\right)$$</div><h4 id="node-embedding模型"><a href="#node-embedding模型" class="headerlink" title="node embedding模型"></a>node embedding模型</h4><blockquote><p>  通过GCN，GAT来学到对于user/item的embedding，用学得的embedding来替代NI模型中的$\mathbf{X}<em>{i}, \mathbf{X}</em>{j}$</p></blockquote><p>假设当前计算的是node u的表示，则i是所有与u在one-hop之内相连的节点，最外层的节点$\mathbf{X}_{j}$用初始向量进行计算，讲过一次卷积得到node i的表示</p><div>$$\mathbf{x}_{i}^{1}=\sigma\left(\frac{1}{\left|N_{i}\right|} \sum_{j \in N_{i}} \mathbf{w}^{1} \mathbf{x}_{j}+\mathbf{b}^{1}\right)$$</div>将node i的表示带入node u的卷积计算中，得到node u的表示<div>$$\mathbf{x}_{u}^{2}=\sigma\left(\frac{1}{\left|N_{u}\right|} \sum_{i \in N_{u}} \mathbf{w}^{2} \mathbf{x}_{i}^{1}+\mathbf{b}^{2}\right)$$</div>#### 引入知识图谱的NI模型<blockquote><p>  即KNI模型</p></blockquote><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/13/211722-856032.png" alt=""></p><p>在user和item之间加入KG的entity来扩充图中的关系和实体</p><h4 id="整体计算流程"><a href="#整体计算流程" class="headerlink" title="整体计算流程"></a>整体计算流程</h4><ol><li><p>通过GNN逐层传递周围2-hop内的信息到目标节点，计算两个节点的embedding</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/13/212035-678806.png" alt=""></p></li><li><p>通过计算到的embedding，计算目标节点的neighborhood之间的期望得分</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/13/212531-251331.png" alt=""></p></li></ol><h3 id="实验结果-2"><a href="#实验结果-2" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/13/213545-632304.png" alt=""></p><p>实验证明NI模型的计算是有用的</p><h3 id="反思-2"><a href="#反思-2" class="headerlink" title="反思"></a>反思</h3><ol><li>将user-item之间的交互转化为neighbor-neighbor之间的交互，摆脱了双塔模型，相当于基于图的计算</li></ol><h2 id="KGAT-Knowledge-Graph-Attention-Network-for-Recommendation"><a href="#KGAT-Knowledge-Graph-Attention-Network-for-Recommendation" class="headerlink" title="KGAT: Knowledge Graph Attention Network for Recommendation"></a>KGAT: Knowledge Graph Attention Network for Recommendation</h2><h2 id="RippleNet"><a href="#RippleNet" class="headerlink" title="RippleNet"></a>RippleNet</h2>]]></content>
    
    <summary type="html">
    
      这是一篇对于推荐系统领域与知识图谱相结合的工作的调研
    
    </summary>
    
    
      <category term="recsys" scheme="https://xdren69.github.io/categories/recsys/"/>
    
    
      <category term="item recommendation" scheme="https://xdren69.github.io/tags/item-recommendation/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统的评测标准</title>
    <link href="https://xdren69.github.io/2020/11/06/performance-measure-of-resys/"/>
    <id>https://xdren69.github.io/2020/11/06/performance-measure-of-resys/</id>
    <published>2020-11-06T02:53:16.000Z</published>
    <updated>2020-11-11T07:37:06.908Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hit-k"><a href="#hit-k" class="headerlink" title="hit@k"></a><strong>hit@k</strong></h2><p>定义如下：</p><p>In a nutshell, it is the count of how many positive triples are ranked in the top-n positions against a bunch of synthetic negatives.</p><p>具体计算方法为：</p><p>In the following example, pretend the test set includes two ground truth positive only:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Jack   born_in   Italy</span><br><span class="line">Jack   friend_with   Thomas</span><br></pre></td></tr></table></figure><p>Let’s assume such positive triples (identified by * below) are ranked against four synthetic negatives each. Now, assign a score to each of the positives and its synthetic negatives using your pre-trained embedding model. Then, sort the triples in descending order. In the example below, the first triple ranks 2nd, and the other triple ranks first (against their respective synthetic negatives):</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">s        p         o            score   rank</span><br><span class="line">Jack   born_in   Ireland        <span class="number">0.789</span>      <span class="number">1</span></span><br><span class="line">Jack   born_in   Italy          <span class="number">0.753</span>      <span class="number">2</span>  *</span><br><span class="line">Jack   born_in   Germany        <span class="number">0.695</span>      <span class="number">3</span></span><br><span class="line">Jack   born_in   China          <span class="number">0.456</span>      <span class="number">4</span></span><br><span class="line">Jack   born_in   Thomas         <span class="number">0.234</span>      <span class="number">5</span></span><br><span class="line"></span><br><span class="line">s        p         o            score   rank</span><br><span class="line">Jack   friend_with   Thomas     <span class="number">0.901</span>      <span class="number">1</span>  *</span><br><span class="line">Jack   friend_with   China      <span class="number">0.345</span>      <span class="number">2</span></span><br><span class="line">Jack   friend_with   Italy      <span class="number">0.293</span>      <span class="number">3</span></span><br><span class="line">Jack   friend_with   Ireland    <span class="number">0.201</span>      <span class="number">4</span></span><br><span class="line">Jack   friend_with   Germany    <span class="number">0.156</span>      <span class="number">5</span></span><br></pre></td></tr></table></figure><p>Then, count how many positives occur in the top-1 or top-3 positions, and divide by the number of triples in the test set (which in this example includes 2 triples):</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">Hits@</span><span class="number">3</span>= <span class="number">2</span>/<span class="number">2</span> = <span class="number">1.0</span></span><br><span class="line"><span class="symbol">Hits@</span><span class="number">1</span>= <span class="number">1</span>/<span class="number">2</span> = <span class="number">0.5</span></span><br></pre></td></tr></table></figure><h2 id="ndcg-k"><a href="#ndcg-k" class="headerlink" title="ndcg@k"></a><strong>ndcg@k</strong></h2><p>全名<strong>Normalized Discounted Cumulative Gain</strong>，是一个测量排序质量的指标。我们将会分成三个步骤来介绍，即：</p><ol><li>Cumulative Gain(CG)</li><li>Discounted Cumulative Gain(DCG)</li><li>Normalized Discounted Cumulative Gain(NDCG)</li></ol><h3 id="CG"><a href="#CG" class="headerlink" title="CG"></a><strong>CG</strong></h3><p>每一个待推荐的item都有一个相关性的得分，所有相关性得分总和即为CG，对于推荐系统给出的<strong>已排序（按照推荐算法）</strong>序列A，其中<strong>相关性得分（与推荐系统的推荐顺序无关）</strong>和CG分别如下所示：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/06/214738-406944.png" alt=""></p><h3 id="DCG"><a href="#DCG" class="headerlink" title="DCG"></a><strong>DCG</strong></h3><p>对于推荐系统给出的两个<strong>有序</strong>推荐序列A和B，虽然B的结果比A的要好（按相关性从大到小进行排序），但是按照CG进行评测两者表现效果相同，因此单纯依靠CG存在很大的缺陷</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/06/204813-22694.png" alt=""></p><p>因此我们提出DCG，希望能够测量推荐系统能否将待推荐的item按照相关性的降序进行排列，其公式如下：</p><div>    $$DCG=\sum_{i=1}^{n} \frac{2^{r e l e v a n c e_{i}}-1}{\log _{2}(i+1)}$$</div><p>或者</p><div>    $$DCG=\sum_{i=1}^{n} \frac{\text {relevance}_{i}}{\log _{2}(i+1)}$$</div><p>其中第一个公式对于具有较高相关性但在推荐序列中排名较后的item具有很大的惩罚，可以作为对于推荐系统排序能力进行测量的工具。对于前面举例子的集合A，B，其DCG的得分如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/06/214715-601173.png" alt=""></p><blockquote><p>  如果相关性用0/1来表示，则两个公式的效果相同</p></blockquote><h3 id="NDCG"><a href="#NDCG" class="headerlink" title="NDCG"></a><strong>NDCG</strong></h3><p>由于不同推荐系统对于不同用户给出的待推荐序列的长度不同，相关度的范围不同，因此DCG无法作为一个通用的衡量标准，为了使其通用化，我们引入了NDCG，对于每一个推荐系统生成的序列，其计算方法如下：</p><ol><li>按照推荐系统给出的顺序计算DCG</li><li>按照相关性排序的顺序计算DCG，带到iDCG</li><li>求出两者的比率DCG/iDCG，取值范围应该在0,1之间</li></ol><p>举例如下，对于推荐系统给出的序列：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/06/214645-909101.png" alt=""></p><p>按照相关性排序可以得到：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/06/214531-962115.png" alt=""></p><p>两个序列DCG分别计算如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/06/214548-830768.png" alt=""></p><p>因此该序列NDCG为：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/06/213029-831561.png" alt=""></p><h2 id="referrence"><a href="#referrence" class="headerlink" title="referrence"></a><strong>referrence</strong></h2><p>[1] <a href="https://stackoverflow.com/questions/58796367/how-is-hitsk-calculated-and-what-does-it-mean-in-the-context-of-link-prediction">How is hits@k calculated and what does it mean in the context of link prediction in knowledge bases</a></p><p>[2] <a href="https://towardsdatascience.com/evaluate-your-recommendation-engine-using-ndcg-759a851452d1">Evaluate your Recommendation Engine using NDCG</a></p>]]></content>
    
    <summary type="html">
    
      这是一篇介绍了推荐系统的两种测评方法：hit\@k和ndcg\@k
    
    </summary>
    
    
      <category term="recsys" scheme="https://xdren69.github.io/categories/recsys/"/>
    
    
  </entry>
  
  <entry>
    <title>实体消歧(Entity Disambiguation)</title>
    <link href="https://xdren69.github.io/2020/11/01/Entity-Disambiguation/"/>
    <id>https://xdren69.github.io/2020/11/01/Entity-Disambiguation/</id>
    <published>2020-11-01T13:21:25.000Z</published>
    <updated>2020-11-06T13:34:23.313Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>  为了将新闻标题中的实体与知识图谱中的实体相对应，我们需要先进行实体识别（NER），再进行实体与知识图谱的映射。在这个过程中受导师指点，需要考虑实体消歧（Entity Disambiguation）的问题。因此进行调研。</p></blockquote><h2 id="对于实体消歧问题的描述"><a href="#对于实体消歧问题的描述" class="headerlink" title="对于实体消歧问题的描述"></a>对于实体消歧问题的描述</h2><p>在一段文本中，由于文本和语言的多态性，一个语言字段在不考虑上下文的情况下可能对应多个实体，为了将文中的实体与知识图谱中相应的实体正确对应，提出了实体消歧问题。</p><blockquote><p>  注：在文本中需要与知识图谱中的entity相对应的文字段称为mention</p></blockquote><h2 id="实体链接的步骤"><a href="#实体链接的步骤" class="headerlink" title="实体链接的步骤"></a>实体链接的步骤</h2><p>一共分为两个步骤：</p><ol><li>对于每一个mention产生一个候选实体集（candidate），实体集中的实体全部来自于知识图谱</li><li>对于候选实体集中的实体进行实体消歧，通过排序选出相关度最高的那个实体作为mention的对应实体</li></ol><h2 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h2><table><thead><tr><th>题目</th><th>时间</th><th>会议</th></tr></thead><tbody><tr><td>Collective Entity Linking in Web Text: A Graph-Based Method</td><td>2011</td><td>SIGIR</td></tr><tr><td>NeuPL</td><td>2017</td><td>CIKM</td></tr><tr><td>Pair-Linking for Collective Entity Disambiguation: Two Could Be Better Than All</td><td>2018</td><td>TKDE</td></tr></tbody></table><h2 id="Collective-Entity-Linking-in-Web-Text-A-Graph-Based-Method"><a href="#Collective-Entity-Linking-in-Web-Text-A-Graph-Based-Method" class="headerlink" title="Collective Entity Linking in Web Text: A Graph-Based Method"></a>Collective Entity Linking in Web Text: A Graph-Based Method</h2><h3 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h3><p>设计了全局协同推理方法Collective Entity Linking，如下图例子所示：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/04/093544-726881.png" alt=""></p><p>即认为单独只依赖上下文<code>standout career at Bulls, [] also acts in the movie</code>无法直接推断出Jordan就是Michael Jordan，需要借助其他的mention对应的entity，即Chicago Bulls and Space Jam来进行推断，此种方法称为协同实体链接。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>对于文本<code>During his standout career at Bull,  Jordan also acts in the movie Space Jam.</code> 构建Referent Graph，如下所示：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/04/102734-858525.png" alt=""></p><p>图中包含两种关系，即</p><ol><li>Local Mention-to-Entity Compatibility: mention与entity之间的连接</li><li>Semantic Relation between Entities: entity之间的连接</li></ol><h2 id="NeuPL-Attention-based-Semantic-Matching-and-Pair-Linking-for-Entity-Disambiguation"><a href="#NeuPL-Attention-based-Semantic-Matching-and-Pair-Linking-for-Entity-Disambiguation" class="headerlink" title="NeuPL: Attention-based Semantic Matching and Pair-Linking for Entity Disambiguation"></a>NeuPL: Attention-based Semantic Matching and Pair-Linking for Entity Disambiguation</h2><h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>本篇论文所需要解决的两个问题是：</p><ol><li>充分利用上下文信息来消除歧义</li><li>增强linked entity之间的一致性 </li></ol><h3 id="创新点-1"><a href="#创新点-1" class="headerlink" title="创新点"></a>创新点</h3><p>目标函数：</p><div>$$\Gamma^{*}=\underset{\Gamma}{\arg \max }\left[\sum_{i=1}^{N} \phi\left(m_{i}, t_{i}\right)+\psi(\Gamma)\right]$$</div>本文将目标函数拆成两个部分（模型）进行优化，即：<h4 id="local-model"><a href="#local-model" class="headerlink" title="local model"></a>local model</h4><ul><li><p>作用及含义：根据mention的context来推断mention与哪一个entity相对应，计算得到Local confidence or local score $\phi\left(m_{i}, t_{i}\right)$反映了$m_{i} \longmapsto t_{i}$的概率</p></li><li><p>现存的方法：用DNN对mentions的context进行建模，此种方法存在一些问题，没有充分利用local context的信息，即：</p><ol><li>上下文中可能包含多个mention，DNN无法确定应该关注于哪一个mention</li><li>忽视了上下文中单词间的顺序</li></ol></li><li><p>本文的改进：</p><ol><li>通过两个LSTM来学习目标mention两侧的上下文信息</li><li>同时为了消除上下文的噪音以及学习对于不同mention的权重，引入了attention机制来提取信息</li></ol><p>具体模型如下所示：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/04/101243-595369.png" alt=""></p></li></ul><h4 id="global-model"><a href="#global-model" class="headerlink" title="global model"></a>global model</h4><ul><li><p>作用及含义：在<strong>一篇文章</strong>中通过多个实体间的关系来确定待确定的实体的对应</p></li><li><p>传统的方法：</p><ul><li><p>构建整篇文章的指代实体图（Referent Graph），传统的collective entity linking方法在图中实体个数过多时会消耗过多的计算时间。</p></li><li><p>同时，也不是一篇文章中的所有实体都可以进行相互推断，比如<code>The Sun and The Times reported that Greece will have to leave the Euro soon.</code> 这句话中的关系如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/04/102542-636485.png" alt=""></p><p>观察发现，并不是所有实体间都有联系</p></li></ul></li><li><p>本文的改进：论文中提出了Pair-Linking (PL)算法，特点是，只使用一次迭代便可以完成一篇文章中所有的mentions之间的collective linking，具体的做法见下面这篇文章中介绍</p></li></ul><h2 id="Pair-Linking-for-Collective-Entity-Disambiguation-Two-Could-Be-Better-Than-All"><a href="#Pair-Linking-for-Collective-Entity-Disambiguation-Two-Could-Be-Better-Than-All" class="headerlink" title="Pair-Linking for Collective Entity Disambiguation: Two Could Be Better Than All"></a>Pair-Linking for Collective Entity Disambiguation: Two Could Be Better Than All</h2><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/04/100507-748143.png" alt=""></p><p>2-4行：计算任意两个mention对应的的candidate entity set之间的最小语义相似距离（构建图中的边）</p><p>7-9行：类似Kruskal算法，每次选取confident最高（语义相似距离最小）的边</p><p>10-13行：一旦一个candidate entity set中的entity被选中，则此set中的其他entity被从图中抹去，更新剩余的集合间语义距离</p><p>可以图示如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/04/101114-268630.png" alt=""></p><p>算法停止时间：所有的实体集中都有一个实体被选中，不需要形成最小生成树</p>]]></content>
    
    <summary type="html">
    
      这是一篇对于实体消歧现有工作的调研
    
    </summary>
    
    
      <category term="NLP" scheme="https://xdren69.github.io/categories/NLP/"/>
    
    
      <category term="entity disambiguation" scheme="https://xdren69.github.io/tags/entity-disambiguation/"/>
    
  </entry>
  
  <entry>
    <title>新闻推荐现有工作的调研(持续更新)</title>
    <link href="https://xdren69.github.io/2020/10/29/news-recommend-papers/"/>
    <id>https://xdren69.github.io/2020/10/29/news-recommend-papers/</id>
    <published>2020-10-29T12:14:29.000Z</published>
    <updated>2020-12-02T14:51:25.512Z</updated>
    
    <content type="html"><![CDATA[<h2 id="新闻推荐的特点"><a href="#新闻推荐的特点" class="headerlink" title="新闻推荐的特点"></a>新闻推荐的特点</h2><ol><li>具有较强的时效性，相比于其它推荐任务，不能使用基于ID的MF方法，可用的用户交互数据较少</li><li>新闻文章往往简洁、准确。便于使用NLP模型进行处理</li><li>新闻中包含很多实体，往往成为文章的keyword；不同于之前的基于item的推荐，往往一个item只对应一个实体</li></ol><h2 id="新闻推荐的基本思路"><a href="#新闻推荐的基本思路" class="headerlink" title="新闻推荐的基本思路"></a>新闻推荐的基本思路</h2><p>对新闻进行分类，具体类别用向量来表示；对于用户的兴趣点采用相同向量空间来表示；最后将两者点乘得到数值表示用户对目标新闻的感兴趣的概率——即分别对news和user学得一个相同向量空间向量表示（不同向量空间则不能使用点乘，可用全连接网络）</p><h2 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h2><p>本文所包含的论文汇总：</p><table><thead><tr><th>题目</th><th>时间</th><th>会议</th><th>包含时间序列</th><th>包含知识图谱</th></tr></thead><tbody><tr><td>TEKGR</td><td>2020</td><td>CIKM</td><td></td><td>是</td></tr><tr><td>KRED</td><td>2020</td><td>RecSys</td><td></td><td>是</td></tr><tr><td>LSTUR</td><td>2019</td><td>ACL</td><td>是</td><td></td></tr><tr><td>NPA</td><td>2019</td><td>KDD</td><td></td><td></td></tr><tr><td>DKN</td><td>2018</td><td>WWW</td><td>是</td><td>是</td></tr><tr><td>DRN</td><td>2018</td><td>WWW</td><td></td><td></td></tr></tbody></table><h2 id="TEKGR-News-Recommendation-with-Topic-Enriched-Knowledge-Graphs"><a href="#TEKGR-News-Recommendation-with-Topic-Enriched-Knowledge-Graphs" class="headerlink" title="TEKGR: News Recommendation with Topic-Enriched Knowledge Graphs"></a>TEKGR: News Recommendation with Topic-Enriched Knowledge Graphs</h2><h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>之前基于知识图谱的推荐模型的存在者不足，即：仅仅根据新闻标题中被识别的实体来表示新闻，但往往存在错误。如下表所示，通过训练集可以看出用户感兴趣的是<strong>Marijuana Stock</strong>，而DKN模型通过实体“Cannabis” or “Marijuana”因此会将用户的兴趣点识别为<strong>Politics</strong> and <strong>Crime</strong>，而用户关心的往往是“Prices” or “Stocks”无法被识别为特殊实体。</p><blockquote><p>  为了解决这个问题，我们给每条新闻加入类别信息，不同于传统的人工加类别信息，我们通过神经网络和知识图谱来学习类别信息</p></blockquote><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202012/02/161006-200573.png" alt=""></p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>整体模型分为三层，即：</p><ol><li>KG-Based News Modeling Layer（核心）</li><li>Attention Layer</li><li>Scoring Layer</li></ol><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202012/02/165742-225544.png" alt=""></p><h4 id="KG-Based-News-Modeling-Layer（news-embedding）"><a href="#KG-Based-News-Modeling-Layer（news-embedding）" class="headerlink" title="KG-Based News Modeling Layer（news embedding）"></a><strong>KG-Based News Modeling Layer</strong>（news embedding）</h4><p>如下所示：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202012/02/165942-367237.png" alt=""></p><p>主要由三个encoder组成，即：</p><ol><li><p>Word-Level News Encoder</p><p>一共有三层组成，即</p><ul><li>embedding层</li><li>Bi-GRU层，学习共同出现的单词间的联系，而不孤立的提取单个单词的表示</li><li>attention层，每一个单词的权重不同</li></ul><p>最终得到word-level的<strong>news representation vector</strong></p></li><li><p>Knowledge Encoder </p><p>为了提取新闻的类别信息，引入了concept的概念，即知识图谱中”is A”的relationship，举例如下：对于新闻标题“Donald Trump vs. Madonna, Everything We Know”，通过知识图谱中的”is A”关系，我们可以得到：Donald Trump is a <strong>politician</strong> and a <strong>CEO</strong> while Madonna is a <strong>singer</strong> and a <strong>feminist</strong>.  通过这些关系，有助于我们判定该新闻为政治类新闻。在这一层中我们先提取出每一个实体的多个”is A”关系对应的实体，称之为”concept”，然后对这些concept进行embedding，最后利用attention机制学习相应的权重。在这个例子中就会为“<strong>politician</strong>”和“<strong>feminist</strong>”设置较高的权重</p><p>最终得到<strong>concept vector</strong></p></li><li><p>KG-Level News Encoder</p><p>分为三个步骤：</p><ul><li>根据标题提取出实体，将相应的实体映射到知识图谱中</li><li>将实体向外延伸2-hop，并从原始的知识图谱中根据这些实体得到相应的子图</li><li>为tittle中的各个实体添加topical relation，得到一张新的子图，在图中使用GNN算法得到news的表示</li></ul><p>最终得到KG-level的<strong>news representation vector</strong></p></li></ol><p>最终得到的news embedding为，word-level和KG-level的<strong>news representation vector</strong>相结合的结果</p><h4 id="Attention-Layer（user-embedding）"><a href="#Attention-Layer（user-embedding）" class="headerlink" title="Attention Layer（user embedding）"></a>Attention Layer（user embedding）</h4><p>通过attention学习目标news与user的历史点击的news的相关程度，最终得到user的embedding</p><h4 id="Scoring-Layer"><a href="#Scoring-Layer" class="headerlink" title="Scoring Layer"></a>Scoring Layer</h4><p>将user embedding和news embedding进行点击，得到点击率</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202012/02/222126-924376.png" alt=""></p><hr><h2 id="KRED-Knowledge-Aware-Document-Representation-for-News"><a href="#KRED-Knowledge-Aware-Document-Representation-for-News" class="headerlink" title="KRED: Knowledge-Aware Document Representation for News"></a>KRED: Knowledge-Aware Document Representation for News</h2><h3 id="Intro-1"><a href="#Intro-1" class="headerlink" title="Intro"></a>Intro</h3><p>正如文章的名字Knowledge-Aware Document Representation for News Recommendations，这篇文章更注重的是创建一个位于任务上游的news representation，为位于下游的新闻推荐子任务提供类似BERT在NLU领域的工具。</p><h3 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h3><ol><li>认为现存的文档理解模型要么是没有考虑知识图谱信息，比如BERT；要么是依赖于特殊的文章编码模型，以至于缺乏泛化能力和效率，比如DKN。本篇文章<strong>将知识图谱与BERT相结合</strong></li><li>对于每一个实体在经过知识图谱表示层后，又加入了上下文信息，即context embedding层</li></ol><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>整体模型分为三层，分别是：</p><ol><li><p>Entity Representation Layer：使用Knowledge Graph Attention (KGAT) Network，将知识图谱中当前实体周围实体的信息汇聚与当前实体，用于增强对于当前实体的表示。其具体公式如下：</p><p>$$\mathbf{e}<em>{\mathcal{N}</em>{h}}=\operatorname{ReLU}\left(\mathbf{W}<em>{0}\left(\mathbf{e}</em>{h} \oplus \sum_{(h, r, t) \in \mathcal{N}<em>{h}} \pi(h, r, t) \mathbf{e}</em>{t}\right)\right)$$</p><p>其中N<sub>h</sub>表示以h为头部实体的三元组，$\pi(h, r, t)$表示注意力权重，用来控制周围节点传播多少信息量到中心节点，其计算方式如下：</p><p>$$\pi_{0}(h, r, t)=\mathbf{w}<em>{2} \operatorname{Re} L U\left(\mathbf{W}</em>{1}\left(\mathbf{e}<em>{h} \oplus \mathbf{e}</em>{r} \oplus \mathbf{e}<em>{t}\right)+\mathbf{b}</em>{1}\right)+b_{2}$$</p><p>$$\pi(h, r, t)=\frac{\exp \left(\pi_{0}(h, r, t)\right)}{\sum_{\left(h, r^{\prime}, t^{\prime}\right) \in \mathcal{N}<em>{h}} \exp \left(\pi</em>{0}\left(h, r^{\prime}, t^{\prime}\right)\right)}$$</p></li><li><p>Context Embedding Layer：为每一个实体向量加入位置、频率、类别这三种信息，通过wise-to-wise的直接相加</p></li><li><p>Information Distillation Layer：使用类似self-attention的机制来得到news representation</p></li></ol><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/29/223438-489569.png" alt="模型"></p><p>随后，为了提升这个上游模型的性能，对该模型进行Multi-Task的学习（包含Category Classification、Popularity Prediction、Local News Detection、Item Recommendation、Item-to-item Recommendation），一共分为两个阶段：</p><ul><li>stage1：we alternately train different tasks every few mini-batches. </li><li>stage2：we only include the target task’s data to finalize a task-specific model. </li></ul><h3 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h3><ol><li>对于知识图谱的使用：其中对于知识图谱的使用也是在news representation的阶段，处于整体模型的第一层</li></ol><hr><h2 id="NPA"><a href="#NPA" class="headerlink" title="NPA:"></a>NPA:</h2><h2 id="LSTUR"><a href="#LSTUR" class="headerlink" title="LSTUR:"></a>LSTUR:</h2><hr><h2 id="DKN-Deep-Knowledge-Aware-Network-for-News-Recommendation"><a href="#DKN-Deep-Knowledge-Aware-Network-for-News-Recommendation" class="headerlink" title="DKN: Deep Knowledge-Aware Network for News Recommendation"></a>DKN: Deep Knowledge-Aware Network for News Recommendation</h2><h3 id="目前新闻推荐的痛点"><a href="#目前新闻推荐的痛点" class="headerlink" title="目前新闻推荐的痛点"></a>目前新闻推荐的痛点</h3><ol><li>时效性强，无法使用传统的基于ID的协同过滤的方法</li><li>用户的兴趣往往涉及许多，我们需要动态的测量用户的兴趣</li><li>新闻中往往包含许多实体和常识，可以借助来进行知识图谱上的推理</li></ol><h3 id="News-encoder"><a href="#News-encoder" class="headerlink" title="News encoder"></a>News encoder</h3><p>对于每一个news tittle一共要学习三个层面得embedding，分别是word embedding, entity embedding以及contextual entity embedding，其中entity embedding和contextual entity embedding的学习如下：</p><h4 id="Entity-embedding"><a href="#Entity-embedding" class="headerlink" title="Entity embedding"></a>Entity embedding</h4><blockquote><p>  利用知识图谱丰富文本表示信息，即知识蒸馏的过程</p></blockquote><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/24/213012-28708.png" alt="Illustration of knowledge distillation process"></p><ol><li>先对Tittle中的文本进行entity linking，得到一些实体</li><li>将识别出来的实体映射到原始的知识图谱中，丰富实体间的关系，并提取出子图</li><li>通过TransE，TransH，TransR，TransD中的一种得到每一个entity的embedding</li></ol><h4 id="Contextual-entity-embedding"><a href="#Contextual-entity-embedding" class="headerlink" title="Contextual entity embedding"></a>Contextual entity embedding</h4><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/24/213434-262642.png" alt="Illustration of context of an entity in a knowledge graph"></p><p>考虑每个entity在知识图谱中one-hop范围内的所有entity，采用如下方法作为目标entity的Contextual entity embedding：</p><p>$\operatorname{context}(e)=\left{e_{i} \mid\left(e, r, e_{i}\right) \in \mathcal{G}\right.$ or $\left.\left(e_{i}, r, e\right) \in \mathcal{G}\right}$</p><p>$\overline{\mathrm{e}}=\frac{1}{\mid \text {context}(e) \mid} \sum_{e_{i} \in \operatorname{context}(e)} \mathbf{e}_{i}$</p><h3 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h3><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/24/210742-912650.png" alt="Illustration of the DKN framework"></p><p>输入：one piece of candidate news and one piece of a user’s clicked news</p><p>输出：click-through rate (CTR) prediction</p><blockquote><p>  此处的user embedding对于不同的candidate news是不同的，因为这里对不同的candidate news使用了注意力机制，对于不同的用户历史记录使用了不同的权重</p></blockquote><h4 id="核心组件：KCNN"><a href="#核心组件：KCNN" class="headerlink" title="核心组件：KCNN"></a>核心组件：KCNN</h4><p>模型源自于Kim CNN，如下图所示：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/24/211047-642292.png" alt="A typical architecture of CNN for sentence repre-sentation learning"></p><p>扩展到本模型中：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/24/211234-262676.png" alt="KCNN"></p><p>特点：</p><ol><li>多通道：将word embedding, entity embedding, and contextual entity embedding of news作为news编码的三个通道</li><li>word-entity-aligned：将每一个word与相应的entity对齐（疑问：有些单词没有entity与之对应，需要检查代码）</li><li>使用多个filter</li></ol><h4 id="User-encoder-Attention-based-User-Interest-Extraction"><a href="#User-encoder-Attention-based-User-Interest-Extraction" class="headerlink" title="User encoder: Attention-based User Interest Extraction"></a>User encoder: Attention-based User Interest Extraction</h4><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/30/163417-344969.png" alt="user encoder"></p><p>我们认为用户的每一次历史点击对于预测用户是否对于candidate news感兴趣的贡献度不同；因此使用attention机制来学习不同的贡献度，其计算方法如下：</p><p>$s_{t_{k}^{i}, t_{j}}=\operatorname{softmax}\left(\mathcal{H}\left(\mathbf{e}\left(t_{k}^{i}\right), \mathbf{e}\left(t_{j}\right)\right)\right)=\frac{\exp \left(\mathcal{H}\left(\mathbf{e}\left(t_{k}^{i}\right), \mathbf{e}\left(t_{j}\right)\right)\right)}{\sum_{k=1}^{N_{i}} \exp \left(\mathcal{H}\left(\mathbf{e}\left(t_{k}^{i}\right), \mathbf{e}\left(t_{j}\right)\right)\right)}$</p><p>$t_{k}^{i}$表示用户i的第k次点击；$t_{j}$表示candidate news；$\mathcal{H}$表示DNN网络</p><h4 id="实验结果-1"><a href="#实验结果-1" class="headerlink" title="实验结果"></a>实验结果</h4><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/24/221727-85238.png" alt="对比其他模型"></p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202011/24/221758-419990.png" alt="消融实验"></p>]]></content>
    
    <summary type="html">
    
      这是一篇对于新闻推荐现有工作的调研
    
    </summary>
    
    
      <category term="recsys" scheme="https://xdren69.github.io/categories/recsys/"/>
    
    
      <category term="news recommendation" scheme="https://xdren69.github.io/tags/news-recommendation/"/>
    
  </entry>
  
  <entry>
    <title>无题</title>
    <link href="https://xdren69.github.io/2020/10/25/poem/"/>
    <id>https://xdren69.github.io/2020/10/25/poem/</id>
    <published>2020-10-25T12:39:48.000Z</published>
    <updated>2020-10-29T12:08:00.260Z</updated>
    
    <content type="html"><![CDATA[<p>我愿</p><p>升入青空</p><p>坠入深海</p><p>只愿</p><p>成长的忧伤</p><p>追不上我轻灵的魂魄</p>]]></content>
    
    <summary type="html">
    
      闷骚青年的无病呻吟
    
    </summary>
    
    
      <category term="随想" scheme="https://xdren69.github.io/categories/%E9%9A%8F%E6%83%B3/"/>
    
    
  </entry>
  
  <entry>
    <title>对于Bert模型的学习</title>
    <link href="https://xdren69.github.io/2020/10/18/learning-Bert/"/>
    <id>https://xdren69.github.io/2020/10/18/learning-Bert/</id>
    <published>2020-10-18T02:57:27.000Z</published>
    <updated>2020-11-06T13:38:01.557Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h2 id="BERT之前"><a href="#BERT之前" class="headerlink" title="BERT之前"></a>BERT之前</h2><h3 id="ELMo的提出"><a href="#ELMo的提出" class="headerlink" title="ELMo的提出"></a>ELMo的提出</h3><p>ELMo属于一种word embeding，由于同一个单词可能有不同的含义，因此对于同一个单词应该有多重向量来表示。具体的应用在于，先观察整个句子，然后再为每个单词生成相应的embeding。</p><p>具体而言是将整个句子的初始embeding过一遍<strong>预训练</strong>的Bi-LSTM（language modeling）——本质上而言，也是在句子单侧的编码，为每个单词生成相应的embeding。如下图所示：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/20/155720-741504.png" alt=""></p><p>随后将这些隐藏层（包括初始的embeding）通过每种方式结合在一起</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/20/155821-940201.png" alt=""></p><p>最后得到相应的embeding，如上图湖蓝色所示</p><h3 id="ULM-FiT：明确的将迁移学习引入NLP"><a href="#ULM-FiT：明确的将迁移学习引入NLP" class="headerlink" title="ULM-FiT：明确的将迁移学习引入NLP"></a>ULM-FiT：明确的将迁移学习引入NLP</h3><p>ULM-FiT介绍了一个语言模型和一种将语言模型进行微调以适应各种其他任务的方法</p><h3 id="Transformer：超过LSTMs"><a href="#Transformer：超过LSTMs" class="headerlink" title="Transformer：超过LSTMs"></a>Transformer：超过LSTMs</h3><p>在处理长序列方面的能力超过了LSTM</p><h3 id="OpenAI-Transformer-Pre-training-a-Transformer-Decoder-for-Language-Modeling"><a href="#OpenAI-Transformer-Pre-training-a-Transformer-Decoder-for-Language-Modeling" class="headerlink" title="OpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling"></a>OpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling</h3><p>将Transformer的Decoder改造为可以与训练的语言模型，The OpenAI Transformer通过7000本书来实现预训练</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/20/163223-611539.png" alt="OpenAI提出的Transformer"></p><h3 id="迁移至下流任务"><a href="#迁移至下流任务" class="headerlink" title="迁移至下流任务"></a>迁移至下流任务</h3><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/20/165245-537620.png" alt=""></p><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><h3 id="BERT：From-Decoders-to-Encoders"><a href="#BERT：From-Decoders-to-Encoders" class="headerlink" title="BERT：From Decoders to Encoders"></a>BERT：From Decoders to Encoders</h3><blockquote><p>  目标：改变Transformer模型，使其能够学习句子双侧的序列信息</p></blockquote><p>使用Transformer的多层Encoder部分（而不是Decoder部分）来学习句子中词向量的表示，提出BERT的论文的全名是<a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>，从其名字可以看出模型的特点：<strong>Pre-training、Deep、Bidirectional、Transformer、Language Understanding</strong></p><p>由于语言模型的特点是：已知前N个词来预测第N+1个词，因此以往的语言模型均是单向学习的语言模型。而BERT提出<strong>遮蔽语言模型（masked language model，MLM）</strong>，即在训练时随机MASK掉15%的单词，对于一个被MASK的单词：</p><ul><li>有80%的概率用“[mask]”标记来替换</li><li>有10%的概率用随机采样的一个单词来替换</li><li>有10%的概率不做替换（虽然不做替换，但是还是要预测哈）</li></ul><p>具体模型如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/20/205930-712704.png" alt="bert-base-bert-large-encoders"></p><p>即进行多层Transformer中的Encoder的堆叠，论文中的模型参数为：</p><p>BERT<sub>BASE</sub> (L=12, H=768, A=12, Total Parameters=110M) </p><p>BERT<sub>LARGE</sub> (L=24, H=1024, A=16, Total Parameters=340M).</p><h3 id="BERT模型的训练特点"><a href="#BERT模型的训练特点" class="headerlink" title="BERT模型的训练特点"></a>BERT模型的训练特点</h3><p>共分为两步进行训练：</p><ol><li><p>对于被MASK的词汇进行预测：进行语言模型的学习</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/20/205321-258379.png" alt="BERT-language-modeling-masked-lm"></p></li><li><p>进行下一句预测：学习句子间的关系</p><p>其中[seq]为句子分隔符，每次输入两个句子，训练模型用来判断第二个句子是否是第一个句子的下一个</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/21/093927-712679.png" alt=""></p></li></ol><p>此后得到的模型为预训练的模型，可以作为后期词向量embeding的生成器</p><h2 id="BERT模型的应用"><a href="#BERT模型的应用" class="headerlink" title="BERT模型的应用"></a>BERT模型的应用</h2><h3 id="Sentence-Classification"><a href="#Sentence-Classification" class="headerlink" title="Sentence Classification"></a>Sentence Classification</h3><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/20/213504-454777.png" alt="bert-classifier"></p><p>在句子前面加入[CLS]标致，通过最后一层输出的[CLS]的向量表示来进行句子分类。因为该向量已经学得了整个句子的信息。因此像RNN一样不用考虑其他位置单词的信息。直接判断即可</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="http://jalammar.github.io/illustrated-bert/">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/47488095">NLP的游戏规则从此改写？从word2vec, ELMo到BERT</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/51413773">NLP必读：十分钟读懂谷歌BERT模型</a></p><p>[4] <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>]]></content>
    
    <summary type="html">
    
      这是一篇对于Bert模型的学习笔记
    
    </summary>
    
    
      <category term="NLP" scheme="https://xdren69.github.io/categories/NLP/"/>
    
    
      <category term="deep learning" scheme="https://xdren69.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>对于transformer模型的学习</title>
    <link href="https://xdren69.github.io/2020/10/13/learning-transformer/"/>
    <id>https://xdren69.github.io/2020/10/13/learning-transformer/</id>
    <published>2020-10-13T02:02:07.000Z</published>
    <updated>2020-11-15T07:59:35.965Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/13/101415-973478.png" alt="pic-1"></p><ul><li>encoders中多个堆叠的encoder之间不共享参数</li><li>encoder的个数与decoder的个数相同</li></ul><p>其中每个encoder的内部构造如下所示：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/13/103132-869378.png" alt="pic-2"></p><ul><li>self-encoder的作用，帮助编码器在编码特单词时留意其他的单词</li></ul><p>其中decoder和encoder之间的连接如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/13/103802-231799.png" alt="pic-3"></p><blockquote><p>  transformer的主要优点在于其并行计算的能力，主要在于feed forward层面</p></blockquote><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/13/110612-707334.png" alt="pic-4"></p><ul><li>通过self-attention之后的词向量，分别经过<strong><em>相同的feed forward network（参数相同）</em></strong>因此可以并行</li></ul><h2 id="Encoder解读"><a href="#Encoder解读" class="headerlink" title="Encoder解读"></a>Encoder解读</h2><h3 id="self-attention的作用机制"><a href="#self-attention的作用机制" class="headerlink" title="self-attention的作用机制"></a>self-attention的作用机制</h3><p>对于句子“The animal didn’t cross the street because it was too tired.”，利用self-attention可以将<strong>it</strong>与<strong>the animal</strong>产生关联，即通过句子中的其他词更好的解释当前词</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/13/144942-137878.png" alt="pic-5"></p><blockquote><p>  Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.</p></blockquote><h3 id="self-attention详解"><a href="#self-attention详解" class="headerlink" title="self-attention详解"></a>self-attention详解</h3><blockquote><p>  此处介绍的是Encoder中的self-attention，不同于Decoder</p></blockquote><p>对于其中一层的self-attention而言：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/13/150413-621094.png" alt="pic-5"></p><p>从向量<strong>X</strong>到向量<strong>Z</strong>的计算步骤为：</p><ol><li><p>初始化三个权重矩阵$$ W^Q $$，$$ W^K $$，$$ W^V $$（这三个权重矩阵在训练过程中参数会得到训练，<strong>在代码中表示为经过了三个线性层</strong>），分别将单词的embedding与这三个矩阵相乘，得到每个embedding对应的Query，key，value向量</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/13/152004-978635.png" alt=""></p></li><li><p>这一步的主要内容是来<strong>计算得分</strong>，即当前词和所输入句子中其他词相关性的得分。计算方法为：假设当前单词所处的位置为#1，计算其与#2位置的单词的相关性的方法为：用$$q_1$$和$$k_2$$进行点乘。举个栗子而言：假设输入的句子为<code>“thinking machines”</code>，其计算如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/13/155244-486986.png" alt=""></p></li><li><p>将得分除以8（key vector维度的平方根，paper中定义为64维），目的是为了维持更稳定的梯度。</p></li><li><p>将当前词与每个单词的<strong>得分</strong>通过softmax函数处理</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/13/155807-360976.png" alt=""></p></li><li><p>根据#1，#2位置的softmax的值，计算加权的#1，#2位置单词对应的结果向量。<strong>self-attention执行完毕</strong>，接下来将向量送入前馈网络即可</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/14/101317-622906.png" alt=""></p></li></ol><h3 id="multi-headed-attention机制详解"><a href="#multi-headed-attention机制详解" class="headerlink" title="multi-headed attention机制详解"></a>multi-headed attention机制详解</h3><blockquote><p>  本质是每次计算self-attention模块时，使用多组Query/Key/Value权重矩阵（Transformer使用的是8个），每一组矩阵代表一个头</p></blockquote><p>执行结果如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/14/103419-344274.png" alt=""></p><p>由于feedforword模块只需要输入一个矩阵，因此执行如下操作：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/14/103746-716139.png" alt=""></p><p>多头注意力机制的可视化：我们可以看出每个attention关注的侧重点不同</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/14/104100-978946.png" alt=""></p><h3 id="对于单词的embedding加入位置和序列信息"><a href="#对于单词的embedding加入位置和序列信息" class="headerlink" title="对于单词的embedding加入位置和序列信息"></a>对于单词的embedding加入位置和序列信息</h3><blockquote><p>  之前值注重于考虑词与词之间的关联性信息，对于输入序列的单词在编码时没有考虑其位置信息，在此处加以处理</p></blockquote><p>通过向量，表示出位置和序列信息，与原有信息进行加和。具体如下图所示：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/14/111647-992352.png" alt=""></p><h3 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h3><p>其可视化结果如下，在进行残差连接后，经过一个<strong>LayerNorm层</strong></p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/14/112412-607998.png" alt=""></p><p>关于LayerNorm和BatchNorm之间的区别</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/14/164818-766333.jpeg" alt=""></p><h2 id="Decoders解读"><a href="#Decoders解读" class="headerlink" title="Decoders解读"></a>Decoders解读</h2><blockquote><p>  Decoders中含有和Encoders中数量相同的encoder，每一个Decoder的结构包含三层，分别是：self-attention，Encoder-Decoder attention，Feed forward层</p></blockquote><p>其图示如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/14/150253-984255.gif" alt="transformer decoding"></p><ul><li><p>Decoders中每一个Decoder的输出会传入上一层作为输入；第一层decoder之前也是<strong>加入位置信息的embeding层</strong>，如下图所示：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/16/211125-787650.png" alt=""></p></li><li><p>Decoders是自回归的，意味着每次Decoders均只预测一个单词，每一次预测的输出都会累加起来重新作为对于下一次预测的输入（此处可以将Decoders看做一个RNN，具体如下图所示）——在输出停止符号&lt;end of sentence&gt;后停止迭代</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/16/211556-230465.gif" alt="decoder"></p><p>比如：输入&lt;start&gt;得到“I”</p><table><thead><tr><th>Decodrs的输入</th><th>Decoders的输出</th></tr></thead><tbody><tr><td>&lt;start&gt;</td><td>I</td></tr><tr><td>&lt;start&gt; I</td><td>am</td></tr><tr><td>&lt;start&gt; I am</td><td>fine</td></tr><tr><td>&lt;start&gt; I am fine</td><td>&lt;end&gt;</td></tr></tbody></table></li></ul><ul><li>Decoder中的self-attention<strong>不同于</strong>之前介绍的encoder中的self-attention，只对序列中的前面的向量计算softmax，而对后面的向量进行mask操作</li></ul><h3 id="Decoder和Encoder之间的合作"><a href="#Decoder和Encoder之间的合作" class="headerlink" title="Decoder和Encoder之间的合作"></a>Decoder和Encoder之间的合作</h3><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/14/152149-610504.png" alt=""></p><h3 id="对于Encoder-Decoder-attention的解读"><a href="#对于Encoder-Decoder-attention的解读" class="headerlink" title="对于Encoder-Decoder attention的解读"></a>对于Encoder-Decoder attention的解读</h3><p>Encoder-Decoder attention的本质还是self-attention，分别将encoder输出的序列经过线性变换得到<strong>k</strong>和<strong>v</strong>序列，将第一层经过self-attention层输出的向量作为<strong>q</strong>向量序列</p><h3 id="对于最后的Linear-and-Softmax-Layer的解读"><a href="#对于最后的Linear-and-Softmax-Layer的解读" class="headerlink" title="对于最后的Linear and Softmax Layer的解读"></a>对于最后的Linear and Softmax Layer的解读</h3><blockquote><p>  主要作用在于将输出的向量转化为对应的单词</p></blockquote><p>Linear Layer是一个全连接网络，用于将Decoders输出的向量映射到词汇表相应的维度，得到logits向量（即进行归一化前的对应到各个词的可能得分）</p><p>Softmax layer则将各个维度对应的得分映射到 (0,1] 之间的概率，对应概率最高的维度对应的单词即为目标单词</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/14/155104-628746.png" alt=""></p><h2 id="参考及图片来源"><a href="#参考及图片来源" class="headerlink" title="参考及图片来源"></a>参考及图片来源</h2><p>[1] <a href="https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0">Illustrated Guide to Transformers- Step by Step Explanation</a></p><p>[2] <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html#decoder">The Annotated Transformer</a> harvard的源码解读</p><p>[3] <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p>]]></content>
    
    <summary type="html">
    
      这是一篇对于Transformer模型的学习笔记
    
    </summary>
    
    
      <category term="NLP" scheme="https://xdren69.github.io/categories/NLP/"/>
    
    
      <category term="deep learning" scheme="https://xdren69.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu16.04安装cuda和cudnn</title>
    <link href="https://xdren69.github.io/2020/10/03/cuda-ubuntu-install/"/>
    <id>https://xdren69.github.io/2020/10/03/cuda-ubuntu-install/</id>
    <published>2020-10-03T02:21:50.000Z</published>
    <updated>2020-10-18T03:05:21.772Z</updated>
    
    <content type="html"><![CDATA[<h2 id="cuda，cudnn之间的关系"><a href="#cuda，cudnn之间的关系" class="headerlink" title="cuda，cudnn之间的关系"></a>cuda，cudnn之间的关系</h2><h2 id="cuda安装"><a href="#cuda安装" class="headerlink" title="cuda安装"></a>cuda安装</h2><h3 id="安装之前的准备"><a href="#安装之前的准备" class="headerlink" title="安装之前的准备"></a>安装之前的准备</h3><h3 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h3><p>一般安装分为装驱动和装cuda两个部分，但是现在在安装cuda的时候会可以由我们选择为我们安装适配的驱动，所以这里选择在安装cuda时安装驱动，那么先去<a href="https://developer.nvidia.com/cuda-10.1-download-archive-base">官网</a>下载合适版本的cuda，这里我们安装cuda10.1，根据系统选择如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202010/03/103126-291106.png" alt=""></p><blockquote><p>  注意：安装前要看清自己的ubuntu版本号</p></blockquote><h3 id="卸载安装"><a href="#卸载安装" class="headerlink" title="卸载安装"></a>卸载安装</h3>]]></content>
    
    <summary type="html">
    
      这是一篇关于在16.04和18.04ubuntu系统下配置cuda10.1,conda和cudnn的记录博客
    
    </summary>
    
    
      <category term="环境配置" scheme="https://xdren69.github.io/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="cuda" scheme="https://xdren69.github.io/tags/cuda/"/>
    
  </entry>
  
  <entry>
    <title>chap 10-XML retrieval</title>
    <link href="https://xdren69.github.io/2020/09/19/information-retrieval-ch10/"/>
    <id>https://xdren69.github.io/2020/09/19/information-retrieval-ch10/</id>
    <published>2020-09-19T07:30:19.000Z</published>
    <updated>2020-09-22T14:02:51.639Z</updated>
    
    <content type="html"><![CDATA[<h2 id="XML文本的基本概念"><a href="#XML文本的基本概念" class="headerlink" title="XML文本的基本概念"></a>XML文本的基本概念</h2><blockquote><p>  XML文本的主要特点为：具有复杂的树形结构，属性之间还存在嵌套关系</p></blockquote><p>XML文本举例：</p><p><img src="C:%5CUsers%5Clenovo%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200922111428101.png" alt=""></p><p>转化为树形结构：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/22/111733-50035.png" alt=""></p><ul><li><p>XML DOM：将元素、属性以及元素内部的文本表示成树的节点</p></li><li><p>XPath: 是XML文档集中的路径表达式描述标准，也称为XML上下文。路径上前后元素间使用”/“来分割；”//“表示中间可以插入多个元素。eg：act/scene 表示选择所有父节点为 act 元素的scene元素；plan//scene表示选择出现在play元素下的所有scene 元素</p></li></ul><h2 id="XML检索中的挑战性问题"><a href="#XML检索中的挑战性问题" class="headerlink" title="XML检索中的挑战性问题"></a>XML检索中的挑战性问题</h2><blockquote><p>  结构化检索中的挑战是用户希望返回文档的一部分（即 XML 元素），而不像非结构 化检索那样往往返回整个文档</p></blockquote><p>选择最合适的文档部分的一个准则是：系统应该总是检索出回答查询的最明确最具体的文档部分，即返回信息需求的最小单位</p><p>该问题相对应的问题是：“对文档的哪些部分建立索引”，具体方法如下：</p><ul><li><p>将节点分组</p></li><li><p>使用最大的元素作为索引单位，然后在最大的元素中寻找相关的子元素——自顶向下</p></li><li><p>先搜索最相关的子节点，然后扩展成更大的单位（父节点）——自底向上</p></li><li><p>对所有元素建立索引</p><p>由此产生的问题，即冗余性增大，同时元素间存在嵌套关系</p><p>解决方法：构造元素选择时的限制策略：</p><ol><li>忽略所有的小元素</li><li>忽略用户不会浏览的所有元素类型（这需要记录当前 XML 检索系统的运行日志信息）</li><li>忽略通常被评估者判定为不相关性的元素类型（如果有相关性判定的话）</li><li>只保留系统设计人员或图书馆员认定为有用的检索结果所对应的的元素类型</li></ol><p>对于剩余的冗余元素，将嵌套元素组合起来，并将查询词<strong><em>高亮显示</em></strong>来吸引用户关注相关段落</p></li></ul><blockquote><p>  对于嵌套问题，还会引起词项统计信息的不准确，解决方法如下：</p><p>  为XML的每个上下文-词项对计算idf，只考虑目标节点的父节点：比如 author#”Gates” 和 section#”Gates”</p></blockquote>]]></content>
    
    <summary type="html">
    
      这是一篇关于《信息检索导论》第十章的读书笔记，主要学习了如何对结构化的文本，即XML文本进行检索
    
    </summary>
    
    
      <category term="information retrieval" scheme="https://xdren69.github.io/categories/information-retrieval/"/>
    
    
  </entry>
  
  <entry>
    <title>chap 9-Relevance feedback and query expansion</title>
    <link href="https://xdren69.github.io/2020/09/19/information-retrieval-ch9/"/>
    <id>https://xdren69.github.io/2020/09/19/information-retrieval-ch9/</id>
    <published>2020-09-19T07:30:12.000Z</published>
    <updated>2020-09-22T14:03:08.293Z</updated>
    
    <content type="html"><![CDATA[<h2 id="相关反馈及伪相关反馈"><a href="#相关反馈及伪相关反馈" class="headerlink" title="相关反馈及伪相关反馈"></a>相关反馈及伪相关反馈</h2><blockquote><p>  属于查询优化的<strong><em>局部方法</em></strong>，在一个查询的初始返回结果的基础上进行完善，使得再次返回的结果得到优化</p></blockquote><p>相关反馈的主要思想：在信息检索的过程中通过用户交互来提高最终的检索效果。让用户来判断相关性</p><p>具体过程如下：</p><ol><li>用户提交一个简短的查询</li><li>系统返回初次检索结果</li><li>用户对部分结果进行标注，将它们标注为相关或不相关</li><li>系统基于用户的反馈计算出一个更好的查询来表示信息需求</li><li>利用新查询系统返回新的检索结果</li></ol><h3 id="Rocchio-相关反馈算法"><a href="#Rocchio-相关反馈算法" class="headerlink" title="Rocchio 相关反馈算法"></a>Rocchio 相关反馈算法</h3><p>基本原理：假定我们要找一个最优查询向量$\vec{q}$ ，它与相关文档之间的 相似度最大且同时又和不相关文档之间的相似度最小</p><p>若$C_{r}$表示相关文档集，$C_{n r}$表示不相关文档集，那么我们希望找到的最优的$\vec{q}$ 是：</p><div>$$\vec{q}_{o p t}=\underset{\vec{q}}{\arg \max }\left[\operatorname{sim}\left(\vec{q}, C_{r}\right)-\operatorname{sim}\left(\vec{q}, C_{n r}\right)\right]$$</div><p>另一种定义为：</p><div>$$\vec{q}_{o p t}=\frac{1}{\left|C_{r}\right|} \sum_{\bar{d}_{j} \in C_{r}} \vec{d}_{j}-\frac{1}{\left|C_{n r}\right|} \sum_{\bar{d}_{j} \in C_{m r}} \vec{d}_{j}$$</div><p>增加权重后为：</p><div>$$\vec{q}_{m}=\alpha \vec{q}_{0}+\beta \frac{1}{\left|D_{r}\right|} \sum_{\vec{d}_{j} \in D_{r}} \vec{d}_{j}-\gamma \frac{1}{\left|D_{n r}\right|} \sum_{\vec{d}_{j} \in D_{n r}} \vec{d}_{j}$$</div><p>其中定义$C_{r}$为已知的相关文档集，$C_{n r}$为已知的不相关文档集</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/21/214025-484233.png" alt=""></p><h3 id="影响相关反馈的因素"><a href="#影响相关反馈的因素" class="headerlink" title="影响相关反馈的因素"></a>影响相关反馈的因素</h3><ul><li><p>初始查询时出问题：</p><ol><li>拼写错误，可通过拼写校正技术来解决</li><li>跨语言IR</li><li>用户的词汇表和文档集的词汇表不同，比如laptop和notebook computer</li></ol></li><li><p>相关反馈方法的使用条件：</p><p>理想条件下，所有相关文档中的词项分布应该与用户标出的相关文档中的词项分布相似，而同时所 有不相关文档中的词项分布与相关文档中的词项分布差别很大。<strong>如果相关文档包括多个不同子类，即它们在向量空间中可 以聚成多个簇，那么 Rocchio 方法效果会不太好</strong></p></li></ul><h3 id="对于相关反馈策略的评价"><a href="#对于相关反馈策略的评价" class="headerlink" title="对于相关反馈策略的评价"></a>对于相关反馈策略的评价</h3><ul><li>首先计算出原始查询 $q_{0}$ 的正确率—召回率曲线，一轮相关反馈之后，我们计算出修改后的 查询 $q_{m}$ 并再次计算出新的正确率—召回率曲线。这样，反馈前与反馈后我们都可以在所有文档 上对结果进行评价，然后直接进行比较</li><li>利用剩余文档集（residual collection，所有文档集中除去用户判定的相关文档 后的文档集）对反馈后的结果进行评价。这种思路看上去更具现实性。不过，性能的度量结果 往往低于原始查询的结果。</li></ul><h3 id="伪相关反馈"><a href="#伪相关反馈" class="headerlink" title="伪相关反馈"></a>伪相关反馈</h3><blockquote><p>  伪相关反馈（pseudo relevance），也称为盲相关反馈（blind relevance feedback），提供了一 种自动局部分析的方法。它将相关反馈的人工操作部分自动化，因此用户不需要进行额外的交 互就可以获得检索性能的提升。</p></blockquote><p>该方法首先进行正常的检索过程，返回最相关的文档构成初始集，然后假设<strong><em>排名靠前的 k 篇文档</em></strong>是相关的，最后在此假设上像以往一样进行相关反馈</p><h3 id="间接相关反馈"><a href="#间接相关反馈" class="headerlink" title="间接相关反馈"></a>间接相关反馈</h3><blockquote><p>  在反馈过程中，我们也可以利用间接的资源而不是显式的反馈结果作为反馈的基础。这种 方法也常常称为隐式相关反馈（implicit relevance feedback）。</p></blockquote><p>隐式反馈不如显式反馈可靠，但是 会比没有任何用户判定信息的伪相关反馈更有用</p><h2 id="查询扩展"><a href="#查询扩展" class="headerlink" title="查询扩展"></a>查询扩展</h2><blockquote><p>  属于查询优化的<strong><em>全局方法</em></strong>，在不考虑查询及其返回文档情况下对初始查询进行扩展和重构的方法</p></blockquote><p>主要思想是使用同义词词典对于查询词t进行自动扩展，其中同义词词典的构建方法共有3种，即：</p><ol><li>简单辅助用户进行查询扩展</li><li>采用人工词典的方法</li><li>自动构建词典的方法</li></ol>]]></content>
    
    <summary type="html">
    
      这是一篇关于《信息检索导论》第九章的读书笔记，主要学习了查询优化的相关技术，主要包括相关反馈技术（全局方法），即利用用户对现有检索系统的反馈，来完善检索系统。以及查询扩展技术（局部方法），包括同义词扩展，拼写校正技术等
    
    </summary>
    
    
      <category term="information retrieval" scheme="https://xdren69.github.io/categories/information-retrieval/"/>
    
    
  </entry>
  
  <entry>
    <title>chap 8-Evaluation in information retrieval</title>
    <link href="https://xdren69.github.io/2020/09/16/information-retrieval-ch8/"/>
    <id>https://xdren69.github.io/2020/09/16/information-retrieval-ch8/</id>
    <published>2020-09-16T11:03:39.000Z</published>
    <updated>2020-09-22T14:03:14.895Z</updated>
    
    <content type="html"><![CDATA[<h2 id="信息检索的评价"><a href="#信息检索的评价" class="headerlink" title="信息检索的评价"></a>信息检索的评价</h2><p>信息系统测试集的组成：</p><ol><li>一个文档集</li><li>一组用于测试的<strong><em>信息需求</em></strong>集合，信息需求可以表示为<strong><em>查询</em></strong>（信息系统!=查询词）</li><li>一组相关性测试结果，对于每个查询-文档而言，赋予一个二值判断结果（相关、不相关）</li></ol><h2 id="对无序检索结果集合的评价"><a href="#对无序检索结果集合的评价" class="headerlink" title="对无序检索结果集合的评价"></a>对无序检索结果集合的评价</h2><p>正确率：</p><div>$$\text { Precision }=\frac{\text { 返回结果中相关文档的数目 }}{\text { 返回结果的数目 }}=P(\text { relevant } \mid \text { retrieved })$$</div><p>召回率：</p><div>$$\text { Recall }=\frac{\text { 返回结果中相关文档的数目 }}{\text { 所有相关文档的数目 }}=P(\text { retrieved } \mid \text { relevant })$$</div><p>精确率：</p><div>$$\text { Recall }=\frac{\text { 返回结果中真正例+正反例 }}{\text { 所有被判断的文档的数目 }}$$</div><p> 精确率往往导致不准确的结果：绝大多数情况下，信息检索中的数据存在着极度的不均衡性，比如通常情况下，超过 99.9%的文档 都是不相关文档。这样的话，一个简单地将所有的文档都判成不相关文档的系统就会获得非常 高的精确率值，从而使得该系统的效果看上去似乎很好。而即使系统实际上非常好</p><p>正确率+召回率（F值）：</p><div>$$F=\frac{1}{\alpha \frac{1}{P}+(1-\alpha) \frac{1}{R}}$$</div><p>可以通过调整$$\alpha$$来控制正确率和召回率的权重</p><h2 id="对有序检索结果的评价"><a href="#对有序检索结果的评价" class="headerlink" title="对有序检索结果的评价"></a>对有序检索结果的评价</h2><p>相比于无序检索结果，有序检索结果只对top-K个返回的结果进行处理</p><h3 id="正确率-召回率曲线："><a href="#正确率-召回率曲线：" class="headerlink" title="正确率-召回率曲线："></a>正确率-召回率曲线：</h3><p>随着K的增加，出现锯齿形图案</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/18/095812-845235.png" alt=""></p><h3 id="插值正确率："><a href="#插值正确率：" class="headerlink" title="插值正确率："></a>插值正确率：</h3><p>起到平滑的作用，具体做法为：对每一个Precision值，使用其右边最大的Precision值替代</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/18/095834-807212.png" alt=""></p><h3 id="11点插值平均正确率："><a href="#11点插值平均正确率：" class="headerlink" title="11点插值平均正确率："></a>11点插值平均正确率：</h3><p>对平滑后的Precision曲线进行均匀采样出11个点（每个点间隔0.1），然后计算这11个点的平均Precision</p><div>$$A P=\frac{1}{11} \times\left(A P_{r}(0)+A P_{r}(0.1)+\ldots+A P_{r}(1.0)\right)$$</div><h3 id="平均正确率均值MAP（Mean-Average-Precision）："><a href="#平均正确率均值MAP（Mean-Average-Precision）：" class="headerlink" title="平均正确率均值MAP（Mean Average Precision）："></a>平均正确率均值MAP（Mean Average Precision）：</h3><blockquote><p>  目前普遍使用，具有较好的稳定性和代表性</p></blockquote><ul><li><p>平均正确率AP：在每个相关文档位置上正确率的平均值</p><blockquote><p>  某个查询Q共有6个相关结果，某系统排序 返回了5篇相关文档，其位置分别是第1，第2，第5，第 10，第20位，则AP=(1/1+2/2+3/5+4/10+5/20+0)/6；其中1/1，2/2，3/5等就是平均正确率</p></blockquote></li><li><p>平均正确率均值MAP：对一组查询的top-K个返回结果求平均正确率</p><div>$$\operatorname{MAP}(Q)=\frac{1}{|Q|} \sum_{j=1}^{|Q|} \frac{1}{m_{j}} \sum_{k=1}^{m_{j}} \operatorname{Precision}\left(R_{j k}\right)$$</div></li></ul><h2 id="相关性判定"><a href="#相关性判定" class="headerlink" title="相关性判定"></a>相关性判定</h2><p>在构建测试集时，需要：</p><ol><li>设计用于测试的查询</li><li>需要判定文档的相关性</li></ol><p>此处主要讨论判定文档的相关性，即考虑雇佣多个人来进行相关性判定，所需要做的是判定多个人之间的判定是否一致，采用kappa统计量，即：</p><div>$$\text {kappa}=\frac{P(A)-P(E)}{1-P(E)}$$</div><p>其中P(A)是观察到的一致性判断比率，p(E)是比较对象间的随机一致性比率，距离如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/18/112523-142507.png" alt=""></p><h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><p>[1] <a href="https://zhuanlan.zhihu.com/p/60834912">白话mAP</a></p><p>[2] <a href="http://home.ustc.edu.cn/~zhufengx/ir/pdf/IR-6_2015.pdf">中科大课件</a></p>]]></content>
    
    <summary type="html">
    
      这是一篇关于《信息检索导论》第八章的读书笔记，主要学习了如何对一个信息检索系统进行评价。包括评价指标、标准测试集等
    
    </summary>
    
    
      <category term="information retrieval" scheme="https://xdren69.github.io/categories/information-retrieval/"/>
    
    
  </entry>
  
  <entry>
    <title>chap 7-Computing scores in a complete search system</title>
    <link href="https://xdren69.github.io/2020/09/16/information-retrieval-ch7/"/>
    <id>https://xdren69.github.io/2020/09/16/information-retrieval-ch7/</id>
    <published>2020-09-15T16:04:26.000Z</published>
    <updated>2020-10-18T03:04:09.464Z</updated>
    
    <content type="html"><![CDATA[<ul><li>以文档为单位的评分方法：每次计算一篇文档的得分</li><li>以词项为单位的评分方法：遇到每个词项时，得分能够逐渐累加</li></ul><h2 id="快速评分及排序"><a href="#快速评分及排序" class="headerlink" title="快速评分及排序"></a>快速评分及排序</h2><blockquote><p>  第六章介绍的是精确返回前K篇得分最高的文档的办法；此处我们开始关注非精确返回前K篇文档的方法，即前K篇返回的文档与最相关的K篇近似，但又不完全相同。同时用户感受不到返回的前K篇文档间的相关度有所降低，这样做的好处是可以降低运算的复杂度</p></blockquote><p>此处介绍的算法主要包含如下的两个步骤：</p><ol><li>找到一个文档集合 A，它包含了参与最后竞争的候选文档，其中$$K&lt;|A|&lt;&lt;N$$。A 不必包 含前 K 篇得分最高的文档，但是它应该包含很多和前 K 篇文档得分相近的文档</li><li>返回 A 中得分最高的 K 篇文档</li></ol><h3 id="索引去除技术"><a href="#索引去除技术" class="headerlink" title="索引去除技术"></a>索引去除技术</h3><ol><li>仅考虑查询中词项idf值超过一定阈值的单词所对应的倒排记录表</li><li>仅考虑包含多个（K个）查询词项的文档</li></ol><h3 id="胜者表"><a href="#胜者表" class="headerlink" title="胜者表"></a>胜者表</h3><p>胜者表（champion list），有时也称为优胜表（fancy list）或高分文档（top doc），它的基本 思路是，对于词典中的每个词项 t，预先计算出 r 个最高权重的文档，其中 r 的值需要事先给定。 对于 tf-idf 权重计算机制而言，词项 t 所对应的 tf 值最高的 r 篇文档构成 t 的胜者表</p><h3 id="静态得分和排序"><a href="#静态得分和排序" class="headerlink" title="静态得分和排序"></a>静态得分和排序</h3><p>很多搜索引擎中，每篇文档 d 往往都有 一个与查询无关的静态得分 g（d）。该得分函数的取值往往在 0 到 1 之间。比如，对于 Web 上 的新闻报道，g（d）可以基于用户的正面评价次数来定义。</p><p>可将静态得分和相似度组合得出每个文档的得分，即：</p><p>$$\text { net-score }(q, d)=g(d)+\frac{\vec{V}(q) \cdot \vec{V}(d)}{|\vec{V}(q) | \vec{V}(d)|}$$</p><h3 id="簇剪枝方法"><a href="#簇剪枝方法" class="headerlink" title="簇剪枝方法"></a>簇剪枝方法</h3><p>主要原理是对所有的文本使用聚类操作，聚类操作如下：</p><ol><li>从 N 篇文档组成的文档集中随机选出$$\sqrt{N}$$篇文档，它们称为先导者（leader）集合</li><li>对于剩余的$$N - \sqrt{N}$$篇（称为追随者，follower）每篇不属于先导者集合的文档，计算离之最近的先导者</li></ol><p>查询操作如下：</p><ol><li>给定查询 q，通过与$$\sqrt{N}$$个先导者计算余弦相似度，找出和它最近的先导者 L</li><li>候选集合 A 包括 L 及其追随者，然后对 A 中的所有的文档计算余弦相似度</li></ol><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/16/143035-709171.png" alt=""></p><h2 id="信息检索系统的组成"><a href="#信息检索系统的组成" class="headerlink" title="信息检索系统的组成"></a>信息检索系统的组成</h2><p>此处介绍信息检索系统中常用的一些概念，即：</p><h3 id="层次型索引"><a href="#层次型索引" class="headerlink" title="层次型索引"></a>层次型索引</h3><p>是对索引去除方法的一般化技术：例如，第 1 层索引中的 tf 阈值是 20， 第 2 层阈值是 10。这意味着第 1 层索引只保留 tf 值超过 20 的倒排记录，而第 2 层的记录只保 留 tf 值超过10的倒排记录。</p><p><img src="C:%5CUsers%5Clenovo%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200916193043677.png" alt=""></p><h3 id="查询词项的邻近性"><a href="#查询词项的邻近性" class="headerlink" title="查询词项的邻近性"></a>查询词项的邻近性</h3><blockquote><p>  对于检索中的查询，特别是Web上的自由文本查询来说，用户往往希望返回的文档中大部分或者全部查询词项之间的距离比较近，因为这表明返回文档中具有聚焦用户 查询意图的文本</p></blockquote><p>考虑一个由两个或者多个查询词项构成的查询$$t_{1}, t_{2}, \ldots, t_{k}$$，令文档中包含所有查询词项得最小窗口大小为$$\omega$$，其取值为窗口内词的个数。如果文档中不包含所有的查询词项， 那么此时可以将ω设成一个非常大的数字。<strong><em>直观上讲，ω的值越小，文档d和查询匹配程度更高</em></strong>，即可以根据ω的大小来设计权重</p><h3 id="查询分析及文档评分函数的设计"><a href="#查询分析及文档评分函数的设计" class="headerlink" title="查询分析及文档评分函数的设计"></a>查询分析及文档评分函数的设计</h3><ol><li>查询分析：将自由文本查询通过查询分析器，转化成带操作符的查询，比如对于rising  interest rates的查询的分析如下：<ol><li>查询rising  interest rates这一短语</li><li>如果太少，转而分别查询 rising interest 和 interest rates 两个查询短语</li><li>如果太少，转而分别查询 rising ，interest 和 rates 这三个查询短语</li></ol></li><li>评分函数的设计：必须融入向量空间计算、静态得分、邻近度加权或其他因素</li></ol><h2 id="搜索系统的组成"><a href="#搜索系统的组成" class="headerlink" title="搜索系统的组成"></a>搜索系统的组成</h2><p><img src="C:%5CUsers%5Clenovo%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200916212242442.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      这是一篇关于《信息检索导论》第七章的读书笔记，对于第六章中学习的文档评分方法进行了扩展
    
    </summary>
    
    
      <category term="information retrieval" scheme="https://xdren69.github.io/categories/information-retrieval/"/>
    
    
  </entry>
  
  <entry>
    <title>chap 6-Scoring, term weighting, and the vector space model</title>
    <link href="https://xdren69.github.io/2020/09/16/information-retrieval-ch6/"/>
    <id>https://xdren69.github.io/2020/09/16/information-retrieval-ch6/</id>
    <published>2020-09-15T16:03:26.000Z</published>
    <updated>2020-11-11T07:22:05.420Z</updated>
    
    <content type="html"><![CDATA[<h2 id="参数化索引及域索引"><a href="#参数化索引及域索引" class="headerlink" title="参数化索引及域索引"></a>参数化索引及域索引</h2><ol><li>元数据：指的是和文档有关的一些具有特定形式的数据，通常包含字段和数值两部分</li><li>域数据：同字段的意义相似</li></ol><p>图示如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/16/003116-419095.png" alt=""></p><p>另一种实现方式：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/16/003155-137907.png" alt=""></p><h3 id="在域索引的基础上添加权重"><a href="#在域索引的基础上添加权重" class="headerlink" title="在域索引的基础上添加权重"></a>在域索引的基础上添加权重</h3><p>假定每篇文档有 l 个域，其对应的权重分别是$$g_{1}, \ldots, g_{l} \in [0,1]$$，且满足$$\sum_{i=1}^{l} g_{i}=1$$，其中$$s_{i}$$是查询文档和某个域的匹配得分（若匹配，值为1；否则，值为0），则一篇文档的查询得分为$$\sum_{i=1}^{l} g_{i} s_{i}$$</p><h3 id="对于权重的学习"><a href="#对于权重的学习" class="headerlink" title="对于权重的学习"></a>对于权重的学习</h3><p>考虑一个简单的域加权评分的例子，其中每篇文档只包含 title 和 body 两个域。给定查询 q和文档d，根据 title 及 body 域是否和 q 匹配，利用布尔匹配函数分别计算出布尔变量$$s_{T}(d, q)$$和$$s_{B}(d, q)$$，主要接下来要确定g的值：</p><p>$$\operatorname{socre}(d, q)=g \cdot s_{T}(d, q)+(1-g) s_{B}(d, q)$$</p><h2 id="词项频率及权重计算"><a href="#词项频率及权重计算" class="headerlink" title="词项频率及权重计算"></a>词项频率及权重计算</h2><blockquote><p>  目前只考虑了词项在文档中出现与否的情况，未考虑词项出现的频率。不同于之前的权重计算方法，</p></blockquote><ol><li><p>我们认为：如果一个查询词在文档中出现的频率越高，所应该赋予的权重就越大</p><p>此处，我们引入<strong><em>词项频率（TF）</em></strong>，我们使用<strong><em>词袋模型</em></strong>（直接将出现的次数作为权重），即不在乎词的位置，只在乎出现的次数</p></li><li><p>我们认为：并不是所有词的重要性都是一样的，应该赋予不同的词以不同的权重（根据它在所有文档中出现的频繁程度，越频繁价值越低）</p><p>此处，我们引入<strong><em>逆文档频率（IDF）</em></strong>，其中对于<strong><em>文档频率（DF）</em></strong>的定义为：词项在文档集中出现的次数（同一文档出现多次算做一次）；而IDF与DF的关系如下：</p><p>$$i d f_{t}=\log \frac{N}{d f_{t}}$$</p></li><li><p>我们通常将TF和IDF结合起来计算一个文档与查询之间的相关度，即：</p><div>$$\operatorname{tf-idf}_{t, d}=\operatorname{tf}_{t, d} \times \mathrm{idf}_{t}$$</div><p>当查询由多个词组成时，tf和idf用向量来表示，结果用<strong><em>内积</em></strong>的形式来表示；具体可见之前的一篇<a href="https://xdren69.github.io/2020/07/25/datawhale-NLP-t3/">博客</a></p></li></ol><h2 id="空间向量模型"><a href="#空间向量模型" class="headerlink" title="空间向量模型"></a>空间向量模型</h2><p>一组文档的集合可以看成向量空间中的多个向量，每个词项 对应一个坐标轴。这种表示忽略了词项在文档中的相对次序。即在这种模型表示下，文档 Mary is quicker than John 和 John is quicker than Mary 是等价的</p><h3 id="利用向量空间进行文档相似度的计算"><a href="#利用向量空间进行文档相似度的计算" class="headerlink" title="利用向量空间进行文档相似度的计算"></a>利用向量空间进行文档相似度的计算</h3><p>此处使用余弦相似度这一计算模型：</p><p>$$\operatorname{sim}\left(d_{1}, d_{2}\right)=\frac{\vec{V}\left(d_{1}\right) \cdot \vec{V}\left(d_{2}\right)}{\left|\vec{V}\left(d_{1}\right) | \vec{V}\left(d_{2}\right)\right|}$$</p><p>于是，将查找与 d 最相似的文档这个问题可以归结成寻找和d有最大内积结果的过程。在查询时，一般需要构建查询向量，即将查询文本视为一个短文档，并为它构建向量。</p><h3 id="通过向量进行查询结果排序"><a href="#通过向量进行查询结果排序" class="headerlink" title="通过向量进行查询结果排序"></a>通过向量进行查询结果排序</h3><blockquote><p>  用于给定查询，从文档集中返回得分最高的k篇文档</p></blockquote><p>计算向量相似度的基本算法如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/16/104037-896668.png" alt=""></p><p>原理：对于每一个查询词项，对于其倒排序表中的每一个文档累加该词项的权重。最后比较所有参与过计算的文档的权重和，选出前K个文档。</p>]]></content>
    
    <summary type="html">
    
      这是一篇关于《信息检索导论》第六章的读书笔记，主要学习了如何对文档进行评分和排序
    
    </summary>
    
    
      <category term="information retrieval" scheme="https://xdren69.github.io/categories/information-retrieval/"/>
    
    
  </entry>
  
  <entry>
    <title>chap 5-Index compression</title>
    <link href="https://xdren69.github.io/2020/09/11/information-retrieval-ch5/"/>
    <id>https://xdren69.github.io/2020/09/11/information-retrieval-ch5/</id>
    <published>2020-09-11T11:03:19.000Z</published>
    <updated>2020-09-22T14:03:34.276Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>  压缩索引的目的：1.增加cache的利用率；2.加速数据从磁盘到内存的速度</p><p>  本章介绍的压缩技术均为无损压缩，即压缩之后所有的原始信息都被保留；大小写转换、词干还原、和停用词剔除均是有损压缩技术</p></blockquote><h2 id="信息检索中词项的统计特性"><a href="#信息检索中词项的统计特性" class="headerlink" title="信息检索中词项的统计特性"></a>信息检索中词项的统计特性</h2><ol><li>Heaps定律：对于词汇表大小的估计</li><li>Zipf定律：对词项分布的建模</li></ol><h2 id="词典压缩"><a href="#词典压缩" class="headerlink" title="词典压缩"></a>词典压缩</h2><ol><li><p>使用变长字符串</p><p>查询表中只存指针，每个词项均对应一个指针</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/15/221648-685702.png" alt=""></p></li><li><p>按块存储</p><p>块间二分查找，块内遍历（块内K个词项），<strong><em>减少指针数目</em></strong></p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/15/222427-744299.png" alt=""></p><p>k越大，压缩率越高；但是查找效率越低</p><p><img src="C:%5CUsers%5Clenovo%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200915224507604.png" alt=""></p></li></ol><h2 id="倒排记录表的压缩"><a href="#倒排记录表的压缩" class="headerlink" title="倒排记录表的压缩"></a>倒排记录表的压缩</h2><blockquote><p>  倒排记录表中的内容为文档的序号</p><p>  主要采用：1.可变字节码；2.Y编码这两种压缩方式</p></blockquote><h3 id="可变字节编码"><a href="#可变字节编码" class="headerlink" title="可变字节编码"></a>可变字节编码</h3><p>利用<strong><em>整数个字节</em></strong>对同一单词的倒排记录表中的相邻文档间的间距进行编码，字节的后七位是间距的有效编码区，第一位是延续位（如果该位为1，表示结尾；否则不是）</p><p>举例如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/15/230000-791791.png" alt=""></p><h3 id="Y编码"><a href="#Y编码" class="headerlink" title="Y编码"></a>Y编码</h3><p>Y编码的组成：将间距G表示成长度和偏移两个部分进行变长编码，G的偏移实际上是G的二进制编码，但是前端的1被去掉；eg：对 13（二进制为 1101）进行编码，其偏移为 101。偏移的长度为3位，长度部分采用一元编码（一开始有连续个1，最后以0结尾，1的个数表示长度）。即为1110.</p><p>举例如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/15/231014-156147.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      这是一篇关于《信息检索导论》第五章的读书笔记，主要学习了如何压缩索引
    
    </summary>
    
    
      <category term="information retrieval" scheme="https://xdren69.github.io/categories/information-retrieval/"/>
    
    
  </entry>
  
  <entry>
    <title>chap 4-Index construction</title>
    <link href="https://xdren69.github.io/2020/09/09/information-retrieval-ch4/"/>
    <id>https://xdren69.github.io/2020/09/09/information-retrieval-ch4/</id>
    <published>2020-09-09T01:36:43.000Z</published>
    <updated>2020-09-22T14:03:40.628Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基于块的排序索引方法（BSBI）"><a href="#基于块的排序索引方法（BSBI）" class="headerlink" title="基于块的排序索引方法（BSBI）"></a>基于块的排序索引方法（BSBI）</h2><blockquote><p>  此处主要使用的算法是外部排序算法和归并算法</p></blockquote><p>主要步骤为：</p><ol><li>将文档集分割成几个大小相等的部分（所谓的块）</li><li>将每个部分的词项 ID—文档 ID 对排序（将每个词项映射为ID）</li><li>将中间产生的临时排序结果存放到磁盘中</li><li>将所有的中间文件合并成最终的索引</li></ol><p>特点：</p><ol><li>需要构建词项-词项ID的映射表</li><li>所有的索引结果都需要经过排序</li></ol><h2 id="single-pass-in-memory-indexing（SPIMI）"><a href="#single-pass-in-memory-indexing（SPIMI）" class="headerlink" title="single-pass in-memory indexing（SPIMI）"></a>single-pass in-memory indexing（SPIMI）</h2><p>相比于BSBI，其优点为：</p><ol><li>不需要构建词项-词项ID的映射表，节约内存</li><li>索引结果不需要排序，节约时间</li></ol><p>算法描述：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/09/102342-615432.png" alt=""></p><p>最后的内存块的合并过程此处省略，与BSBI相同。</p><p>特点：将词典直接存入每一个内存块中</p><h2 id="分布式索引构建方法"><a href="#分布式索引构建方法" class="headerlink" title="分布式索引构建方法"></a>分布式索引构建方法</h2><p>采用MapReduce框架，其图示如下所示：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/09/131230-92186.png" alt=""></p><ul><li>Map 阶段：将输入的数据片映射成键—值对。这个映射过程对应于 BSBI 和 SPIMI 算法中的分析任务，因此也将执行 Map 过程的机器称为分析器（parser）</li><li>Reduce阶段：将同一键（词项ID）的所有值（文档ID）集中存储，以便快速读取和处理。实现时，将所有的键按照词项区间划分成j个段，并将属于每个段的键—值对写入各自分区文件即可</li></ul><h2 id="动态索引构建方法"><a href="#动态索引构建方法" class="headerlink" title="动态索引构建方法"></a>动态索引构建方法</h2><ul><li>周期性重建索引：周期性地对文档集从头开始进行索引重构。如果随时间的推移 文档更新的次数不是很多，并且<strong><em>能够接受对新文档检索的一定延迟</em></strong>，再加上如果有足够的资源 能够支持在建立新索引的同时让旧索引继续工作，那么周期性索引重构不失为一种较好的选择</li><li>添加辅助索引：<strong><em>要求能够及时检索到新文档</em></strong>，一个是大的主 索引，另一个是小的用于存储新文档信息的辅助索引，后者保存在内存中。检 索时可以同时遍历两个索引并将结果合并。文档的删除记录在一个无效位向量中，在返回结果之前可以利用它过滤掉已删除文档。某篇文档的更新通过先删除后重新 插入来实现。</li></ul><h2 id="其他检索类型"><a href="#其他检索类型" class="headerlink" title="其他检索类型"></a>其他检索类型</h2><ul><li>排序式检索系统（ranked retrieval）：相比于布尔检索系统，不按照文档ID排序，仅仅按照权重或者影响程度排序</li><li>安全性检索系统：添加用户访问权限，构建ACL（access control list，访问控制表）</li></ul>]]></content>
    
    <summary type="html">
    
      这是一篇关于《信息检索导论》第四章的读书笔记，主要学习了如何在不同的硬件环境和文本集大小的条件下构建索引表
    
    </summary>
    
    
      <category term="information retrieval" scheme="https://xdren69.github.io/categories/information-retrieval/"/>
    
    
  </entry>
  
  <entry>
    <title>chap 3-Dictionaries and tolerant retrieval</title>
    <link href="https://xdren69.github.io/2020/09/08/information-retrieval-ch3/"/>
    <id>https://xdren69.github.io/2020/09/08/information-retrieval-ch3/</id>
    <published>2020-09-08T02:20:05.000Z</published>
    <updated>2020-09-22T14:03:46.684Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>  本章主要讲解了如何对倒排索引进行优化，分为三个部分：对于词典部分的查询进行优化、对通配符查询的处理、拼写校正的问题以及基于发音的校正技术</p></blockquote><h2 id="词典搜索的数据结构"><a href="#词典搜索的数据结构" class="headerlink" title="词典搜索的数据结构"></a>词典搜索的数据结构</h2><ol><li><p>哈希表</p><p>容易产生冲突问题，尤其是词汇表不断变大的情况下</p></li><li><p>搜索树——二叉树、B树</p><ul><li>二叉树：注意二叉树的平衡问题</li><li>B树：适用于部分词典常驻磁盘的情况</li></ul></li></ol><h2 id="通配符查询"><a href="#通配符查询" class="headerlink" title="通配符查询"></a>通配符查询</h2><h3 id="使用B树和反向B树相结合"><a href="#使用B树和反向B树相结合" class="headerlink" title="使用B树和反向B树相结合"></a>使用B树和反向B树相结合</h3><p>单一通配符：对于单词lemon，其中反向B树中的路径就是 root-n-o-m-e-l；对于查询se*mon，可以通过B-树来返回所有前缀为se且后缀非空的词项子集W，再通过反向B-树来返 回所有后缀为mon且前缀非空的词项子集R，然后，对W和R求交集W ∩ R</p><p>多通配符：通过穷举法检查返回的集合中的每个元素</p><h3 id="轮排索引"><a href="#轮排索引" class="headerlink" title="轮排索引"></a>轮排索引</h3><blockquote><p>  此方法是对于正反向B树的改进，即只使用一棵B树</p></blockquote><p>单一通配符：我们在字符集中引入一个新的符号$，用于标识词项结束；对于单词hello的轮排索引的结果如下所示：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/08/134111-733594.png" alt=""></p><p>处理通配符时的应用：考虑通配符查询 m*n，这里的关键是将查询进行旋转让*号出现 在字符串末尾，即得到 n$m*，此时便可以用正向B树来解决</p><p>多通配符：通过穷举法检查返回的集合中的每个元素</p><h3 id="K-gram索引"><a href="#K-gram索引" class="headerlink" title="K-gram索引"></a>K-gram索引</h3><blockquote><p>  k-gram表示的是将每个单词按照k个字母来划分，分成字母片段对于 castle 来说，所有的 3-gram包括$ca、cas、ast、stl、tle 及 le$</p></blockquote><p>本方法直接适用于多通配符，此时词汇表变为k-gram形式，而每个倒排记录表则由包含该k-gram的词项组成，而每个单词还会对应一个文章ID的倒排记录表</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/08/135024-806429.png" alt=""></p><h2 id="拼写校正"><a href="#拼写校正" class="headerlink" title="拼写校正"></a>拼写校正</h2><blockquote><p>  用于解决在用户输入拼写错误的查询词时，能检测到并还原为正确的查询词，最终返回正确查询词的查询结果。</p></blockquote><h3 id="拼写校正的实现"><a href="#拼写校正的实现" class="headerlink" title="拼写校正的实现"></a>拼写校正的实现</h3><p>对于一个拼写错误的查询，在其可能的正确拼写中，选择“距离”最近或者邻近度最小的那个。在距离和邻居度相同的情况下，选择其他用户查询最频繁的</p><h3 id="校正方法"><a href="#校正方法" class="headerlink" title="校正方法"></a>校正方法</h3><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/09/090154-850966.png" alt=""></p><h4 id="词项独立的校正方法"><a href="#词项独立的校正方法" class="headerlink" title="词项独立的校正方法"></a>词项独立的校正方法</h4><blockquote><p>   在对查询进行分词之后，只对单个查询词进行校正，此时很难检测到flew form Heathrow中的错误</p></blockquote><ol><li>编辑距离法</li></ol><p>编辑距离的定义：将字符串s1转换为字符串s2所需要的最小编辑操作数（删除一个字符、替换一个字符、增加一个字符）</p><p>一般通过动态规划来计算，其算法描述为：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/09/092015-24223.png" alt="image-20200909092014035"></p><ol start="2"><li>K-gram法</li></ol><p>原理：正确的目标查询词为待匹配词项包含查询 q（拼写错误） 中某个固定数目的 k-gram 即可</p><p>举例：假定我们想返回 bord 的 3 个 2-gram 中的至少 2 个词项，对倒排记录表的单遍扫描会返回满足该条件的所有词项，本例当中，这些词项包括 aboard、boardroom 及 border</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/09/092938-723616.png" alt=""></p><ol start="3"><li>混合法</li></ol><p>首先使用 k-gram 索引返回可能是 q 的潜在正确拼写形式的词项集合，然后计算该集合中的每个元素和 q 之间的编辑距离并选择 具有较小距离的那些词项。</p><h4 id="上下文敏感的拼写校正"><a href="#上下文敏感的拼写校正" class="headerlink" title="上下文敏感的拼写校正"></a>上下文敏感的拼写校正</h4><p>尝试对短语中的每个词进行替换。比如对于上面 flew form Heathrow 的例子，我们 可能会返回如下短语 fled from Heathrow 和 flew fore Heathrow。对每个替换后的短语，搜索引擎 进行查找并确定最后的返回数目。</p><h4 id="基于发音的校正技术"><a href="#基于发音的校正技术" class="headerlink" title="基于发音的校正技术"></a>基于发音的校正技术</h4><p>原理：：（1）在名称转录时，元音是可以互换的；（2）发音相似的辅音字母归为一类。这就会导致相关的名称通常有相同的 soundex 编码结果</p><p>方法：使用soundex编码方法</p>]]></content>
    
    <summary type="html">
    
      这是一篇关于《信息检索导论》第三章的读书笔记
    
    </summary>
    
    
      <category term="information retrieval" scheme="https://xdren69.github.io/categories/information-retrieval/"/>
    
    
  </entry>
  
  <entry>
    <title>chap 2-The term vocabulary and postings lists</title>
    <link href="https://xdren69.github.io/2020/09/07/information-retrieval-ch2/"/>
    <id>https://xdren69.github.io/2020/09/07/information-retrieval-ch2/</id>
    <published>2020-09-07T10:32:59.000Z</published>
    <updated>2020-10-18T03:03:55.882Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Document-delineation-and-character-sequence-decoding"><a href="#Document-delineation-and-character-sequence-decoding" class="headerlink" title="Document delineation and character sequence decoding"></a>Document delineation and character sequence decoding</h2><p>根据不同的编码方式，将字节序列转化为字符序列</p><h2 id="文本的序列化处理"><a href="#文本的序列化处理" class="headerlink" title="文本的序列化处理"></a>文本的序列化处理</h2><blockquote><p>  将文档和查询使用相同的方法转化为词条</p></blockquote><ol><li><p>关键术语的区分</p><ul><li>token： tokenization is the task of chopping it up into pieces, called tokens；</li><li>type： A type is the class of all tokens term containing the same character sequence</li><li>term:  A term is a (perhaps normalized) type that is included in the IR system’s dictionary. </li></ul><blockquote><p>  example: For example, if the document to be indexed is to sleep perchance to dream, then there are five <strong>tokens</strong>, but only four <strong>types</strong> (because there are two instances of to). However, if to is omitted from the index (as a stop word) , then there are only three <strong>terms</strong>: sleep, perchance, and dream.</p></blockquote></li><li><p>需要解决的问题</p><ol><li><p>分词问题：</p><p>英语：对于空格和连字符的处理</p><p>中文：无法通过空格来分词，词边界不明显</p><p>德语：对复合词进行拆分</p></li><li><p>去除停用词：通过文档集频率来选择停用词，频率越大说明越不具有特殊性（现代搜索系统的影响并不大）——一般先分词，后去除停用词</p></li><li><p>term normalization：将看起来不完全一致的多个词条归纳成一个等价类，如：anti-discriminatory和antidiscriminatory均映射成antidiscriminatory，一般的方法是构建同义词表，归一化的方法如下：</p><ul><li><p>合并同义词表中多个词的查询结果</p></li><li><p>构建索引时，便对词进行扩展</p></li></ul></li><li><p>词干还原（stemming）与词形归并（lemmatization）</p><ul><li>词干还原：利用启发式规则去除两端前缀</li><li>词形归并：利用词汇表和词形分析</li></ul></li></ol></li></ol><h2 id="基于skip-pointers的倒排记录表快速合并算法"><a href="#基于skip-pointers的倒排记录表快速合并算法" class="headerlink" title="基于skip pointers的倒排记录表快速合并算法"></a>基于skip pointers的倒排记录表快速合并算法</h2><blockquote><p>  只适用于AND操作，不适用于OR操作</p></blockquote><p>算法图示：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/08/085927-684023.png" alt=""></p><p>算法描述：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/08/085901-511093.png" alt=""></p><p>跳表指针的设置的位置：</p><p>每隔$$\sqrt{P}$$均设置一个跳表指针，指向下一个跳表指针的位置，P是跳表的长度</p><h2 id="短语查询的解决方法"><a href="#短语查询的解决方法" class="headerlink" title="短语查询的解决方法"></a>短语查询的解决方法</h2><blockquote><p>  目前一般采用如下两种方式的混合</p></blockquote><h3 id="二元词索引"><a href="#二元词索引" class="headerlink" title="二元词索引"></a>二元词索引</h3><ul><li><p>将文档中的每个连接词对看成一个短语</p><p>“stanford university palo alto”会被转化成“stanford university” and “university palo” and “palo alto”</p></li><li><p>扩展的二元词对：先进行词性标注，再将一个多词序列看成一个扩展的二元词</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/08/095513-349431.png" alt=""></p></li></ul><h3 id="位置信息索引"><a href="#位置信息索引" class="headerlink" title="位置信息索引"></a>位置信息索引</h3><blockquote><p>  在每个倒排记录中，存储该词在文本中出现的位置</p></blockquote><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/08/100443-886366.png" alt=""></p><p>举例如下：</p><p><img src="https://gitblog-1302688916.cos.ap-beijing.myqcloud.com/cs224n/202009/08/100559-306434.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      这是一篇关于《信息检索导论》第二章的读书笔记
    
    </summary>
    
    
      <category term="information retrieval" scheme="https://xdren69.github.io/categories/information-retrieval/"/>
    
    
  </entry>
  
</feed>
